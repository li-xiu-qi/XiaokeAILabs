{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24c4a58",
   "metadata": {},
   "source": [
    "## 安装依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c69674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q matplotlib opencv-python torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille ipywidgets openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b7819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新加载autoreload扩展，使代码修改后自动生效\n",
    "%reload_ext autoreload\n",
    "# 设置autoreload模式为2，自动重新加载所有导入的模块\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a0e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312ea9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d46f8812541138607d2dac139f5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed681b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92641539c984a21a89261a0eee527fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\rag\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\k\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bf7210041b4bc8a07a8b216030a5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f5ea253bc248a1a28aed239b2efac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed34d0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'source'],\n",
      "    num_rows: 2647\n",
      "})\n",
      "{'text': ' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n', 'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'}\n",
      "2013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 打印数据集信息（特征，行数）\n",
    "print(ds)\n",
    "\n",
    "# 打印数据集的第一个示例\n",
    "print(ds[0])\n",
    "\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['text', 'source'],\n",
    "#     num_rows: 2647\n",
    "# })\n",
    "text = ds[0][\"text\"]\n",
    "print(len(text))\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6f0606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2439ae0a294c4cfe99a3dff5bcd3f879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6e65d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG（Retrieval-Augmented Generation）是一种结合了检索和生成技术的人工智能模型架构，旨在增强生成模型的能力。在标准的生成模型中，模型根据输入的上下文信息生成文本。然而，这些模型可能缺乏对最新信息或特定领域知识的访问。RAG 通过检索相关的外部知识源，然后利用这些检索到的信息来生成更准确、更相关的文本。\n",
      "\n",
      "以下是 RAG 的工作流程：\n",
      "\n",
      "1. **检索（Retrieval）**：当输入一个查询或上下文时，RAG 首先从外部知识库中检索相关的文档或信息。这通常通过嵌入技术和相似度匹配来实现，例如使用词嵌入或句子嵌入来找到与查询最相似的文档。\n",
      "\n",
      "2. **生成（Generation）**：在检索到相关文档后，RAG 使用这些文档作为额外的输入，结合原始查询或上下文信息，生成更准确和相关的文本。生成部分通常使用预训练的生成模型，如 GPT 或 BERT 的变体。\n",
      "\n",
      "通过这种方式，RAG 能够利用外部知识库的丰富信息，提高生成文本的质量和准确性。这在需要特定领域知识或最新信息的任务中特别有用，例如问答系统、文档摘要、内容生成等。\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# llm_client = InferenceClient(\n",
    "#     model=repo_id,\n",
    "#     timeout=120,\n",
    "# )\n",
    "\n",
    "\n",
    "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "#     response = inference_client.post(\n",
    "#         json={\n",
    "#             \"inputs\": prompt,\n",
    "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
    "#             \"task\": \"text-generation\",\n",
    "#         },\n",
    "#     )\n",
    "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "# call_llm(llm_client, \"This is a test context\")\n",
    "import openai\n",
    "import os\n",
    "import dotenv\n",
    "# 建议将 API 密钥存储在环境变量中\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_name = \"THUDM/GLM-4-32B-0414\"\n",
    "\n",
    "# 或者直接设置（不推荐用于生产环境）\n",
    "# openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "# 如果您使用的是 Azure OpenAI，请按如下方式初始化：\n",
    "llm_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url = os.getenv(\"BASE_URL\"),\n",
    ")\n",
    "# 否则，使用默认的 OpenAI 初始化：\n",
    "# client = openai.OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "def call_llm(openai_client: openai.OpenAI, prompt: str, model: str = \"THUDM/GLM-4-32B-0414\"):\n",
    "    \"\"\"\n",
    "    使用指定的 OpenAI 模型生成文本。\n",
    "\n",
    "    Args:\n",
    "        openai_client: 初始化后的 OpenAI 客户端实例。\n",
    "        prompt: 输入给模型的提示文本。\n",
    "        model: 要使用的模型名称，比如：THUDM/GLM-4-32B-0414.\n",
    "\n",
    "    Returns:\n",
    "        模型生成的文本内容。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=4096, # 根据需要调整\n",
    "            temperature=0.7 # 根据需要调整\n",
    "        )\n",
    "        # 检查 response.choices 是否为空以及第一个 choice 是否存在\n",
    "        if response.choices and len(response.choices) > 0 and response.choices[0].message:\n",
    "             return response.choices[0].message.content.strip()\n",
    "        else:\n",
    "             print(\"Warning: OpenAI API returned an empty response or invalid structure.\")\n",
    "             return \"\" # 或者可以抛出异常\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while calling the OpenAI API: {e}\")\n",
    "        return \"\" # 或者根据错误处理策略返回 None 或抛出异常\n",
    "\n",
    "\n",
    "test_prompt = \"解释一下什么是 RAG？\"\n",
    "generated_text = call_llm(llm_client, test_prompt)\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a4e13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n",
      "\n",
      "## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\n",
      "\n",
      "## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\n",
      "\n",
      "## 3. Define the [Security Level](security) for the Endpoint:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\n",
      "\n",
      "## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\n",
      "\n",
      "## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\n",
      "\n",
      "## 6. Test your Endpoint in the overview with the Inference widget 🏁 🎉!\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\n",
      "\n",
      "============================================================\n",
      "1216\n",
      "============================================================\n",
      "创建端点\n",
      "\n",
      "首次登录后，您将被引导至[端点创建页面](https://ui.endpoints.huggingface.co/new)。作为示例，本指南将详细介绍部署[distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)进行文本分类的步骤。\n",
      "\n",
      "## 1. 输入Hugging Face仓库ID和您想要的端点名称：\n",
      "\n",
      "![选择仓库](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png)\n",
      "\n",
      "## 2. 选择您的云提供商和地区。最初，只有AWS可用作为云提供商，地区为`us-east-1`和`eu-west-1`。我们很快会添加Azure，如果您需要测试与其他云提供商或地区的端点，请告知我们。\n",
      "\n",
      "![选择地区](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png)\n",
      "\n",
      "## 3. 为端点定义[安全级别](security)：\n",
      "\n",
      "![定义安全](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png)\n",
      "\n",
      "## 4. 通过点击**创建端点**来创建您的端点。默认情况下，您的端点使用中等CPU（2 x 4GB vCPUs，配备Intel Xeon Ice Lake）。成本估算假设端点将运行整个月，且不考虑自动缩放。\n",
      "\n",
      "![创建端点和成本](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png)\n",
      "\n",
      "## 5. 等待端点构建、初始化和运行，这可能需要1到5分钟。\n",
      "\n",
      "![概述](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png)\n",
      "\n",
      "## 6. 使用推理小部件在概述中测试您的端点！🏁 🎉\n",
      "\n",
      "![运行推理](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png)\n"
     ]
    }
   ],
   "source": [
    "zh_text = call_llm(llm_client, f\"翻译下面的内容:\\n{text}\")\n",
    "print(text)\n",
    "print(\"===\" * 20)\n",
    "print(len(zh_text))\n",
    "print(\"===\" * 20)\n",
    "print(zh_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "ZH_QA_generation_prompt = \"\"\"\n",
    "\n",
    "以下是中文版指示：\n",
    "你的任务是根据给定的上下文编写一个事实性问题及其答案。\n",
    "你的事实性问题应当能够通过上下文中的具体、简洁的事实信息来回答。\n",
    "你的事实性问题应当以用户在搜索引擎中提问的风格来表述。\n",
    "这意味着你的事实性问题不能包含\"根据文章\"或\"上下文\"等表述。\n",
    "\n",
    "请按照以下格式提供你的回答：\n",
    "\n",
    "Output:::\n",
    "Factoid question: (你的事实性问题)\n",
    "Answer: (你对该事实性问题的回答)\n",
    "\n",
    "以下是上下文：\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef574eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe2b1bfd9f147dda0e15bb8eda79dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 10  # 我们在这里有意只生成10个QA对，考虑到成本和时间因素\n",
    "\n",
    "print(f\"生成 {N_GENERATIONS} 个QA对...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # 生成QA对\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"答案太长\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd7dc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Tip&gt;\\n\\nGuess mode does not have any impact on prompt conditioning and you can still provide a prompt if you want.\\n\\n&lt;/Tip&gt;\\n\\nSet `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) to set the `guidance_scale` value between 3.0 and 5.0.\\n\\n```py\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\nfrom diffusers.utils import load_image, make_image_grid\\nimport numpy as np\\nimport torch\\nfrom PIL import Image\\nimport cv2\\n\\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\\n\\noriginal_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png\")\\n\\nimage = np.array(original_image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)\\n\\nimage = pipe(\"\", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]\\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\\n```\\n\\n&lt;div class=\"flex gap-4\"&gt;\\n  &lt;div&gt;\\n    &lt;img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png\"/&gt;\\n    &lt;figcaption class=\"mt-2 text-center text-sm text-gray-500\"&gt;regular mode with prompt&lt;/figcaption&gt;\\n  &lt;/div&gt;\\n  &lt;div&gt;\\n    &lt;img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png\"/&gt;\\n    &lt;figcaption class=\"mt-2 text-center text-sm text-gray-500\"&gt;guess mode without prompt&lt;/figcaption&gt;\\n  &lt;/div&gt;\\n&lt;/div&gt;\\n\\n## ControlNet with Stable Diffusion XL</td>\n",
       "      <td>What is the recommended guidance scale value for guess mode in StableDiffusionControlNetPipeline?\\n</td>\n",
       "      <td>Between 3.0 and 5.0</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 context  \\\n",
       "0  <Tip>\\n\\nGuess mode does not have any impact on prompt conditioning and you can still provide a prompt if you want.\\n\\n</Tip>\\n\\nSet `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) to set the `guidance_scale` value between 3.0 and 5.0.\\n\\n```py\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\nfrom diffusers.utils import load_image, make_image_grid\\nimport numpy as np\\nimport torch\\nfrom PIL import Image\\nimport cv2\\n\\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\\n\\noriginal_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png\")\\n\\nimage = np.array(original_image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)\\n\\nimage = pipe(\"\", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]\\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\\n```\\n\\n<div class=\"flex gap-4\">\\n  <div>\\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png\"/>\\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">regular mode with prompt</figcaption>\\n  </div>\\n  <div>\\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png\"/>\\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">guess mode without prompt</figcaption>\\n  </div>\\n</div>\\n\\n## ControlNet with Stable Diffusion XL   \n",
       "\n",
       "                                                                                              question  \\\n",
       "0  What is the recommended guidance scale value for guess mode in StableDiffusionControlNetPipeline?\\n   \n",
       "\n",
       "                answer  \\\n",
       "0  Between 3.0 and 5.0   \n",
       "\n",
       "                                                                     source_doc  \n",
       "0  huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eab277",
   "metadata": {},
   "source": [
    "## 设置批判智能体\n",
    "1. 具体性（Groundedness）：问题是否可以从给定的上下文中得到回答？\n",
    "2. 相关性（Relevance）：问题对用户是否相关？例如，\"transformers 4.29.1 发布的日期是什么？\"对于 ML 用户来说并不相关。\n",
    "3. 独立（Stand-alone）：对于一个具有领域知识/互联网访问权限的人来说，问题在没有任何上下文的情况下是否可以理解？与此相反的是，对于从特定博客文章生成的问题比如”这篇文章中使用的函数是什么？”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "# 作用：让 LLM 判断提供的问题是否能仅根据给定的上下文明确回答。\n",
    "# 评估维度：“扎根性”（Groundedness）。\n",
    "# 评分标准：1（根据上下文完全无法回答）到 5（根据上下文可以清晰明确地回答）。\n",
    "\n",
    "zh_question_groundedness_critique_prompt = \"\"\"\n",
    "你将得到一个上下文和一个问题。\n",
    "你的任务是提供一个“总评分”，评估在给定上下文的情况下，问题能在多大程度上得到明确的回答。\n",
    "请按1到5的等级评分，其中1表示根据上下文完全无法回答该问题，5表示根据上下文可以清晰明确地回答该问题。\n",
    "\n",
    "请按以下格式提供你的回答：\n",
    "\n",
    "回答:::\n",
    "评价理由: (你给出评分的理由，文本形式)\n",
    "总评分: (你的评分，1到5之间的数字)\n",
    "\n",
    "你必须在回答中提供“评价理由:”和“总评分:”的值。\n",
    "\n",
    "以下是问题和上下文。\n",
    "\n",
    "问题: {question}\\n\n",
    "上下文: {context}\\n\n",
    "回答::: \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# 作用：让 LLM 判断提供的问题对于使用 Hugging Face 生态系统构建 NLP 应用的机器学习开发者来说是否有用。\n",
    "# 评估维度：“相关性”（Relevance）。\n",
    "# 评分标准：1（该问题完全没有用处）到 5（该问题非常有用）。\n",
    "\n",
    "zh_question_relevance_critique_prompt = \"\"\"\n",
    "你将得到一个问题。\n",
    "你的任务是提供一个“总评分”，代表这个问题对于使用Hugging Face生态系统构建NLP应用的机器学习开发者有多大用处。\n",
    "请按1到5的等级评分，其中1表示该问题完全没有用处，5表示该问题非常有用。\n",
    "\n",
    "请按以下格式提供你的回答：\n",
    "\n",
    "回答:::\n",
    "评价理由: (你给出评分的理由，文本形式)\n",
    "总评分: (你的评分，1到5之间的数字)\n",
    "\n",
    "你必须在回答中提供“评价理由:”和“总评分:”的值。\n",
    "\n",
    "以下是问题。\n",
    "\n",
    "问题: {question}\\n\n",
    "回答::: \"\"\"\n",
    "\n",
    "# 作用：让 LLM 判断提供的问题是否能在脱离原始上下文的情况下被理解。\n",
    "# 评估维度：“独立性”（Stand-alone）。\n",
    "# 评分标准：1（该问题需要依赖额外信息才能被理解）到 5（该问题本身就有意义）。特别注意，即使问题包含专业术语，只要能通过查阅文档理解，也可评为 5 分；但如果问题中隐含提及“上下文”或“文档中”，则必须评为 1 分。\n",
    "\n",
    "zh_question_standalone_critique_prompt = \"\"\"\n",
    "你将得到一个问题。\n",
    "你的任务是提供一个“总评分”，代表这个问题在多大程度上是独立于上下文的。\n",
    "请按1到5的等级评分，其中1表示该问题需要依赖额外信息才能被理解，5表示该问题本身就有意义。\n",
    "例如，如果问题提到了特定的环境，如“在上下文中”或“在文档中”，评分必须为1。\n",
    "问题可以包含像Gradio、Hub、Hugging Face或Space这样晦涩的技术名词或缩写，并且仍然可以评为5分：只要对于一个能够查阅文档的操作员来说，问题的含义是清晰的即可。\n",
    "\n",
    "例如，“ViT模型是从哪个检查点导入的？”应评为1分，因为它隐含地提到了一个上下文，因此问题并非独立于上下文。\n",
    "\n",
    "请按以下格式提供你的回答：\n",
    "\n",
    "回答:::\n",
    "评价理由: (你给出评分的理由，文本形式)\n",
    "总评分: (你的评分，1到5之间的数字)\n",
    "\n",
    "你必须在回答中提供“评价理由:”和“总评分:”的值。\n",
    "\n",
    "以下是问题。\n",
    "\n",
    "问题: {question}\\n\n",
    "回答::: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a33b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在为每个QA对生成评价...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df44ae35539a4f8888b34a0cabe5767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"正在为每个QA对生成评价...\")\n",
    "for output in tqdm(outputs):\n",
    "    # 使用中文版的评价提示\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            zh_question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            zh_question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            zh_question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            # 从评价结果中提取评分和评价理由\n",
    "            # 对于中文评价，我们需要查找\"总评分:\"和\"评价理由:\"\n",
    "            if \"总评分:\" in evaluation:\n",
    "                score = int(evaluation.split(\"总评分:\")[-1].strip())\n",
    "                eval = evaluation.split(\"总评分:\")[0].split(\"评价理由:\")[-1].strip()\n",
    "            else:\n",
    "                # 如果找不到中文标识，回退到英文格式\n",
    "                score = int(evaluation.split(\"Total rating:\")[-1].strip())\n",
    "                eval = evaluation.split(\"Total rating:\")[-2].split(\"Evaluation:\")[-1].strip()\n",
    "                \n",
    "            # 更新输出字典，添加评分和评价\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"处理评价时出现错误: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6ebc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee42bcd1b70440549c09599f07b962ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\rag\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\k\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc_qa_eval. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b56bee5c34c440b9d4213cbdae67372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132ed164b5044eb095c5d12d0ee890a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3258fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4e07680e3942078df38f091e561480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    将文档分割成大小为 `chunk_size` 个字符的块，并返回文档列表。\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10), # 设置块重叠大小为块大小的10%\n",
    "        add_start_index=True, # 添加块在原始文档中的起始索引\n",
    "        strip_whitespace=True, # 去除块首尾的空白字符\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"], # 用于分割文本的分隔符列表\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base: # 遍历知识库中的每个文档\n",
    "        docs_processed += text_splitter.split_documents([doc]) # 将当前文档分割成块并添加到处理后的文档列表中\n",
    "\n",
    "    # 移除重复项\n",
    "    unique_texts = {} # 用于存储已处理过的唯一文本内容\n",
    "    docs_processed_unique = [] # 用于存储去重后的文档块\n",
    "    for doc in docs_processed: # 遍历所有处理后的文档块\n",
    "        if doc.page_content not in unique_texts: # 如果当前块的内容不在唯一文本字典中\n",
    "            unique_texts[doc.page_content] = True # 将当前块的内容添加到唯一文本字典中\n",
    "            docs_processed_unique.append(doc) # 将当前块添加到去重后的文档块列表中\n",
    "\n",
    "    return docs_processed_unique # 返回去重后的文档块列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6a11702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\bge-m3\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "zh_RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "请根据上下文信息，对问题给出全面回答。\n",
    "仅回答被问到的问题，回答应简洁并与问题相关。\n",
    "在相关时，请提供源文档的编号。\n",
    "如果无法从上下文中推断出答案，请不要给出答案。</s>\n",
    "<|user|>\n",
    "上下文:\n",
    "{context}\n",
    "---\n",
    "现在，这是你需要回答的问题。\n",
    "\n",
    "问题: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98561308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm_client: openai.OpenAI,\n",
    "    knowledge_index_path: str = None,\n",
    "    embedding_model: str = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\bge-m3\",\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    use_chinese: bool = False,\n",
    "    model_name: str = \"THUDM/GLM-4-32B-0414\"\n",
    ") -> Tuple[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    使用RAG方法回答问题，基于FAISS索引和Sentence Transformers\n",
    "    \n",
    "    Args:\n",
    "        question: 用户提出的问题\n",
    "        llm_client: OpenAI客户端实例\n",
    "        knowledge_index_path: FAISS索引的路径，如果为None则需要重新构建\n",
    "        embedding_model: 用于文本嵌入的sentence-transformers模型\n",
    "        num_retrieved_docs: 初始检索的文档数量\n",
    "        num_docs_final: 最终用于生成答案的文档数量\n",
    "        use_chinese: 是否使用中文提示模板\n",
    "        model_name: 大模型的名称\n",
    "        \n",
    "    Returns:\n",
    "        生成的答案文本和相关的检索文档列表\n",
    "    \"\"\"\n",
    "    # 初始化或加载文本嵌入模型\n",
    "    embed_model = SentenceTransformer(embedding_model)\n",
    "    \n",
    "    # 如果没有提供知识库索引路径，从RAW_KNOWLEDGE_BASE构建嵌入并创建索引\n",
    "    if knowledge_index_path is None:\n",
    "        # 从RAW_KNOWLEDGE_BASE中提取文档并进行分割\n",
    "        # 这里假设RAW_KNOWLEDGE_BASE已经被定义并包含了所有需要的文档\n",
    "        doc_texts = [doc.page_content for doc in docs_processed]\n",
    "        doc_embeddings = embed_model.encode(doc_texts, show_progress_bar=True, convert_to_tensor=True)\n",
    "        \n",
    "        # 创建FAISS索引\n",
    "        dimension = doc_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(doc_embeddings.cpu().numpy())\n",
    "    else:\n",
    "        # 从文件加载FAISS索引\n",
    "        index = faiss.read_index(knowledge_index_path)\n",
    "        doc_texts = [doc.page_content for doc in docs_processed]  # 假设docs_processed已经定义\n",
    "    \n",
    "    # 生成问题的嵌入\n",
    "    question_embedding = embed_model.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # 检索相关文档\n",
    "    distances, indices = index.search(question_embedding.cpu().numpy().reshape(1, -1), k=num_retrieved_docs)\n",
    "    retrieved_docs = [doc_texts[i] for i in indices[0]]\n",
    "    \n",
    "    # 限制最终使用的文档数量\n",
    "    relevant_docs = retrieved_docs[:num_docs_final]\n",
    "    \n",
    "    # 构建上下文\n",
    "    context = \"\\n检索到的文档:\\n\" if use_chinese else \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"文档 {str(i)}:::\\n\" + doc + \"\\n\\n\" if use_chinese \n",
    "                        else f\"Document {str(i)}:::\\n\" + doc + \"\\n\\n\" \n",
    "                        for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    # 根据语言选择提示模板\n",
    "    prompt_template = zh_RAG_PROMPT_TEMPLATE if use_chinese else RAG_PROMPT_TEMPLATE\n",
    "    final_prompt = prompt_template.format(question=question, context=context)\n",
    "\n",
    "    # 生成答案\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": final_prompt}\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # 返回答案和使用的文档\n",
    "    retrieved_doc_objects = [{\"page_content\": doc} for doc in relevant_docs]\n",
    "    return answer, retrieved_doc_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "982f65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset,\n",
    "    llm_client,\n",
    "    output_file=\"rag_test_results.json\",\n",
    "    embedding_model=r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\bge-m3\",\n",
    "    knowledge_index_path=None,\n",
    "    num_retrieved_docs=30,\n",
    "    num_docs_final=7,\n",
    "    verbose=True,\n",
    "    test_settings=None,\n",
    "    use_chinese=False,\n",
    "    model_name=\"THUDM/GLM-4-32B-0414\"\n",
    "):\n",
    "    \"\"\"\n",
    "    在给定数据集上运行RAG测试，并将结果保存到指定的输出文件。\n",
    "    \n",
    "    Args:\n",
    "        eval_dataset: 评估数据集\n",
    "        llm_client: 用于生成回答的OpenAI客户端\n",
    "        output_file: 保存结果的文件路径\n",
    "        embedding_model: 用于文本嵌入的模型路径\n",
    "        knowledge_index_path: FAISS索引的路径，如果为None则需要重新构建\n",
    "        num_retrieved_docs: 初始检索的文档数量\n",
    "        num_docs_final: 最终用于生成答案的文档数量\n",
    "        verbose: 是否打印详细输出\n",
    "        test_settings: 测试设置的描述信息\n",
    "        use_chinese: 是否使用中文提示模板\n",
    "        model_name: 大模型的名称\n",
    "    \"\"\"\n",
    "    try:  # 如果输出文件已存在，加载之前的生成结果\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        # 使用RAG回答问题\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question,\n",
    "            llm_client,\n",
    "            knowledge_index_path,\n",
    "            embedding_model=embedding_model,\n",
    "            num_retrieved_docs=num_retrieved_docs,\n",
    "            num_docs_final=num_docs_final,\n",
    "            use_chinese=use_chinese,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"问题: {question}\" if use_chinese else f\"Question: {question}\")\n",
    "            print(f\"回答: {answer}\" if use_chinese else f\"Answer: {answer}\")\n",
    "            print(f\"正确答案: {example['answer']}\" if use_chinese else f\"True answer: {example['answer']}\")\n",
    "            \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc[\"page_content\"] for doc in relevant_docs],  # 只保存文本内容\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        # 实时保存结果，以防程序中断\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, ensure_ascii=False, indent=2)  # 使用ensure_ascii=False以正确保存中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e36975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
