{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e24c4a58",
   "metadata": {},
   "source": [
    "## å®‰è£…ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c69674f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q matplotlib opencv-python torch transformers langchain sentence-transformers tqdm openpyxl openai pandas datasets langchain-community ragatouille ipywidgets openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b7819f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é‡æ–°åŠ è½½autoreloadæ‰©å±•ï¼Œä½¿ä»£ç ä¿®æ”¹åè‡ªåŠ¨ç”Ÿæ•ˆ\n",
    "%reload_ext autoreload\n",
    "# è®¾ç½®autoreloadæ¨¡å¼ä¸º2ï¼Œè‡ªåŠ¨é‡æ–°åŠ è½½æ‰€æœ‰å¯¼å…¥çš„æ¨¡å—\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a0e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import json\n",
    "import datasets\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "312ea9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37d46f8812541138607d2dac139f5c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed681b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92641539c984a21a89261a0eee527fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\rag\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\k\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bf7210041b4bc8a07a8b216030a5fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "huggingface_doc.csv:   0%|          | 0.00/22.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f5ea253bc248a1a28aed239b2efac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = datasets.load_dataset(\"m-ric/huggingface_doc\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed34d0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'source'],\n",
      "    num_rows: 2647\n",
      "})\n",
      "{'text': ' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget ğŸ ğŸ‰!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n', 'source': 'huggingface/hf-endpoints-documentation/blob/main/docs/source/guides/create_endpoint.mdx'}\n",
      "2013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Create an Endpoint\\n\\nAfter your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \\n\\n## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\\n\\n## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\\n\\n## 3. Define the [Security Level](security) for the Endpoint:\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\\n\\n## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\\n\\n## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\\n\\n## 6. Test your Endpoint in the overview with the Inference widget ğŸ ğŸ‰!\\n\\n<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ‰“å°æ•°æ®é›†ä¿¡æ¯ï¼ˆç‰¹å¾ï¼Œè¡Œæ•°ï¼‰\n",
    "print(ds)\n",
    "\n",
    "# æ‰“å°æ•°æ®é›†çš„ç¬¬ä¸€ä¸ªç¤ºä¾‹\n",
    "print(ds[0])\n",
    "\n",
    "\n",
    "# Dataset({\n",
    "#     features: ['text', 'source'],\n",
    "#     num_rows: 2647\n",
    "# })\n",
    "text = ds[0][\"text\"]\n",
    "print(len(text))\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d6f0606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2439ae0a294c4cfe99a3dff5bcd3f879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "langchain_docs = [LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)]\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "docs_processed = []\n",
    "for doc in langchain_docs:\n",
    "    docs_processed += text_splitter.split_documents([doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6e65d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGï¼ˆRetrieval-Augmented Generationï¼‰æ˜¯ä¸€ç§ç»“åˆäº†æ£€ç´¢å’Œç”ŸæˆæŠ€æœ¯çš„äººå·¥æ™ºèƒ½æ¨¡å‹æ¶æ„ï¼Œæ—¨åœ¨å¢å¼ºç”Ÿæˆæ¨¡å‹çš„èƒ½åŠ›ã€‚åœ¨æ ‡å‡†çš„ç”Ÿæˆæ¨¡å‹ä¸­ï¼Œæ¨¡å‹æ ¹æ®è¾“å…¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç”Ÿæˆæ–‡æœ¬ã€‚ç„¶è€Œï¼Œè¿™äº›æ¨¡å‹å¯èƒ½ç¼ºä¹å¯¹æœ€æ–°ä¿¡æ¯æˆ–ç‰¹å®šé¢†åŸŸçŸ¥è¯†çš„è®¿é—®ã€‚RAG é€šè¿‡æ£€ç´¢ç›¸å…³çš„å¤–éƒ¨çŸ¥è¯†æºï¼Œç„¶ååˆ©ç”¨è¿™äº›æ£€ç´¢åˆ°çš„ä¿¡æ¯æ¥ç”Ÿæˆæ›´å‡†ç¡®ã€æ›´ç›¸å…³çš„æ–‡æœ¬ã€‚\n",
      "\n",
      "ä»¥ä¸‹æ˜¯ RAG çš„å·¥ä½œæµç¨‹ï¼š\n",
      "\n",
      "1. **æ£€ç´¢ï¼ˆRetrievalï¼‰**ï¼šå½“è¾“å…¥ä¸€ä¸ªæŸ¥è¯¢æˆ–ä¸Šä¸‹æ–‡æ—¶ï¼ŒRAG é¦–å…ˆä»å¤–éƒ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³çš„æ–‡æ¡£æˆ–ä¿¡æ¯ã€‚è¿™é€šå¸¸é€šè¿‡åµŒå…¥æŠ€æœ¯å’Œç›¸ä¼¼åº¦åŒ¹é…æ¥å®ç°ï¼Œä¾‹å¦‚ä½¿ç”¨è¯åµŒå…¥æˆ–å¥å­åµŒå…¥æ¥æ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„æ–‡æ¡£ã€‚\n",
      "\n",
      "2. **ç”Ÿæˆï¼ˆGenerationï¼‰**ï¼šåœ¨æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£åï¼ŒRAG ä½¿ç”¨è¿™äº›æ–‡æ¡£ä½œä¸ºé¢å¤–çš„è¾“å…¥ï¼Œç»“åˆåŸå§‹æŸ¥è¯¢æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œç”Ÿæˆæ›´å‡†ç¡®å’Œç›¸å…³çš„æ–‡æœ¬ã€‚ç”Ÿæˆéƒ¨åˆ†é€šå¸¸ä½¿ç”¨é¢„è®­ç»ƒçš„ç”Ÿæˆæ¨¡å‹ï¼Œå¦‚ GPT æˆ– BERT çš„å˜ä½“ã€‚\n",
      "\n",
      "é€šè¿‡è¿™ç§æ–¹å¼ï¼ŒRAG èƒ½å¤Ÿåˆ©ç”¨å¤–éƒ¨çŸ¥è¯†åº“çš„ä¸°å¯Œä¿¡æ¯ï¼Œæé«˜ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚è¿™åœ¨éœ€è¦ç‰¹å®šé¢†åŸŸçŸ¥è¯†æˆ–æœ€æ–°ä¿¡æ¯çš„ä»»åŠ¡ä¸­ç‰¹åˆ«æœ‰ç”¨ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿã€æ–‡æ¡£æ‘˜è¦ã€å†…å®¹ç”Ÿæˆç­‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "# from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "# repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# llm_client = InferenceClient(\n",
    "#     model=repo_id,\n",
    "#     timeout=120,\n",
    "# )\n",
    "\n",
    "\n",
    "# def call_llm(inference_client: InferenceClient, prompt: str):\n",
    "#     response = inference_client.post(\n",
    "#         json={\n",
    "#             \"inputs\": prompt,\n",
    "#             \"parameters\": {\"max_new_tokens\": 1000},\n",
    "#             \"task\": \"text-generation\",\n",
    "#         },\n",
    "#     )\n",
    "#     return json.loads(response.decode())[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "# call_llm(llm_client, \"This is a test context\")\n",
    "import openai\n",
    "import os\n",
    "import dotenv\n",
    "# å»ºè®®å°† API å¯†é’¥å­˜å‚¨åœ¨ç¯å¢ƒå˜é‡ä¸­\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "model_name = \"THUDM/GLM-4-32B-0414\"\n",
    "\n",
    "# æˆ–è€…ç›´æ¥è®¾ç½®ï¼ˆä¸æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒï¼‰\n",
    "# openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "# åˆå§‹åŒ– OpenAI å®¢æˆ·ç«¯\n",
    "# å¦‚æœæ‚¨ä½¿ç”¨çš„æ˜¯ Azure OpenAIï¼Œè¯·æŒ‰å¦‚ä¸‹æ–¹å¼åˆå§‹åŒ–ï¼š\n",
    "llm_client = openai.OpenAI(\n",
    "    api_key=os.getenv(\"API_KEY\"),\n",
    "    base_url = os.getenv(\"BASE_URL\"),\n",
    ")\n",
    "# å¦åˆ™ï¼Œä½¿ç”¨é»˜è®¤çš„ OpenAI åˆå§‹åŒ–ï¼š\n",
    "# client = openai.OpenAI()\n",
    "\n",
    "\n",
    "\n",
    "def call_llm(openai_client: openai.OpenAI, prompt: str, model: str = \"THUDM/GLM-4-32B-0414\"):\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŒ‡å®šçš„ OpenAI æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ã€‚\n",
    "\n",
    "    Args:\n",
    "        openai_client: åˆå§‹åŒ–åçš„ OpenAI å®¢æˆ·ç«¯å®ä¾‹ã€‚\n",
    "        prompt: è¾“å…¥ç»™æ¨¡å‹çš„æç¤ºæ–‡æœ¬ã€‚\n",
    "        model: è¦ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼Œæ¯”å¦‚ï¼šTHUDM/GLM-4-32B-0414.\n",
    "\n",
    "    Returns:\n",
    "        æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ã€‚\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=4096, # æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "            temperature=0.7 # æ ¹æ®éœ€è¦è°ƒæ•´\n",
    "        )\n",
    "        # æ£€æŸ¥ response.choices æ˜¯å¦ä¸ºç©ºä»¥åŠç¬¬ä¸€ä¸ª choice æ˜¯å¦å­˜åœ¨\n",
    "        if response.choices and len(response.choices) > 0 and response.choices[0].message:\n",
    "             return response.choices[0].message.content.strip()\n",
    "        else:\n",
    "             print(\"Warning: OpenAI API returned an empty response or invalid structure.\")\n",
    "             return \"\" # æˆ–è€…å¯ä»¥æŠ›å‡ºå¼‚å¸¸\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while calling the OpenAI API: {e}\")\n",
    "        return \"\" # æˆ–è€…æ ¹æ®é”™è¯¯å¤„ç†ç­–ç•¥è¿”å› None æˆ–æŠ›å‡ºå¼‚å¸¸\n",
    "\n",
    "\n",
    "test_prompt = \"è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ RAGï¼Ÿ\"\n",
    "generated_text = call_llm(llm_client, test_prompt)\n",
    "print(generated_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a4e13ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Create an Endpoint\n",
      "\n",
      "After your first login, you will be directed to the [Endpoint creation page](https://ui.endpoints.huggingface.co/new). As an example, this guide will go through the steps to deploy [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) for text classification. \n",
      "\n",
      "## 1. Enter the Hugging Face Repository ID and your desired endpoint name:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png\" alt=\"select repository\" />\n",
      "\n",
      "## 2. Select your Cloud Provider and region. Initially, only AWS will be available as a Cloud Provider with the `us-east-1` and `eu-west-1` regions. We will add Azure soon, and if you need to test Endpoints with other Cloud Providers or regions, please let us know.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png\" alt=\"select region\" />\n",
      "\n",
      "## 3. Define the [Security Level](security) for the Endpoint:\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png\" alt=\"define security\" />\n",
      "\n",
      "## 4. Create your Endpoint by clicking **Create Endpoint**. By default, your Endpoint is created with a medium CPU (2 x 4GB vCPUs with Intel Xeon Ice Lake) The cost estimate assumes the Endpoint will be up for an entire month, and does not take autoscaling into account.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png\" alt=\"create endpoint\" />\n",
      "\n",
      "## 5. Wait for the Endpoint to build, initialize and run which can take between 1 to 5 minutes.\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png\" alt=\"overview\" />\n",
      "\n",
      "## 6. Test your Endpoint in the overview with the Inference widget ğŸ ğŸ‰!\n",
      "\n",
      "<img src=\"https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png\" alt=\"run inference\" />\n",
      "\n",
      "============================================================\n",
      "1216\n",
      "============================================================\n",
      "åˆ›å»ºç«¯ç‚¹\n",
      "\n",
      "é¦–æ¬¡ç™»å½•åï¼Œæ‚¨å°†è¢«å¼•å¯¼è‡³[ç«¯ç‚¹åˆ›å»ºé¡µé¢](https://ui.endpoints.huggingface.co/new)ã€‚ä½œä¸ºç¤ºä¾‹ï¼Œæœ¬æŒ‡å—å°†è¯¦ç»†ä»‹ç»éƒ¨ç½²[distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)è¿›è¡Œæ–‡æœ¬åˆ†ç±»çš„æ­¥éª¤ã€‚\n",
      "\n",
      "## 1. è¾“å…¥Hugging Faceä»“åº“IDå’Œæ‚¨æƒ³è¦çš„ç«¯ç‚¹åç§°ï¼š\n",
      "\n",
      "![é€‰æ‹©ä»“åº“](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_repository.png)\n",
      "\n",
      "## 2. é€‰æ‹©æ‚¨çš„äº‘æä¾›å•†å’Œåœ°åŒºã€‚æœ€åˆï¼Œåªæœ‰AWSå¯ç”¨ä½œä¸ºäº‘æä¾›å•†ï¼Œåœ°åŒºä¸º`us-east-1`å’Œ`eu-west-1`ã€‚æˆ‘ä»¬å¾ˆå¿«ä¼šæ·»åŠ Azureï¼Œå¦‚æœæ‚¨éœ€è¦æµ‹è¯•ä¸å…¶ä»–äº‘æä¾›å•†æˆ–åœ°åŒºçš„ç«¯ç‚¹ï¼Œè¯·å‘ŠçŸ¥æˆ‘ä»¬ã€‚\n",
      "\n",
      "![é€‰æ‹©åœ°åŒº](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_region.png)\n",
      "\n",
      "## 3. ä¸ºç«¯ç‚¹å®šä¹‰[å®‰å…¨çº§åˆ«](security)ï¼š\n",
      "\n",
      "![å®šä¹‰å®‰å…¨](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_security.png)\n",
      "\n",
      "## 4. é€šè¿‡ç‚¹å‡»**åˆ›å»ºç«¯ç‚¹**æ¥åˆ›å»ºæ‚¨çš„ç«¯ç‚¹ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‚¨çš„ç«¯ç‚¹ä½¿ç”¨ä¸­ç­‰CPUï¼ˆ2 x 4GB vCPUsï¼Œé…å¤‡Intel Xeon Ice Lakeï¼‰ã€‚æˆæœ¬ä¼°ç®—å‡è®¾ç«¯ç‚¹å°†è¿è¡Œæ•´ä¸ªæœˆï¼Œä¸”ä¸è€ƒè™‘è‡ªåŠ¨ç¼©æ”¾ã€‚\n",
      "\n",
      "![åˆ›å»ºç«¯ç‚¹å’Œæˆæœ¬](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_create_cost.png)\n",
      "\n",
      "## 5. ç­‰å¾…ç«¯ç‚¹æ„å»ºã€åˆå§‹åŒ–å’Œè¿è¡Œï¼Œè¿™å¯èƒ½éœ€è¦1åˆ°5åˆ†é’Ÿã€‚\n",
      "\n",
      "![æ¦‚è¿°](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/overview.png)\n",
      "\n",
      "## 6. ä½¿ç”¨æ¨ç†å°éƒ¨ä»¶åœ¨æ¦‚è¿°ä¸­æµ‹è¯•æ‚¨çš„ç«¯ç‚¹ï¼ğŸ ğŸ‰\n",
      "\n",
      "![è¿è¡Œæ¨ç†](https://raw.githubusercontent.com/huggingface/hf-endpoints-documentation/main/assets/1_inference.png)\n"
     ]
    }
   ],
   "source": [
    "zh_text = call_llm(llm_client, f\"ç¿»è¯‘ä¸‹é¢çš„å†…å®¹:\\n{text}\")\n",
    "print(text)\n",
    "print(\"===\" * 20)\n",
    "print(len(zh_text))\n",
    "print(\"===\" * 20)\n",
    "print(zh_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebd83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_generation_prompt = \"\"\"\n",
    "Your task is to write a factoid question and an answer given a context.\n",
    "Your factoid question should be answerable with a specific, concise piece of factual information from the context.\n",
    "Your factoid question should be formulated in the same style as questions users could ask in a search engine.\n",
    "This means that your factoid question MUST NOT mention something like \"according to the passage\" or \"context\".\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Output:::\n",
    "Factoid question: (your factoid question)\n",
    "Answer: (your answer to the factoid question)\n",
    "\n",
    "Now here is the context.\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\"\n",
    "\n",
    "ZH_QA_generation_prompt = \"\"\"\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸­æ–‡ç‰ˆæŒ‡ç¤ºï¼š\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡ç¼–å†™ä¸€ä¸ªäº‹å®æ€§é—®é¢˜åŠå…¶ç­”æ¡ˆã€‚\n",
    "ä½ çš„äº‹å®æ€§é—®é¢˜åº”å½“èƒ½å¤Ÿé€šè¿‡ä¸Šä¸‹æ–‡ä¸­çš„å…·ä½“ã€ç®€æ´çš„äº‹å®ä¿¡æ¯æ¥å›ç­”ã€‚\n",
    "ä½ çš„äº‹å®æ€§é—®é¢˜åº”å½“ä»¥ç”¨æˆ·åœ¨æœç´¢å¼•æ“ä¸­æé—®çš„é£æ ¼æ¥è¡¨è¿°ã€‚\n",
    "è¿™æ„å‘³ç€ä½ çš„äº‹å®æ€§é—®é¢˜ä¸èƒ½åŒ…å«\"æ ¹æ®æ–‡ç« \"æˆ–\"ä¸Šä¸‹æ–‡\"ç­‰è¡¨è¿°ã€‚\n",
    "\n",
    "è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼æä¾›ä½ çš„å›ç­”ï¼š\n",
    "\n",
    "Output:::\n",
    "Factoid question: (ä½ çš„äº‹å®æ€§é—®é¢˜)\n",
    "Answer: (ä½ å¯¹è¯¥äº‹å®æ€§é—®é¢˜çš„å›ç­”)\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸Šä¸‹æ–‡ï¼š\n",
    "\n",
    "Context: {context}\\n\n",
    "Output:::\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef574eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 10 QA couples...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe2b1bfd9f147dda0e15bb8eda79dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "N_GENERATIONS = 10  # æˆ‘ä»¬åœ¨è¿™é‡Œæœ‰æ„åªç”Ÿæˆ10ä¸ªQAå¯¹ï¼Œè€ƒè™‘åˆ°æˆæœ¬å’Œæ—¶é—´å› ç´ \n",
    "\n",
    "print(f\"ç”Ÿæˆ {N_GENERATIONS} ä¸ªQAå¯¹...\")\n",
    "\n",
    "outputs = []\n",
    "for sampled_context in tqdm(random.sample(docs_processed, N_GENERATIONS)):\n",
    "    # ç”ŸæˆQAå¯¹\n",
    "    output_QA_couple = call_llm(llm_client, QA_generation_prompt.format(context=sampled_context.page_content))\n",
    "    try:\n",
    "        question = output_QA_couple.split(\"Factoid question: \")[-1].split(\"Answer: \")[0]\n",
    "        answer = output_QA_couple.split(\"Answer: \")[-1]\n",
    "        assert len(answer) < 300, \"ç­”æ¡ˆå¤ªé•¿\"\n",
    "        outputs.append(\n",
    "            {\n",
    "                \"context\": sampled_context.page_content,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer,\n",
    "                \"source_doc\": sampled_context.metadata[\"source\"],\n",
    "            }\n",
    "        )\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bd7dc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source_doc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;Tip&gt;\\n\\nGuess mode does not have any impact on prompt conditioning and you can still provide a prompt if you want.\\n\\n&lt;/Tip&gt;\\n\\nSet `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) to set the `guidance_scale` value between 3.0 and 5.0.\\n\\n```py\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\nfrom diffusers.utils import load_image, make_image_grid\\nimport numpy as np\\nimport torch\\nfrom PIL import Image\\nimport cv2\\n\\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\\n\\noriginal_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png\")\\n\\nimage = np.array(original_image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)\\n\\nimage = pipe(\"\", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]\\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\\n```\\n\\n&lt;div class=\"flex gap-4\"&gt;\\n  &lt;div&gt;\\n    &lt;img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png\"/&gt;\\n    &lt;figcaption class=\"mt-2 text-center text-sm text-gray-500\"&gt;regular mode with prompt&lt;/figcaption&gt;\\n  &lt;/div&gt;\\n  &lt;div&gt;\\n    &lt;img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png\"/&gt;\\n    &lt;figcaption class=\"mt-2 text-center text-sm text-gray-500\"&gt;guess mode without prompt&lt;/figcaption&gt;\\n  &lt;/div&gt;\\n&lt;/div&gt;\\n\\n## ControlNet with Stable Diffusion XL</td>\n",
       "      <td>What is the recommended guidance scale value for guess mode in StableDiffusionControlNetPipeline?\\n</td>\n",
       "      <td>Between 3.0 and 5.0</td>\n",
       "      <td>huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 context  \\\n",
       "0  <Tip>\\n\\nGuess mode does not have any impact on prompt conditioning and you can still provide a prompt if you want.\\n\\n</Tip>\\n\\nSet `guess_mode=True` in the pipeline, and it is [recommended](https://github.com/lllyasviel/ControlNet#guess-mode--non-prompt-mode) to set the `guidance_scale` value between 3.0 and 5.0.\\n\\n```py\\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel\\nfrom diffusers.utils import load_image, make_image_grid\\nimport numpy as np\\nimport torch\\nfrom PIL import Image\\nimport cv2\\n\\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", use_safetensors=True)\\npipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, use_safetensors=True).to(\"cuda\")\\n\\noriginal_image = load_image(\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/bird_512x512.png\")\\n\\nimage = np.array(original_image)\\n\\nlow_threshold = 100\\nhigh_threshold = 200\\n\\nimage = cv2.Canny(image, low_threshold, high_threshold)\\nimage = image[:, :, None]\\nimage = np.concatenate([image, image, image], axis=2)\\ncanny_image = Image.fromarray(image)\\n\\nimage = pipe(\"\", image=canny_image, guess_mode=True, guidance_scale=3.0).images[0]\\nmake_image_grid([original_image, canny_image, image], rows=1, cols=3)\\n```\\n\\n<div class=\"flex gap-4\">\\n  <div>\\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0.png\"/>\\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">regular mode with prompt</figcaption>\\n  </div>\\n  <div>\\n    <img class=\"rounded-xl\" src=\"https://huggingface.co/takuma104/controlnet_dev/resolve/main/gen_compare_guess_mode/output_images/diffusers/output_bird_canny_0_gm.png\"/>\\n    <figcaption class=\"mt-2 text-center text-sm text-gray-500\">guess mode without prompt</figcaption>\\n  </div>\\n</div>\\n\\n## ControlNet with Stable Diffusion XL   \n",
       "\n",
       "                                                                                              question  \\\n",
       "0  What is the recommended guidance scale value for guess mode in StableDiffusionControlNetPipeline?\\n   \n",
       "\n",
       "                answer  \\\n",
       "0  Between 3.0 and 5.0   \n",
       "\n",
       "                                                                     source_doc  \n",
       "0  huggingface/diffusers/blob/main/docs/source/en/using-diffusers/controlnet.md  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame(outputs).head(1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eab277",
   "metadata": {},
   "source": [
    "## è®¾ç½®æ‰¹åˆ¤æ™ºèƒ½ä½“\n",
    "1. å…·ä½“æ€§ï¼ˆGroundednessï¼‰ï¼šé—®é¢˜æ˜¯å¦å¯ä»¥ä»ç»™å®šçš„ä¸Šä¸‹æ–‡ä¸­å¾—åˆ°å›ç­”ï¼Ÿ\n",
    "2. ç›¸å…³æ€§ï¼ˆRelevanceï¼‰ï¼šé—®é¢˜å¯¹ç”¨æˆ·æ˜¯å¦ç›¸å…³ï¼Ÿä¾‹å¦‚ï¼Œ\"transformers 4.29.1 å‘å¸ƒçš„æ—¥æœŸæ˜¯ä»€ä¹ˆï¼Ÿ\"å¯¹äº ML ç”¨æˆ·æ¥è¯´å¹¶ä¸ç›¸å…³ã€‚\n",
    "3. ç‹¬ç«‹ï¼ˆStand-aloneï¼‰ï¼šå¯¹äºä¸€ä¸ªå…·æœ‰é¢†åŸŸçŸ¥è¯†/äº’è”ç½‘è®¿é—®æƒé™çš„äººæ¥è¯´ï¼Œé—®é¢˜åœ¨æ²¡æœ‰ä»»ä½•ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹æ˜¯å¦å¯ä»¥ç†è§£ï¼Ÿä¸æ­¤ç›¸åçš„æ˜¯ï¼Œå¯¹äºä»ç‰¹å®šåšå®¢æ–‡ç« ç”Ÿæˆçš„é—®é¢˜æ¯”å¦‚â€è¿™ç¯‡æ–‡ç« ä¸­ä½¿ç”¨çš„å‡½æ•°æ˜¯ä»€ä¹ˆï¼Ÿâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_groundedness_critique_prompt = \"\"\"\n",
    "You will be given a context and a question.\n",
    "Your task is to provide a 'total rating' scoring how well one can answer the given question unambiguously with the given context.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not answerable at all given the context, and 5 means that the question is clearly and unambiguously answerable with the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here are the question and context.\n",
    "\n",
    "Question: {question}\\n\n",
    "Context: {context}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_relevance_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how useful this question can be to machine learning developers building NLP applications with the Hugging Face ecosystem.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question is not useful at all, and 5 means that the question is extremely useful.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "question_standalone_critique_prompt = \"\"\"\n",
    "You will be given a question.\n",
    "Your task is to provide a 'total rating' representing how context-independant this question is.\n",
    "Give your answer on a scale of 1 to 5, where 1 means that the question depends on additional information to be understood, and 5 means that the question makes sense by itself.\n",
    "For instance, if the question refers to a particular setting, like 'in the context' or 'in the document', the rating must be 1.\n",
    "The questions can contain obscure technical nouns or acronyms like Gradio, Hub, Hugging Face or Space and still be a 5: it must simply be clear to an operator with access to documentation what the question is about.\n",
    "\n",
    "For instance, \"What is the name of the checkpoint from which the ViT model is imported?\" should receive a 1, since there is an implicit mention of a context, thus the question is not independant from the context.\n",
    "\n",
    "Provide your answer as follows:\n",
    "\n",
    "Answer:::\n",
    "Evaluation: (your rationale for the rating, as a text)\n",
    "Total rating: (your rating, as a number between 1 and 5)\n",
    "\n",
    "You MUST provide values for 'Evaluation:' and 'Total rating:' in your answer.\n",
    "\n",
    "Now here is the question.\n",
    "\n",
    "Question: {question}\\n\n",
    "Answer::: \"\"\"\n",
    "\n",
    "# ä½œç”¨ï¼šè®© LLM åˆ¤æ–­æä¾›çš„é—®é¢˜æ˜¯å¦èƒ½ä»…æ ¹æ®ç»™å®šçš„ä¸Šä¸‹æ–‡æ˜ç¡®å›ç­”ã€‚\n",
    "# è¯„ä¼°ç»´åº¦ï¼šâ€œæ‰æ ¹æ€§â€ï¼ˆGroundednessï¼‰ã€‚\n",
    "# è¯„åˆ†æ ‡å‡†ï¼š1ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡å®Œå…¨æ— æ³•å›ç­”ï¼‰åˆ° 5ï¼ˆæ ¹æ®ä¸Šä¸‹æ–‡å¯ä»¥æ¸…æ™°æ˜ç¡®åœ°å›ç­”ï¼‰ã€‚\n",
    "\n",
    "zh_question_groundedness_critique_prompt = \"\"\"\n",
    "ä½ å°†å¾—åˆ°ä¸€ä¸ªä¸Šä¸‹æ–‡å’Œä¸€ä¸ªé—®é¢˜ã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æä¾›ä¸€ä¸ªâ€œæ€»è¯„åˆ†â€ï¼Œè¯„ä¼°åœ¨ç»™å®šä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹ï¼Œé—®é¢˜èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šå¾—åˆ°æ˜ç¡®çš„å›ç­”ã€‚\n",
    "è¯·æŒ‰1åˆ°5çš„ç­‰çº§è¯„åˆ†ï¼Œå…¶ä¸­1è¡¨ç¤ºæ ¹æ®ä¸Šä¸‹æ–‡å®Œå…¨æ— æ³•å›ç­”è¯¥é—®é¢˜ï¼Œ5è¡¨ç¤ºæ ¹æ®ä¸Šä¸‹æ–‡å¯ä»¥æ¸…æ™°æ˜ç¡®åœ°å›ç­”è¯¥é—®é¢˜ã€‚\n",
    "\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›ä½ çš„å›ç­”ï¼š\n",
    "\n",
    "å›ç­”:::\n",
    "è¯„ä»·ç†ç”±: (ä½ ç»™å‡ºè¯„åˆ†çš„ç†ç”±ï¼Œæ–‡æœ¬å½¢å¼)\n",
    "æ€»è¯„åˆ†: (ä½ çš„è¯„åˆ†ï¼Œ1åˆ°5ä¹‹é—´çš„æ•°å­—)\n",
    "\n",
    "ä½ å¿…é¡»åœ¨å›ç­”ä¸­æä¾›â€œè¯„ä»·ç†ç”±:â€å’Œâ€œæ€»è¯„åˆ†:â€çš„å€¼ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯é—®é¢˜å’Œä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "é—®é¢˜: {question}\\n\n",
    "ä¸Šä¸‹æ–‡: {context}\\n\n",
    "å›ç­”::: \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# ä½œç”¨ï¼šè®© LLM åˆ¤æ–­æä¾›çš„é—®é¢˜å¯¹äºä½¿ç”¨ Hugging Face ç”Ÿæ€ç³»ç»Ÿæ„å»º NLP åº”ç”¨çš„æœºå™¨å­¦ä¹ å¼€å‘è€…æ¥è¯´æ˜¯å¦æœ‰ç”¨ã€‚\n",
    "# è¯„ä¼°ç»´åº¦ï¼šâ€œç›¸å…³æ€§â€ï¼ˆRelevanceï¼‰ã€‚\n",
    "# è¯„åˆ†æ ‡å‡†ï¼š1ï¼ˆè¯¥é—®é¢˜å®Œå…¨æ²¡æœ‰ç”¨å¤„ï¼‰åˆ° 5ï¼ˆè¯¥é—®é¢˜éå¸¸æœ‰ç”¨ï¼‰ã€‚\n",
    "\n",
    "zh_question_relevance_critique_prompt = \"\"\"\n",
    "ä½ å°†å¾—åˆ°ä¸€ä¸ªé—®é¢˜ã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æä¾›ä¸€ä¸ªâ€œæ€»è¯„åˆ†â€ï¼Œä»£è¡¨è¿™ä¸ªé—®é¢˜å¯¹äºä½¿ç”¨Hugging Faceç”Ÿæ€ç³»ç»Ÿæ„å»ºNLPåº”ç”¨çš„æœºå™¨å­¦ä¹ å¼€å‘è€…æœ‰å¤šå¤§ç”¨å¤„ã€‚\n",
    "è¯·æŒ‰1åˆ°5çš„ç­‰çº§è¯„åˆ†ï¼Œå…¶ä¸­1è¡¨ç¤ºè¯¥é—®é¢˜å®Œå…¨æ²¡æœ‰ç”¨å¤„ï¼Œ5è¡¨ç¤ºè¯¥é—®é¢˜éå¸¸æœ‰ç”¨ã€‚\n",
    "\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›ä½ çš„å›ç­”ï¼š\n",
    "\n",
    "å›ç­”:::\n",
    "è¯„ä»·ç†ç”±: (ä½ ç»™å‡ºè¯„åˆ†çš„ç†ç”±ï¼Œæ–‡æœ¬å½¢å¼)\n",
    "æ€»è¯„åˆ†: (ä½ çš„è¯„åˆ†ï¼Œ1åˆ°5ä¹‹é—´çš„æ•°å­—)\n",
    "\n",
    "ä½ å¿…é¡»åœ¨å›ç­”ä¸­æä¾›â€œè¯„ä»·ç†ç”±:â€å’Œâ€œæ€»è¯„åˆ†:â€çš„å€¼ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯é—®é¢˜ã€‚\n",
    "\n",
    "é—®é¢˜: {question}\\n\n",
    "å›ç­”::: \"\"\"\n",
    "\n",
    "# ä½œç”¨ï¼šè®© LLM åˆ¤æ–­æä¾›çš„é—®é¢˜æ˜¯å¦èƒ½åœ¨è„±ç¦»åŸå§‹ä¸Šä¸‹æ–‡çš„æƒ…å†µä¸‹è¢«ç†è§£ã€‚\n",
    "# è¯„ä¼°ç»´åº¦ï¼šâ€œç‹¬ç«‹æ€§â€ï¼ˆStand-aloneï¼‰ã€‚\n",
    "# è¯„åˆ†æ ‡å‡†ï¼š1ï¼ˆè¯¥é—®é¢˜éœ€è¦ä¾èµ–é¢å¤–ä¿¡æ¯æ‰èƒ½è¢«ç†è§£ï¼‰åˆ° 5ï¼ˆè¯¥é—®é¢˜æœ¬èº«å°±æœ‰æ„ä¹‰ï¼‰ã€‚ç‰¹åˆ«æ³¨æ„ï¼Œå³ä½¿é—®é¢˜åŒ…å«ä¸“ä¸šæœ¯è¯­ï¼Œåªè¦èƒ½é€šè¿‡æŸ¥é˜…æ–‡æ¡£ç†è§£ï¼Œä¹Ÿå¯è¯„ä¸º 5 åˆ†ï¼›ä½†å¦‚æœé—®é¢˜ä¸­éšå«æåŠâ€œä¸Šä¸‹æ–‡â€æˆ–â€œæ–‡æ¡£ä¸­â€ï¼Œåˆ™å¿…é¡»è¯„ä¸º 1 åˆ†ã€‚\n",
    "\n",
    "zh_question_standalone_critique_prompt = \"\"\"\n",
    "ä½ å°†å¾—åˆ°ä¸€ä¸ªé—®é¢˜ã€‚\n",
    "ä½ çš„ä»»åŠ¡æ˜¯æä¾›ä¸€ä¸ªâ€œæ€»è¯„åˆ†â€ï¼Œä»£è¡¨è¿™ä¸ªé—®é¢˜åœ¨å¤šå¤§ç¨‹åº¦ä¸Šæ˜¯ç‹¬ç«‹äºä¸Šä¸‹æ–‡çš„ã€‚\n",
    "è¯·æŒ‰1åˆ°5çš„ç­‰çº§è¯„åˆ†ï¼Œå…¶ä¸­1è¡¨ç¤ºè¯¥é—®é¢˜éœ€è¦ä¾èµ–é¢å¤–ä¿¡æ¯æ‰èƒ½è¢«ç†è§£ï¼Œ5è¡¨ç¤ºè¯¥é—®é¢˜æœ¬èº«å°±æœ‰æ„ä¹‰ã€‚\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœé—®é¢˜æåˆ°äº†ç‰¹å®šçš„ç¯å¢ƒï¼Œå¦‚â€œåœ¨ä¸Šä¸‹æ–‡ä¸­â€æˆ–â€œåœ¨æ–‡æ¡£ä¸­â€ï¼Œè¯„åˆ†å¿…é¡»ä¸º1ã€‚\n",
    "é—®é¢˜å¯ä»¥åŒ…å«åƒGradioã€Hubã€Hugging Faceæˆ–Spaceè¿™æ ·æ™¦æ¶©çš„æŠ€æœ¯åè¯æˆ–ç¼©å†™ï¼Œå¹¶ä¸”ä»ç„¶å¯ä»¥è¯„ä¸º5åˆ†ï¼šåªè¦å¯¹äºä¸€ä¸ªèƒ½å¤ŸæŸ¥é˜…æ–‡æ¡£çš„æ“ä½œå‘˜æ¥è¯´ï¼Œé—®é¢˜çš„å«ä¹‰æ˜¯æ¸…æ™°çš„å³å¯ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œâ€œViTæ¨¡å‹æ˜¯ä»å“ªä¸ªæ£€æŸ¥ç‚¹å¯¼å…¥çš„ï¼Ÿâ€åº”è¯„ä¸º1åˆ†ï¼Œå› ä¸ºå®ƒéšå«åœ°æåˆ°äº†ä¸€ä¸ªä¸Šä¸‹æ–‡ï¼Œå› æ­¤é—®é¢˜å¹¶éç‹¬ç«‹äºä¸Šä¸‹æ–‡ã€‚\n",
    "\n",
    "è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æä¾›ä½ çš„å›ç­”ï¼š\n",
    "\n",
    "å›ç­”:::\n",
    "è¯„ä»·ç†ç”±: (ä½ ç»™å‡ºè¯„åˆ†çš„ç†ç”±ï¼Œæ–‡æœ¬å½¢å¼)\n",
    "æ€»è¯„åˆ†: (ä½ çš„è¯„åˆ†ï¼Œ1åˆ°5ä¹‹é—´çš„æ•°å­—)\n",
    "\n",
    "ä½ å¿…é¡»åœ¨å›ç­”ä¸­æä¾›â€œè¯„ä»·ç†ç”±:â€å’Œâ€œæ€»è¯„åˆ†:â€çš„å€¼ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯é—®é¢˜ã€‚\n",
    "\n",
    "é—®é¢˜: {question}\\n\n",
    "å›ç­”::: \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74a33b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨ä¸ºæ¯ä¸ªQAå¯¹ç”Ÿæˆè¯„ä»·...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df44ae35539a4f8888b34a0cabe5767e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"æ­£åœ¨ä¸ºæ¯ä¸ªQAå¯¹ç”Ÿæˆè¯„ä»·...\")\n",
    "for output in tqdm(outputs):\n",
    "    # ä½¿ç”¨ä¸­æ–‡ç‰ˆçš„è¯„ä»·æç¤º\n",
    "    evaluations = {\n",
    "        \"groundedness\": call_llm(\n",
    "            llm_client,\n",
    "            zh_question_groundedness_critique_prompt.format(context=output[\"context\"], question=output[\"question\"]),\n",
    "        ),\n",
    "        \"relevance\": call_llm(\n",
    "            llm_client,\n",
    "            zh_question_relevance_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "        \"standalone\": call_llm(\n",
    "            llm_client,\n",
    "            zh_question_standalone_critique_prompt.format(question=output[\"question\"]),\n",
    "        ),\n",
    "    }\n",
    "    try:\n",
    "        for criterion, evaluation in evaluations.items():\n",
    "            # ä»è¯„ä»·ç»“æœä¸­æå–è¯„åˆ†å’Œè¯„ä»·ç†ç”±\n",
    "            # å¯¹äºä¸­æ–‡è¯„ä»·ï¼Œæˆ‘ä»¬éœ€è¦æŸ¥æ‰¾\"æ€»è¯„åˆ†:\"å’Œ\"è¯„ä»·ç†ç”±:\"\n",
    "            if \"æ€»è¯„åˆ†:\" in evaluation:\n",
    "                score = int(evaluation.split(\"æ€»è¯„åˆ†:\")[-1].strip())\n",
    "                eval = evaluation.split(\"æ€»è¯„åˆ†:\")[0].split(\"è¯„ä»·ç†ç”±:\")[-1].strip()\n",
    "            else:\n",
    "                # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡æ ‡è¯†ï¼Œå›é€€åˆ°è‹±æ–‡æ ¼å¼\n",
    "                score = int(evaluation.split(\"Total rating:\")[-1].strip())\n",
    "                eval = evaluation.split(\"Total rating:\")[-2].split(\"Evaluation:\")[-1].strip()\n",
    "                \n",
    "            # æ›´æ–°è¾“å‡ºå­—å…¸ï¼Œæ·»åŠ è¯„åˆ†å’Œè¯„ä»·\n",
    "            output.update(\n",
    "                {\n",
    "                    f\"{criterion}_score\": score,\n",
    "                    f\"{criterion}_eval\": eval,\n",
    "                }\n",
    "            )\n",
    "    except Exception as e:\n",
    "        print(f\"å¤„ç†è¯„ä»·æ—¶å‡ºç°é”™è¯¯: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a6ebc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee42bcd1b70440549c09599f07b962ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/893 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\rag\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\k\\.cache\\huggingface\\hub\\datasets--m-ric--huggingface_doc_qa_eval. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b56bee5c34c440b9d4213cbdae67372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/289k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132ed164b5044eb095c5d12d0ee890a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/65 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_dataset = datasets.load_dataset(\"m-ric/huggingface_doc_qa_eval\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd3258fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4e07680e3942078df38f091e561480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2647 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "\n",
    "RAW_KNOWLEDGE_BASE = [\n",
    "    LangchainDocument(page_content=doc[\"text\"], metadata={\"source\": doc[\"source\"]}) for doc in tqdm(ds)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c05411ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "def split_documents(\n",
    "    chunk_size: int,\n",
    "    knowledge_base: List[LangchainDocument],\n",
    "    tokenizer_name: str,\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"\n",
    "    å°†æ–‡æ¡£åˆ†å‰²æˆå¤§å°ä¸º `chunk_size` ä¸ªå­—ç¬¦çš„å—ï¼Œå¹¶è¿”å›æ–‡æ¡£åˆ—è¡¨ã€‚\n",
    "    \"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=int(chunk_size / 10), # è®¾ç½®å—é‡å å¤§å°ä¸ºå—å¤§å°çš„10%\n",
    "        add_start_index=True, # æ·»åŠ å—åœ¨åŸå§‹æ–‡æ¡£ä¸­çš„èµ·å§‹ç´¢å¼•\n",
    "        strip_whitespace=True, # å»é™¤å—é¦–å°¾çš„ç©ºç™½å­—ç¬¦\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"], # ç”¨äºåˆ†å‰²æ–‡æœ¬çš„åˆ†éš”ç¬¦åˆ—è¡¨\n",
    "    )\n",
    "\n",
    "    docs_processed = []\n",
    "    for doc in knowledge_base: # éå†çŸ¥è¯†åº“ä¸­çš„æ¯ä¸ªæ–‡æ¡£\n",
    "        docs_processed += text_splitter.split_documents([doc]) # å°†å½“å‰æ–‡æ¡£åˆ†å‰²æˆå—å¹¶æ·»åŠ åˆ°å¤„ç†åçš„æ–‡æ¡£åˆ—è¡¨ä¸­\n",
    "\n",
    "    # ç§»é™¤é‡å¤é¡¹\n",
    "    unique_texts = {} # ç”¨äºå­˜å‚¨å·²å¤„ç†è¿‡çš„å”¯ä¸€æ–‡æœ¬å†…å®¹\n",
    "    docs_processed_unique = [] # ç”¨äºå­˜å‚¨å»é‡åçš„æ–‡æ¡£å—\n",
    "    for doc in docs_processed: # éå†æ‰€æœ‰å¤„ç†åçš„æ–‡æ¡£å—\n",
    "        if doc.page_content not in unique_texts: # å¦‚æœå½“å‰å—çš„å†…å®¹ä¸åœ¨å”¯ä¸€æ–‡æœ¬å­—å…¸ä¸­\n",
    "            unique_texts[doc.page_content] = True # å°†å½“å‰å—çš„å†…å®¹æ·»åŠ åˆ°å”¯ä¸€æ–‡æœ¬å­—å…¸ä¸­\n",
    "            docs_processed_unique.append(doc) # å°†å½“å‰å—æ·»åŠ åˆ°å»é‡åçš„æ–‡æ¡£å—åˆ—è¡¨ä¸­\n",
    "\n",
    "    return docs_processed_unique # è¿”å›å»é‡åçš„æ–‡æ¡£å—åˆ—è¡¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6a11702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "import os\n",
    "\n",
    "\n",
    "def load_embeddings(\n",
    "    langchain_docs: List[LangchainDocument],\n",
    "    chunk_size: int,\n",
    "    embedding_model_name: Optional[str] = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\bge-m3\",\n",
    ") -> FAISS:\n",
    "    \"\"\"\n",
    "    Creates a FAISS index from the given embedding model and documents. Loads the index directly if it already exists.\n",
    "\n",
    "    Args:\n",
    "        langchain_docs: list of documents\n",
    "        chunk_size: size of the chunks to split the documents into\n",
    "        embedding_model_name: name of the embedding model to use\n",
    "\n",
    "    Returns:\n",
    "        FAISS index\n",
    "    \"\"\"\n",
    "    # load embedding_model\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=embedding_model_name,\n",
    "        multi_process=True,\n",
    "        model_kwargs={\"device\": \"cuda\"},\n",
    "        encode_kwargs={\"normalize_embeddings\": True},  # set True to compute cosine similarity\n",
    "    )\n",
    "\n",
    "    # Check if embeddings already exist on disk\n",
    "    index_name = f\"index_chunk:{chunk_size}_embeddings:{embedding_model_name.replace('/', '~')}\"\n",
    "    index_folder_path = f\"./data/indexes/{index_name}/\"\n",
    "    if os.path.isdir(index_folder_path):\n",
    "        return FAISS.load_local(\n",
    "            index_folder_path,\n",
    "            embedding_model,\n",
    "            distance_strategy=DistanceStrategy.COSINE,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        print(\"Index not found, generating it...\")\n",
    "        docs_processed = split_documents(\n",
    "            chunk_size,\n",
    "            langchain_docs,\n",
    "            embedding_model_name,\n",
    "        )\n",
    "        knowledge_index = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
    "        )\n",
    "        knowledge_index.save_local(index_folder_path)\n",
    "        return knowledge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126eb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "Using the information contained in the context,\n",
    "give a comprehensive answer to the question.\n",
    "Respond only to the question asked, response should be concise and relevant to the question.\n",
    "Provide the number of the source document when relevant.\n",
    "If the answer cannot be deduced from the context, do not give an answer.</s>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Now here is the question you need to answer.\n",
    "\n",
    "Question: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "zh_RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "<|system|>\n",
    "è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå¯¹é—®é¢˜ç»™å‡ºå…¨é¢å›ç­”ã€‚\n",
    "ä»…å›ç­”è¢«é—®åˆ°çš„é—®é¢˜ï¼Œå›ç­”åº”ç®€æ´å¹¶ä¸é—®é¢˜ç›¸å…³ã€‚\n",
    "åœ¨ç›¸å…³æ—¶ï¼Œè¯·æä¾›æºæ–‡æ¡£çš„ç¼–å·ã€‚\n",
    "å¦‚æœæ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ¨æ–­å‡ºç­”æ¡ˆï¼Œè¯·ä¸è¦ç»™å‡ºç­”æ¡ˆã€‚</s>\n",
    "<|user|>\n",
    "ä¸Šä¸‹æ–‡:\n",
    "{context}\n",
    "---\n",
    "ç°åœ¨ï¼Œè¿™æ˜¯ä½ éœ€è¦å›ç­”çš„é—®é¢˜ã€‚\n",
    "\n",
    "é—®é¢˜: {question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98561308",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm_client: openai.OpenAI,\n",
    "    knowledge_index_path: str = None,\n",
    "    embedding_model: str = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\bge-m3\",\n",
    "    num_retrieved_docs: int = 30,\n",
    "    num_docs_final: int = 7,\n",
    "    use_chinese: bool = False,\n",
    "    model_name: str = \"THUDM/GLM-4-32B-0414\"\n",
    ") -> Tuple[str, List[dict]]:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨RAGæ–¹æ³•å›ç­”é—®é¢˜ï¼ŒåŸºäºFAISSç´¢å¼•å’ŒSentence Transformers\n",
    "    \n",
    "    Args:\n",
    "        question: ç”¨æˆ·æå‡ºçš„é—®é¢˜\n",
    "        llm_client: OpenAIå®¢æˆ·ç«¯å®ä¾‹\n",
    "        knowledge_index_path: FAISSç´¢å¼•çš„è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™éœ€è¦é‡æ–°æ„å»º\n",
    "        embedding_model: ç”¨äºæ–‡æœ¬åµŒå…¥çš„sentence-transformersæ¨¡å‹\n",
    "        num_retrieved_docs: åˆå§‹æ£€ç´¢çš„æ–‡æ¡£æ•°é‡\n",
    "        num_docs_final: æœ€ç»ˆç”¨äºç”Ÿæˆç­”æ¡ˆçš„æ–‡æ¡£æ•°é‡\n",
    "        use_chinese: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æç¤ºæ¨¡æ¿\n",
    "        model_name: å¤§æ¨¡å‹çš„åç§°\n",
    "        \n",
    "    Returns:\n",
    "        ç”Ÿæˆçš„ç­”æ¡ˆæ–‡æœ¬å’Œç›¸å…³çš„æ£€ç´¢æ–‡æ¡£åˆ—è¡¨\n",
    "    \"\"\"\n",
    "    # åˆå§‹åŒ–æˆ–åŠ è½½æ–‡æœ¬åµŒå…¥æ¨¡å‹\n",
    "    embed_model = SentenceTransformer(embedding_model)\n",
    "    \n",
    "    # å¦‚æœæ²¡æœ‰æä¾›çŸ¥è¯†åº“ç´¢å¼•è·¯å¾„ï¼Œä»RAW_KNOWLEDGE_BASEæ„å»ºåµŒå…¥å¹¶åˆ›å»ºç´¢å¼•\n",
    "    if knowledge_index_path is None:\n",
    "        # ä»RAW_KNOWLEDGE_BASEä¸­æå–æ–‡æ¡£å¹¶è¿›è¡Œåˆ†å‰²\n",
    "        # è¿™é‡Œå‡è®¾RAW_KNOWLEDGE_BASEå·²ç»è¢«å®šä¹‰å¹¶åŒ…å«äº†æ‰€æœ‰éœ€è¦çš„æ–‡æ¡£\n",
    "        doc_texts = [doc.page_content for doc in docs_processed]\n",
    "        doc_embeddings = embed_model.encode(doc_texts, show_progress_bar=True, convert_to_tensor=True)\n",
    "        \n",
    "        # åˆ›å»ºFAISSç´¢å¼•\n",
    "        dimension = doc_embeddings.shape[1]\n",
    "        index = faiss.IndexFlatL2(dimension)\n",
    "        index.add(doc_embeddings.cpu().numpy())\n",
    "    else:\n",
    "        # ä»æ–‡ä»¶åŠ è½½FAISSç´¢å¼•\n",
    "        index = faiss.read_index(knowledge_index_path)\n",
    "        doc_texts = [doc.page_content for doc in docs_processed]  # å‡è®¾docs_processedå·²ç»å®šä¹‰\n",
    "    \n",
    "    # ç”Ÿæˆé—®é¢˜çš„åµŒå…¥\n",
    "    question_embedding = embed_model.encode(question, convert_to_tensor=True)\n",
    "    \n",
    "    # æ£€ç´¢ç›¸å…³æ–‡æ¡£\n",
    "    distances, indices = index.search(question_embedding.cpu().numpy().reshape(1, -1), k=num_retrieved_docs)\n",
    "    retrieved_docs = [doc_texts[i] for i in indices[0]]\n",
    "    \n",
    "    # é™åˆ¶æœ€ç»ˆä½¿ç”¨çš„æ–‡æ¡£æ•°é‡\n",
    "    relevant_docs = retrieved_docs[:num_docs_final]\n",
    "    \n",
    "    # æ„å»ºä¸Šä¸‹æ–‡\n",
    "    context = \"\\næ£€ç´¢åˆ°çš„æ–‡æ¡£:\\n\" if use_chinese else \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"æ–‡æ¡£ {str(i)}:::\\n\" + doc + \"\\n\\n\" if use_chinese \n",
    "                        else f\"Document {str(i)}:::\\n\" + doc + \"\\n\\n\" \n",
    "                        for i, doc in enumerate(relevant_docs)])\n",
    "\n",
    "    # æ ¹æ®è¯­è¨€é€‰æ‹©æç¤ºæ¨¡æ¿\n",
    "    prompt_template = zh_RAG_PROMPT_TEMPLATE if use_chinese else RAG_PROMPT_TEMPLATE\n",
    "    final_prompt = prompt_template.format(question=question, context=context)\n",
    "\n",
    "    # ç”Ÿæˆç­”æ¡ˆ\n",
    "    response = llm_client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": final_prompt}\n",
    "        ],\n",
    "        max_tokens=1000,\n",
    "        temperature=0.1\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    \n",
    "    # è¿”å›ç­”æ¡ˆå’Œä½¿ç”¨çš„æ–‡æ¡£\n",
    "    retrieved_doc_objects = [{\"page_content\": doc} for doc in relevant_docs]\n",
    "    return answer, retrieved_doc_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "982f65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def run_rag_tests(\n",
    "    eval_dataset,\n",
    "    llm_client,\n",
    "    output_file=\"rag_test_results.json\",\n",
    "    embedding_model=r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\bge-m3\",\n",
    "    knowledge_index_path=None,\n",
    "    num_retrieved_docs=30,\n",
    "    num_docs_final=7,\n",
    "    verbose=True,\n",
    "    test_settings=None,\n",
    "    use_chinese=False,\n",
    "    model_name=\"THUDM/GLM-4-32B-0414\"\n",
    "):\n",
    "    \"\"\"\n",
    "    åœ¨ç»™å®šæ•°æ®é›†ä¸Šè¿è¡ŒRAGæµ‹è¯•ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°æŒ‡å®šçš„è¾“å‡ºæ–‡ä»¶ã€‚\n",
    "    \n",
    "    Args:\n",
    "        eval_dataset: è¯„ä¼°æ•°æ®é›†\n",
    "        llm_client: ç”¨äºç”Ÿæˆå›ç­”çš„OpenAIå®¢æˆ·ç«¯\n",
    "        output_file: ä¿å­˜ç»“æœçš„æ–‡ä»¶è·¯å¾„\n",
    "        embedding_model: ç”¨äºæ–‡æœ¬åµŒå…¥çš„æ¨¡å‹è·¯å¾„\n",
    "        knowledge_index_path: FAISSç´¢å¼•çš„è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™éœ€è¦é‡æ–°æ„å»º\n",
    "        num_retrieved_docs: åˆå§‹æ£€ç´¢çš„æ–‡æ¡£æ•°é‡\n",
    "        num_docs_final: æœ€ç»ˆç”¨äºç”Ÿæˆç­”æ¡ˆçš„æ–‡æ¡£æ•°é‡\n",
    "        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†è¾“å‡º\n",
    "        test_settings: æµ‹è¯•è®¾ç½®çš„æè¿°ä¿¡æ¯\n",
    "        use_chinese: æ˜¯å¦ä½¿ç”¨ä¸­æ–‡æç¤ºæ¨¡æ¿\n",
    "        model_name: å¤§æ¨¡å‹çš„åç§°\n",
    "    \"\"\"\n",
    "    try:  # å¦‚æœè¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½ä¹‹å‰çš„ç”Ÿæˆç»“æœ\n",
    "        with open(output_file, \"r\") as f:\n",
    "            outputs = json.load(f)\n",
    "    except:\n",
    "        outputs = []\n",
    "\n",
    "    for example in tqdm(eval_dataset):\n",
    "        question = example[\"question\"]\n",
    "        if question in [output[\"question\"] for output in outputs]:\n",
    "            continue\n",
    "\n",
    "        # ä½¿ç”¨RAGå›ç­”é—®é¢˜\n",
    "        answer, relevant_docs = answer_with_rag(\n",
    "            question,\n",
    "            llm_client,\n",
    "            knowledge_index_path,\n",
    "            embedding_model=embedding_model,\n",
    "            num_retrieved_docs=num_retrieved_docs,\n",
    "            num_docs_final=num_docs_final,\n",
    "            use_chinese=use_chinese,\n",
    "            model_name=model_name\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"=======================================================\")\n",
    "            print(f\"é—®é¢˜: {question}\" if use_chinese else f\"Question: {question}\")\n",
    "            print(f\"å›ç­”: {answer}\" if use_chinese else f\"Answer: {answer}\")\n",
    "            print(f\"æ­£ç¡®ç­”æ¡ˆ: {example['answer']}\" if use_chinese else f\"True answer: {example['answer']}\")\n",
    "            \n",
    "        result = {\n",
    "            \"question\": question,\n",
    "            \"true_answer\": example[\"answer\"],\n",
    "            \"source_doc\": example[\"source_doc\"],\n",
    "            \"generated_answer\": answer,\n",
    "            \"retrieved_docs\": [doc[\"page_content\"] for doc in relevant_docs],  # åªä¿å­˜æ–‡æœ¬å†…å®¹\n",
    "        }\n",
    "        if test_settings:\n",
    "            result[\"test_settings\"] = test_settings\n",
    "        outputs.append(result)\n",
    "\n",
    "        # å®æ—¶ä¿å­˜ç»“æœï¼Œä»¥é˜²ç¨‹åºä¸­æ–­\n",
    "        with open(output_file, \"w\") as f:\n",
    "            json.dump(outputs, f, ensure_ascii=False, indent=2)  # ä½¿ç”¨ensure_ascii=Falseä»¥æ­£ç¡®ä¿å­˜ä¸­æ–‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e36975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
