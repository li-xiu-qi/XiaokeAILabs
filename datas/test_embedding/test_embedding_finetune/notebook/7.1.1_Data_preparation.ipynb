{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备用于微调"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "在本教程中，我们将展示微调的第一个步骤：数据集准备的示例。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 安装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调准备\n",
    "\n",
    "假设我们希望对金融任务进行模型微调。我们找到了一个可能有用的开源数据集：[financial-qa-10k](https://huggingface.co/datasets/virattt/financial-qa-10K)。让我们看看如何正确准备数据集用于微调。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原始数据集具有以下结构：\n",
    "- 5个列：'question'（问题），'answer'（回答），'context'（上下文），'ticker'（股票代码），和'filing'（申报文件）。\n",
    "- 7000行数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\XiaokeAILabs\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\k\\.conda\\envs\\XiaokeAILabs\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\k\\.cache\\huggingface\\hub\\datasets--virattt--financial-qa-10K. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 7000/7000 [00:00<00:00, 57538.19 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'context', 'ticker', 'filing'],\n",
       "    num_rows: 7000\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"virattt/financial-qa-10K\", split=\"train\")\n",
    "ds\n",
    "\n",
    "# 数据格式说明：\n",
    "# - question: 金融相关问题，例如关于公司财务状况、业务运营等的问询\n",
    "# - answer: 对问题的回答，通常摘自公司的财务报告\n",
    "# - context: 问题的背景信息，通常是从财务文件中提取的原始文本段落\n",
    "# - ticker: 股票代码，标识相关公司的股票市场符号\n",
    "# - filing: 财务申报文件信息，如10-K（年度报告）或10-Q（季度报告）的引用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 微调用数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建符合以下格式的数据集：\n",
    "\n",
    "``` python\n",
    "{\"query\": str, \"pos\": List[str], \"neg\":List[str], \"pos_scores\": List[int], \"neg_scores\": List[int], \"prompt\": str, \"type\": str}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`query` 是查询语句，`pos` 是一个正向文本列表，`neg` 是一个负向文本列表。`pos_scores` 是对应查询和正向文本的分数列表，`neg_scores` 是对应查询和负向文本的分数列表，如果你不使用知识蒸馏，可以忽略这两项。`prompt` 是用于查询的提示语，它会覆盖查询检索指令。`type` 用于 bge-en-icl，包括 `normal`、`symmetric_class`、`symmetric_clustering` 等类型。如果查询没有负向文本，你可以从整个语料库中随机抽样一些作为负向样本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们选择 'question' 和 'context' 列作为我们的查询和回答（正向样本），并重命名这些列。然后添加 'id' 列用于后续评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?',\n",
       " 'pos': 'Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.',\n",
       " 'id': '0'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = ds.select_columns(column_names=[\"question\", \"context\"])\n",
    "ds = ds.rename_column(\"question\", \"query\")\n",
    "ds = ds.rename_column(\"context\", \"pos\")\n",
    "ds = ds.add_column(\"id\", [str(i) for i in range(len(ds))])\n",
    "ds[0]\n",
    "\n",
    "# {\n",
    "#   'query': '英伟达（NVIDIA）在扩展到其他计算密集型领域之前，最初专注于哪个领域？',\n",
    "#   'pos': '自从我们最初专注于个人电脑图形处理以来，我们已经扩展到其他几个大型且重要的计算密集型领域。',\n",
    "#   'id': '0'\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "负向样本在嵌入模型训练中非常重要。我们的初始数据集没有负向文本，因此我们直接从整个语料库中抽取一些样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7000/7000 [00:00<00:00, 13704.86 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 设置每个查询对应的负样本数量\n",
    "neg_num = 10\n",
    "\n",
    "def str_to_lst(data):\n",
    "    # 将单个字符串转换为列表，使格式符合微调要求\n",
    "    # 微调数据格式要求'pos'字段为列表形式\n",
    "    data[\"pos\"] = [data[\"pos\"]]\n",
    "    return data\n",
    "\n",
    "# 为每个查询采样负例文本\n",
    "# 注意这里是使用随机采样的方式来生成负样本，实际场景里面我们应该使用大模型或者其他方式获取真正的负样本。\n",
    "new_col = []\n",
    "for i in range(len(ds)):\n",
    "    # 从数据集中随机采样neg_num个索引作为负样本\n",
    "    ids = np.random.randint(0, len(ds), size=neg_num)\n",
    "    # 确保不会将当前样本自身作为负样本\n",
    "    while i in ids:\n",
    "        ids = np.random.randint(0, len(ds), size=neg_num)\n",
    "    # 根据采样的索引获取对应的文本作为负样本\n",
    "    neg = [ds[i.item()][\"pos\"] for i in ids]\n",
    "    new_col.append(neg)\n",
    "# 将采样得到的负样本添加到数据集中\n",
    "ds = ds.add_column(\"neg\", new_col)\n",
    "\n",
    "# 将'pos'键的值转换为列表格式\n",
    "# 通过map函数对数据集中的每一行应用str_to_lst函数\n",
    "ds = ds.map(str_to_lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们添加用于查询的提示语。在推理过程中，它将作为 `query_instruction_for_retrieval`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Represent this sentence for searching relevant passages: \"\n",
    "ds = ds.add_column(\"prompt\", [instruction]*len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在数据集的单行样例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What area did NVIDIA initially focus on before expanding to other computationally intensive fields?',\n",
       " 'pos': ['Since our original focus on PC graphics, we have expanded to several other large and important computationally intensive fields.'],\n",
       " 'id': '0',\n",
       " 'neg': ['Kroger expects that its value creation model will deliver total shareholder return within a target range of 8% to 11% over time.',\n",
       "  'CSB purchased First Mortgages of $2.9 billion during 2023.',\n",
       "  'See Note 13 to our Consolidated Financial Statements for information on certain legal proceedings for which there are contingencies.',\n",
       "  'Diluted earnings per share were $16.69 in fiscal 2022 compared to $15.53 in fiscal 2021.',\n",
       "  'In the year ended December 31, 2023, Total net sales and revenue increased primarily due to: (1) increased net wholesale volumes primarily due to increased sales of crossover vehicles and full-size pickup trucks, partially offset by decreased sales of mid-size pickup trucks; (2) favorable Price as a result of low dealer inventory levels and strong demand for our products; (3) favorable Mix associated with increased sales of full-size pickup trucks and full-size SUVs and decreased sales of vans, passenger cars and mid-size pickup trucks, partially offset by increased sales of crossover vehicles; and (4) favorable Other due to increased sales of parts and accessories.',\n",
       "  'As of December 31, 2023, we had 3,157 full-time employees.',\n",
       "  'Item 3. Legal Proceedings. The information contained in Note 18 ‘‘Commitments and Contingencies’’ included in Item 8 of this 10-K is incorporated herein by reference.',\n",
       "  'Under the amended 2019 Secured Facility, the maturity date is set to July 20, 2026.',\n",
       "  'Accounts receivable for Las Vegas Sands Corp. on December 31, 2023, totaled $685 million, with a provision for credit losses of $201 million, resulting in a net balance of $484 million.',\n",
       "  'Operating expenses as a percentage of segment net sales decreased 25 basis points for fiscal 2023 when compared to the previous fiscal year, primarily driven by strong sales growth and lower incremental COVID-19 related costs, partially offset by increased wage costs.'],\n",
       " 'prompt': 'Represent this sentence for searching relevant passages: '}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们将数据集分割为训练集和测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据集分割为训练集和测试集\n",
    "# test_size=0.1 表示将10%的数据用于测试集\n",
    "# shuffle=True 表示在分割前对数据进行随机打乱\n",
    "# seed=520 设置随机种子，确保每次运行结果一致\n",
    "split = ds.train_test_split(test_size=0.1, shuffle=True, seed=520)\n",
    "\n",
    "# 从分割结果中获取训练集和测试集\n",
    "train = split[\"train\"]  # 包含90%的数据，用于模型训练\n",
    "test = split[\"test\"]    # 包含10%的数据，用于模型评估\n",
    "\n",
    "# 至此，我们已经成功将原始数据集分为两部分：\n",
    "# 1. train - 用于微调模型的训练数据\n",
    "# 2. test - 用于评估微调后模型性能的测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以存储数据以供后续微调使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 28.16ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16583481"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.to_json(\"ft_data/training.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 用于评估的测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步是构建用于评估的测试数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['query', 'pos', 'id', 'neg', 'prompt'],\n",
       "    num_rows: 700\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先选择查询所需的列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1289',\n",
       " 'text': 'How does Starbucks recognize the interest and penalties related to income tax matters on their financial statements?'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 从测试集中选择 ID 和查询列，用于后续评估\n",
    "queries = test.select_columns(column_names=[\"id\", \"query\"])\n",
    "# 将原 \"query\" 列重命名为 \"text\"，这样以便后续处理\n",
    "queries = queries.rename_column(\"query\", \"text\")\n",
    "# 显示第一条数据，验证数据格式是否正确\n",
    "queries[0]\n",
    "\n",
    "# 这段代码做了以下几件事情：\n",
    "# 1. 从测试数据集(test)中选择两列：\"id\"和\"query\"\n",
    "#   - id：每个查询的唯一标识符\n",
    "#   - query：查询文本内容，即用户的问题\n",
    "# 2. 将\"query\"列重命名为\"text\"，符合评估时的数据格式要求\n",
    "#   - 这是因为大多数评估框架期望查询数据使用\"text\"字段名\n",
    "# 3. 输出处理后数据集的第一条记录，用于验证数据格式转换是否正确\n",
    "#\n",
    "# 该处理后的数据集将用于微调后的模型评估，评估模型针对这些查询返回\n",
    "# 相关文档的能力。处理后每条数据包含唯一ID和对应的查询文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后选择语料库所需的列："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为语料库选择必要的列：id（唯一标识符）和pos（文本内容）\n",
    "corpus = ds.select_columns(column_names=[\"id\", \"pos\"])\n",
    "\n",
    "# 将\"pos\"列重命名为\"text\"，符合后续评估框架的标准格式\n",
    "# 大多数评估框架期望语料库中的文本字段使用\"text\"作为字段名\n",
    "corpus = corpus.rename_column(\"pos\", \"text\")\n",
    "\n",
    "# 至此，语料库数据集已包含两个关键列：\n",
    "# 1. id: 每个文档的唯一标识符，用于在评估时关联查询和相关文档\n",
    "# 2. text: 文档的实际内容，即财务报告中的文本段落\n",
    "#\n",
    "# 这个语料库将用于评估微调后的模型，模型需要从这个语料库中\n",
    "# 检索与用户查询最相关的文档。正确的检索表明模型能够理解\n",
    "# 金融文本的语义并建立查询与文档间的关联。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，创建指示查询与相应语料库关系的 qrels："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flattening the indices: 100%|██████████| 700/700 [00:00<00:00, 31334.18 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'qid': '1289', 'docid': '1289', 'relevance': 1}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建qrels（查询与相关文档对应关系）\n",
    "# qrels是用于评估检索系统性能的标准格式，记录了每个查询与相关文档的关系\n",
    "# 首先从test数据集中选择id列，这将作为查询ID\n",
    "qrels = test.select_columns([\"id\"])\n",
    "\n",
    "# 将id列重命名为qid（query id），遵循评估格式标准\n",
    "qrels = qrels.rename_column(\"id\", \"qid\")\n",
    "\n",
    "# 添加docid列，表示与每个查询相关的文档ID\n",
    "# 在这个例子中，我们直接使用测试集中的ID作为相关文档ID\n",
    "# 这意味着对于每个查询，我们认为测试集中的对应文档是相关的\n",
    "qrels = qrels.add_column(\"docid\", list(test[\"id\"]))\n",
    "\n",
    "# 添加relevance（相关性）列，表示查询和文档之间的相关程度\n",
    "# 这里所有关系都标记为1，表示所有指定的文档对于对应查询都是相关的\n",
    "# 在更复杂的评估中，相关性可能有不同级别（如0-3）表示不相关到高度相关\n",
    "qrels = qrels.add_column(\"relevance\", [1]*len(test))\n",
    "\n",
    "# 显示第一条记录，验证数据格式是否正确\n",
    "qrels[0]\n",
    "\n",
    "# 至此，我们已经构建了用于评估的qrels数据集，包含三个关键列：\n",
    "# 1. qid: 查询的唯一标识符\n",
    "# 2. docid: 与查询相关的文档标识符\n",
    "# 3. relevance: 查询与文档的相关性得分（这里统一为1）\n",
    "# \n",
    "# 这个qrels数据集将用于评估模型的检索性能，评估时会比较模型返回的文档\n",
    "# 与这个qrels中记录的相关文档，计算各种评估指标如MAP、MRR、NDCG等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "存储数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 205.49ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 283.59ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 501.59ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30574"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries.to_json(\"ft_data/test_queries.jsonl\")\n",
    "corpus.to_json(\"ft_data/corpus.jsonl\")\n",
    "qrels.to_json(\"ft_data/test_qrels.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "XiaokeAILabs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
