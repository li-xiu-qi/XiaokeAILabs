# BGE 重排器

与嵌入模型类似，BGE拥有一组具有不同规模和功能的重排器。在本教程中，我们将介绍BGE重排器系列。

## 0. 安装

在环境中安装依赖项。

```python
%pip install -U FlagEmbedding
```

## 1. bge-reranker

BGE重排器的第一代包含两个模型：

| 模型  | 语言 |   参数量   |    描述    |   基础模型     |
|:-------|:--------:|:----:|:-----------------:|:--------------------------------------:|
| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   | 中文和英文 |     278M     | 交叉编码器模型，更准确但效率较低 | XLM-RoBERTa-Base |
| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) | 中文和英文 |     560M     | 交叉编码器模型，更准确但效率较低 | XLM-RoBERTa-Large |

```python
from FlagEmbedding import FlagReranker

model = FlagReranker(
    'BAAI/bge-reranker-large',
    use_fp16=True,
    devices=["cuda:0"],   # 如果没有GPU，可以使用"cpu"
)

pairs = [
    ["What is the capital of France?", "Paris is the capital of France."],
    ["What is the capital of France?", "The population of China is over 1.4 billion people."],
    ["What is the population of China?", "Paris is the capital of France."],
    ["What is the population of China?", "The population of China is over 1.4 billion people."]
]

scores = model.compute_score(pairs)
scores
```

输出:

```
/share/project/xzy/Envs/ft/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
  from .autonotebook import tqdm as notebook_tqdm
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

```

结果:

```
[7.984375, -6.84375, -7.15234375, 5.44921875]
```

## 2. bge-reranker v2

| 模型  | 语言 |   参数量   |    描述    |   基础模型     |
|:-------|:--------:|:----:|:-----------------:|:--------------------------------------:|
| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | 多语言 |     568M     | 轻量级交叉编码器模型，具有强大的多语言能力，易于部署，推理速度快。 | XLM-RoBERTa-Large |
| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | 多语言 |     2.51B     | 适用于多语言环境的交叉编码器模型，在英语熟练度和多语言能力方面表现良好。 | Gemma2-2B |
| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | 多语言 |    2.72B    | 适用于多语言环境的交叉编码器模型，在英语和中文熟练度方面表现良好，允许自由选择层输出，促进加速推理。 | MiniCPM |
| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | 多语言 |    9.24B    | 适用于多语言环境的交叉编码器模型，在英语和中文熟练度方面表现良好，允许自由选择层、压缩比和压缩层输出，促进加速推理。 | Gemma2-9B |

### bge-reranker-v2-m3

bge-reranker-v2-m3基于bge-m3训练，在保持模型规模小的同时引入了出色的多语言能力。

```python
from FlagEmbedding import FlagReranker

# 设置use_fp16为True可以加速计算，但性能会略有下降（如果使用GPU）
reranker = FlagReranker('BAAI/bge-reranker-v2-m3', devices=["cuda:0"], use_fp16=True)

score = reranker.compute_score(['query', 'passage'])
# 或设置"normalize=True"对分数应用sigmoid函数，将范围限制在0-1之间
score = reranker.compute_score(['query', 'passage'], normalize=True)

print(score)
```

输出:

```
You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.

```

输出:

```
[0.003483424193080668]

```

### bge-reranker-v2-gemma

bge-reranker-v2-gemma基于gemma-2b训练。它在英语熟练度和多语言能力方面都有出色的表现。

```python
from FlagEmbedding import FlagLLMReranker

reranker = FlagLLMReranker('BAAI/bge-reranker-v2-gemma', devices=["cuda:0"], use_fp16=True)

score = reranker.compute_score(['query', 'passage'])
print(score)
```

输出:

```
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  5.29it/s]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 1/1 [00:00<00:00, 45.99it/s]
```

输出:

```
[1.974609375]

```

输出:

```


```

### bge-reranker-v2-minicpm-layerwise

bge-reranker-v2-minicpm-layerwise基于minicpm-2b-dpo-bf16训练。它适用于多语言环境，在英语和中文熟练度方面表现良好。

另一个特殊功能是其分层设计让用户可以自由选择用于输出的层，从而促进加速推理。

```python
from FlagEmbedding import LayerWiseFlagLLMReranker

reranker = LayerWiseFlagLLMReranker('BAAI/bge-reranker-v2-minicpm-layerwise', devices=["cuda:0"], use_fp16=True)

# 调整'cutoff_layers'以选择用于计算分数的层。
score = reranker.compute_score(['query', 'passage'], cutoff_layers=[28])
print(score)
```

输出:

```
Loading checkpoint shards: 100%|██████████| 3/3 [00:00<00:00,  3.85it/s]
You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 1/1 [00:00<00:00, 24.51it/s]
```

输出:

```
[-7.06640625]

```

输出:

```


```

### bge-reranker-v2.5-gemma2-lightweight

bge-reranker-v2.5-gemma2-lightweight基于gemma2-9b训练。它也适用于多语言环境。

除了分层减少功能外，bge-reranker-v2.5-gemma2-lightweight还集成了令牌压缩功能，在保持出色性能的同时进一步节省更多资源。

```python
from FlagEmbedding import LightWeightFlagLLMReranker

reranker = LightWeightFlagLLMReranker('BAAI/bge-reranker-v2.5-gemma2-lightweight', devices=["cuda:0"], use_fp16=True)

# 调整'cutoff_layers'以选择用于计算分数的层。
score = reranker.compute_score(['query', 'passage'], cutoff_layers=[28], compress_ratio=2, compress_layers=[24, 40])
print(score)
```

输出:

```
Loading checkpoint shards: 100%|██████████| 4/4 [00:01<00:00,  3.60it/s]
You're using a GemmaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
100%|██████████| 1/1 [00:00<00:00, 23.95it/s]
```

输出:

```
[14.734375]

```

输出:

```


```

## 比较

BGE重排器系列为各种功能提供了大量选择。您可以根据您的场景和资源选择模型：

- 对于多语言，使用`BAAI/bge-reranker-v2-m3`、`BAAI/bge-reranker-v2-gemma`和`BAAI/bge-reranker-v2.5-gemma2-lightweight`。

- 对于中文或英文，使用`BAAI/bge-reranker-v2-m3`和`BAAI/bge-reranker-v2-minicpm-layerwise`。

- 为了效率，使用`BAAI/bge-reranker-v2-m3`和`BAAI/bge-reranker-v2-minicpm-layerwise`的低层。

- 为了节省资源和极致效率，使用`BAAI/bge-reranker-base`和`BAAI/bge-reranker-large`。

- 为了更好的性能，推荐`BAAI/bge-reranker-v2-minicpm-layerwise`和`BAAI/bge-reranker-v2-gemma`。

务必始终在您的实际用例上测试，并选择速度与质量平衡最佳的模型！
