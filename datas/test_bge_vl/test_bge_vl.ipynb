{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\bge\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIPModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 768)\n",
       "      (position_embedding): Embedding(77, 768)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (vision_model): CLIPVisionTransformer(\n",
       "    (embeddings): CLIPVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "      (position_embedding): Embedding(257, 1024)\n",
       "    )\n",
       "    (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-23): 24 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n",
       "  (text_projection): Linear(in_features=768, out_features=768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModel\n",
    "\n",
    "# 定义模型名称，可以是本地路径或Hugging Face模型ID\n",
    "MODEL_NAME = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\BGE-VL-large\" # 或 \"BAAI/BGE-VL-large\"\n",
    "\n",
    "# 加载预训练模型，必须设置trust_remote_code=True以运行远程代码\n",
    "model = AutoModel.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "# 设置模型处理器，用于处理输入的图像和文本\n",
    "model.set_processor(MODEL_NAME)\n",
    "# 将模型设置为评估模式，不计算梯度\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bge-en-icl',\n",
       " 'bge-multilingual-gemma2',\n",
       " 'bge-m3',\n",
       " 'bge-large-en-v1.5',\n",
       " 'bge-base-en-v1.5',\n",
       " 'bge-small-en-v1.5',\n",
       " 'bge-large-zh-v1.5',\n",
       " 'bge-base-zh-v1.5',\n",
       " 'bge-small-zh-v1.5',\n",
       " 'bge-large-en',\n",
       " 'bge-base-en',\n",
       " 'bge-small-en',\n",
       " 'bge-large-zh',\n",
       " 'bge-base-zh',\n",
       " 'bge-small-zh',\n",
       " 'e5-mistral-7b-instruct',\n",
       " 'e5-large-v2',\n",
       " 'e5-base-v2',\n",
       " 'e5-small-v2',\n",
       " 'multilingual-e5-large-instruct',\n",
       " 'multilingual-e5-large',\n",
       " 'multilingual-e5-base',\n",
       " 'multilingual-e5-small',\n",
       " 'e5-large',\n",
       " 'e5-base',\n",
       " 'e5-small',\n",
       " 'gte-Qwen2-7B-instruct',\n",
       " 'gte-Qwen2-1.5B-instruct',\n",
       " 'gte-Qwen1.5-7B-instruct',\n",
       " 'gte-multilingual-base',\n",
       " 'gte-large-en-v1.5',\n",
       " 'gte-base-en-v1.5',\n",
       " 'gte-large',\n",
       " 'gte-base',\n",
       " 'gte-small',\n",
       " 'gte-large-zh',\n",
       " 'gte-base-zh',\n",
       " 'gte-small-zh',\n",
       " 'SFR-Embedding-2_R',\n",
       " 'SFR-Embedding-Mistral',\n",
       " 'Linq-Embed-Mistral',\n",
       " 'bce-embedding-base_v1']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from FlagEmbedding.inference.embedder.model_mapping import AUTO_EMBEDDER_MAPPING\n",
    "\n",
    "with open(\"model_mapping.txt\", \"w\") as f:\n",
    "    for key in AUTO_EMBEDDER_MAPPING.keys():\n",
    "        f.write(key + \"\\n\")\n",
    "# 通过模型名称获取模型类\n",
    "list(AUTO_EMBEDDER_MAPPING.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.4867]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用torch.no_grad()上下文管理器，在推理阶段禁用梯度计算以提高速度和减少内存使用\n",
    "with torch.no_grad():\n",
    "    # 编码查询，包含图像和文本描述\n",
    "    query = model.encode(\n",
    "        images = \"cat.png\", \n",
    "    )\n",
    "\n",
    "    # 编码候选图像，不包含文本描述\n",
    "    candidates = model.encode(\n",
    "        images = [\"cat.png\", \"dog.png\"]\n",
    "    )\n",
    "    \n",
    "    # 计算查询向量与候选向量之间的相似度分数（点积）\n",
    "    scores = query @ candidates.T\n",
    "# 打印相似度分数\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0644, 0.0356]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用torch.no_grad()上下文管理器，在推理阶段禁用梯度计算以提高速度和减少内存使用\n",
    "with torch.no_grad():\n",
    "    # 编码查询，包含图像和文本描述\n",
    "    query = model.encode(\n",
    "        images = \"cat.png\", \n",
    "        text = \"将背景变暗，就像相机在夜间拍摄的照片一样\"  # 中文文本描述\n",
    "    )\n",
    "\n",
    "    # 编码候选图像，不包含文本描述\n",
    "    candidates = model.encode(\n",
    "        images = [\"cat.png\", \"dog.png\"]\n",
    "    )\n",
    "    \n",
    "    # 计算查询向量与候选向量之间的相似度分数（点积）\n",
    "    scores = query @ candidates.T\n",
    "# 打印相似度分数\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0727, 0.0536]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用torch.no_grad()上下文管理器，在推理阶段禁用梯度计算以提高速度和减少内存使用\n",
    "with torch.no_grad():\n",
    "    # 编码查询，包含图像和文本描述\n",
    "    query = model.encode(\n",
    "        images = \"cat.png\", \n",
    "        text = \"图片里面是一只狗\"  # 中文文本描述\n",
    "    )\n",
    "\n",
    "    # 编码候选图像，不包含文本描述\n",
    "    candidates = model.encode(\n",
    "        images = [\"cat.png\", \"dog.png\"]\n",
    "    )\n",
    "    \n",
    "    # 计算查询向量与候选向量之间的相似度分数（点积）\n",
    "    scores = query @ candidates.T\n",
    "# 打印相似度分数\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2093, 0.2747]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 使用torch.no_grad()上下文管理器，在推理阶段禁用梯度计算以提高速度和减少内存使用\n",
    "with torch.no_grad():\n",
    "    # 编码查询，包含图像和文本描述\n",
    "    query = model.encode(\n",
    "        images = \"cat.png\", \n",
    "        text = \"The image contains a dog\"  # English text description\n",
    "    )\n",
    "\n",
    "    # 编码候选图像，不包含文本描述\n",
    "    candidates = model.encode(\n",
    "        images = [\"cat.png\", \"dog.png\"]\n",
    "    )\n",
    "    \n",
    "    # 计算查询向量与候选向量之间的相似度分数（点积）\n",
    "    scores = query @ candidates.T\n",
    "# 打印相似度分数\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bge",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
