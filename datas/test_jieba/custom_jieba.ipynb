{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "### 注意事项与优化\n",
        "\n",
        "1. **分词优化**：可以根据需要调整jieba的分词模式，如精确模式、全模式或搜索引擎模式\n",
        "2. **停用词处理**：可以添加中文停用词列表，滤除常见但无意义的词语\n",
        "3. **自定义词典**：对于特定领域，可以通过jieba的自定义词典功能增强分词效果\n",
        "4. **索引刷新**：当原始数据更新时，需要重新处理文本并更新索引\n",
        "5. **性能考虑**：对于大量文本，建议批量处理和索引，以提高性能\n",
        "\n",
        "### jieba自定义词典的使用\n",
        "\n",
        "jieba分词器支持自定义词典，可以显著提高特定领域文本的分词准确度。\n",
        "\n",
        "#### 自定义词典格式\n",
        "\n",
        "jieba的自定义词典是一个文本文件，每行一个词条，格式为：\n",
        "\n",
        "词语 词频(可省略) 词性(可省略)\n",
        "\n",
        "例如：\n",
        "\n",
        "机器学习 100 n\n",
        "深度学习 80 n\n",
        "自然语言处理 60 n\n",
        "神经网络模型 50 n\n",
        "DuckDB全文检索 40 n\n",
        "\n",
        "- 词频越高，该词语被分出来的可能性越高\n",
        "- 词性是可选的，如n(名词)、v(动词)、adj(形容词)等\n",
        "\n",
        "#### 加载自定义词典\n",
        "\n",
        "以下是正确加载和使用自定义词典的方式：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import jieba\n",
        "\n",
        "# 在任何分词操作之前加载自定义词典\n",
        "def load_custom_dict():\n",
        "    # 方式1：从文件加载\n",
        "    jieba.load_userdict(\"custom_dict.txt\")\n",
        "    \n",
        "    # 方式2：动态添加词条\n",
        "    jieba.add_word(\"DuckDB全文检索\", freq=100, tag='n')\n",
        "    jieba.add_word(\"中文分词系统\", freq=80)\n",
        "    \n",
        "    print(\"已加载自定义词典\")\n",
        "\n",
        "# 确保在任何分词操作前调用此函数\n",
        "load_custom_dict()\n",
        "\n",
        "# 自定义词典加载后的分词效果示例\n",
        "text = \"DuckDB全文检索系统支持中文分词\"\n",
        "seg_list = jieba.cut(text)\n",
        "print(\"分词结果: \" + \" / \".join(seg_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "#### 完整示例：结合自定义词典的中文全文检索\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7865d84c",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import duckdb\n",
        "import jieba\n",
        "import re\n",
        "import os\n",
        "\n",
        "# 创建一个简单的自定义词典文件\n",
        "def create_custom_dict(dict_path):\n",
        "    \"\"\"创建一个临时的自定义词典文件\"\"\"\n",
        "    # 确保目录存在\n",
        "    dir_path = os.path.dirname(os.path.abspath(dict_path))\n",
        "    if dir_path and not os.path.exists(dir_path):\n",
        "        os.makedirs(dir_path, exist_ok=True)\n",
        "        \n",
        "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"数据库系统 100 n\\n\")\n",
        "        f.write(\"全文检索 120 n\\n\")\n",
        "        f.write(\"机器学习模型 90 n\\n\")\n",
        "        f.write(\"自然语言处理技术 85 n\\n\")\n",
        "        f.write(\"深度神经网络 95 n\\n\")\n",
        "    print(f\"自定义词典已创建在 {dict_path}\")\n",
        "\n",
        "def preprocess_chinese_text(text):\n",
        "    \"\"\"对中文文本进行分词处理\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # 使用精确模式进行分词\n",
        "    words = jieba.cut(text, cut_all=False)\n",
        "    # 过滤掉空白和标点符号\n",
        "    filtered_words = [word for word in words \n",
        "                     if word.strip() and not re.match(r'[^\\w\\u4e00-\\u9fff]+', word)]\n",
        "    return \" \".join(filtered_words)\n",
        "\n",
        "# 创建持久性自定义词典文件\n",
        "def create_persistent_custom_dict(dict_path):\n",
        "    \"\"\"创建一个持久性的自定义词典文件用于jieba分词\"\"\"\n",
        "    # 确保目录存在\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(dict_path)), exist_ok=True)\n",
        "    \n",
        "    with open(dict_path, 'w', encoding='utf-8') as f:\n",
        "        f.write(\"数据库系统 100 n\\n\")\n",
        "        f.write(\"全文检索 120 n\\n\")\n",
        "        f.write(\"机器学习模型 90 n\\n\")\n",
        "        f.write(\"自然语言处理技术 85 n\\n\")\n",
        "        f.write(\"深度神经网络 95 n\\n\")\n",
        "        f.write(\"DuckDB全文检索 110 n\\n\")\n",
        "        f.write(\"中文分词系统 80 n\\n\")\n",
        "        # 添加更多特定领域词汇\n",
        "        f.write(\"数据挖掘算法 75 n\\n\")\n",
        "        f.write(\"语义理解框架 70 n\\n\")\n",
        "        f.write(\"知识图谱构建 85 n\\n\")\n",
        "    print(f\"持久性自定义词典已创建在 {dict_path}\")\n",
        "    return dict_path\n",
        "\n",
        "def run_chinese_fts_demo_with_custom_dict():\n",
        "    # 创建临时词典文件\n",
        "    dict_path = \"temp_custom_dict.txt\"\n",
        "    create_custom_dict(dict_path)\n",
        "    \n",
        "    # 在任何分词操作前加载自定义词典\n",
        "    jieba.load_userdict(dict_path)\n",
        "    print(\"已加载jieba自定义词典\")\n",
        "    \n",
        "    # 测试分词效果\n",
        "    test_text = \"使用DuckDB进行全文检索和自然语言处理技术分析\"\n",
        "    seg_list = jieba.cut(test_text)\n",
        "    print(\"使用自定义词典的分词效果: \" + \" / \".join(seg_list))\n",
        "    \n",
        "    # 创建内存数据库连接并执行FTS操作\n",
        "    conn = duckdb.connect(':memory:')\n",
        "    try:\n",
        "        # 安装和加载FTS扩展\n",
        "        conn.execute(\"INSTALL fts\")\n",
        "        conn.execute(\"LOAD fts\")\n",
        "        \n",
        "        # 创建文档表并插入数据\n",
        "        conn.execute(\"CREATE TABLE chinese_docs (id VARCHAR, content VARCHAR)\")\n",
        "        conn.execute(\"\"\"\n",
        "        INSERT INTO chinese_docs VALUES\n",
        "            ('1', '使用DuckDB进行全文检索分析'),\n",
        "            ('2', '自然语言处理技术应用于搜索引擎'),\n",
        "            ('3', '深度神经网络在机器学习模型中的应用')\n",
        "        \"\"\")\n",
        "        \n",
        "        # 创建预处理表\n",
        "        conn.execute(\"CREATE TABLE processed_docs (id VARCHAR, content_processed VARCHAR)\")\n",
        "        \n",
        "        # 预处理并填充数据\n",
        "        docs = conn.execute(\"SELECT * FROM chinese_docs\").fetchall()\n",
        "        for doc in docs:\n",
        "            doc_id, content = doc\n",
        "            processed = preprocess_chinese_text(content)\n",
        "            conn.execute(\"INSERT INTO processed_docs VALUES (?, ?)\", \n",
        "                        (doc_id, processed))\n",
        "        \n",
        "        # 创建FTS索引\n",
        "        conn.execute(\"\"\"\n",
        "        PRAGMA create_fts_index('processed_docs', 'id', 'content_processed')\n",
        "        \"\"\")\n",
        "        \n",
        "        # 搜索示例\n",
        "        query = \"全文检索\"\n",
        "        processed_query = preprocess_chinese_text(query)\n",
        "        print(f\"\\n原始查询: '{query}'\")\n",
        "        print(f\"处理后查询: '{processed_query}'\")\n",
        "        \n",
        "        results = conn.execute(\"\"\"\n",
        "        SELECT pd.id, cd.content, score\n",
        "        FROM (\n",
        "            SELECT *, fts_main_processed_docs.match_bm25(id, ?) AS score\n",
        "            FROM processed_docs\n",
        "        ) pd\n",
        "        JOIN chinese_docs cd ON pd.id = cd.id\n",
        "        WHERE score IS NOT NULL\n",
        "        ORDER BY score DESC\n",
        "        \"\"\", (processed_query,)).fetchall()\n",
        "        \n",
        "        if results:\n",
        "            print(\"\\n搜索结果:\")\n",
        "            for res in results:\n",
        "                print(f\"ID: {res[0]}, 内容: {res[1]}, 得分: {res[2]:.6f}\")\n",
        "        else:\n",
        "            print(\"\\n未找到匹配结果\")\n",
        "            \n",
        "    finally:\n",
        "        # 清理资源\n",
        "        conn.close()\n",
        "        # 删除临时词典文件\n",
        "        if os.path.exists(dict_path):\n",
        "            os.remove(dict_path)\n",
        "            print(f\"\\n已删除临时词典文件 {dict_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_chinese_fts_demo_with_custom_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97471cd0",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "# 使用持久性自定义词典的完整示例\n",
        "\n",
        "以下代码展示了如何创建和使用持久性的自定义词典，这对于长期项目更为实用：\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edadccdc",
      "metadata": {
        "language": "python"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import jieba\n",
        "import duckdb\n",
        "import re\n",
        "\n",
        "def create_and_use_persistent_dict():\n",
        "    # 定义自定义词典路径（使用相对或绝对路径）\n",
        "    dict_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"custom_jieba_dict.txt\")\n",
        "    \n",
        "    # 创建持久性词典\n",
        "    create_persistent_custom_dict(dict_path)\n",
        "    \n",
        "    # 加载词典\n",
        "    jieba.load_userdict(dict_path)\n",
        "    print(f\"已加载持久性词典: {dict_path}\")\n",
        "    \n",
        "    # 测试分词效果\n",
        "    test_texts = [\n",
        "        \"DuckDB全文检索系统支持中文分词和语义理解框架\",\n",
        "        \"知识图谱构建需要自然语言处理技术支持\",\n",
        "        \"数据挖掘算法在机器学习模型中的应用\"\n",
        "    ]\n",
        "    \n",
        "    print(\"\\n分词测试结果:\")\n",
        "    for text in test_texts:\n",
        "        seg_list = jieba.cut(text)\n",
        "        print(f\"原文: {text}\")\n",
        "        print(f\"分词: {' / '.join(seg_list)}\")\n",
        "        print(\"-\"*50)\n",
        "    \n",
        "    return dict_path\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 创建并使用持久性词典\n",
        "    dict_path = create_and_use_persistent_dict()\n",
        "    \n",
        "    print(\"\\n此词典文件可以在后续项目中重复使用\")\n",
        "    print(f\"词典位置: {os.path.abspath(dict_path)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14900478",
      "metadata": {
        "language": "markdown"
      },
      "source": [
        "### 自定义词典管理的最佳实践\n",
        "\n",
        "1. **词典位置**：将自定义词典放在项目易于访问的位置，并使用绝对路径或相对于项目根目录的路径\n",
        "2. **定期维护**：根据领域需求定期更新词典内容，添加新词条或调整词频\n",
        "3. **分词质量检测**：定期检查分词结果，确保自定义词典正常工作\n",
        "4. **版本控制**：将词典文件纳入版本控制系统，跟踪词典的变更历史\n",
        "5. **备份策略**：对重要的自定义词典建立备份机制\n",
        "\n",
        "通过以上改进，您可以更有效地管理jieba分词器的自定义词典，提高中文文本处理的精准度。\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
