{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26794aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\modelscope\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# 1. 选择并加载模型和 Tokenizer\n",
    "model_name = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# 确保模型处于评估模式\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afc8c9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2. 准备输入文本\n",
    "prompt = \"你是谁？\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\") # \"pt\" 代表 PyTorch tensors\n",
    "\n",
    "# 3. 执行单步推理（前向传播）\n",
    "# 我们不需要计算梯度，所以使用 no_grad() 来节省计算资源并避免意外的训练。\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    # 'outputs' 包含多个部分，我们最关心的是 'logits'\n",
    "    # logits 的形状通常是 [batch_size, sequence_length, vocab_size]\n",
    "    # 我们想要的是 *下一个* Token 的预测，所以我们取序列中 *最后一个* Token 的 logits\n",
    "    next_token_logits = outputs.logits[:, -1, :] # 形状变为 [batch_size, vocab_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b6c96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[14.7066, 14.6095, 14.1463, 14.1403, 13.7114, 13.2062, 13.0800, 12.9611,\n",
       "          12.7823, 12.7142]]),\n",
       " tensor([[ 35946,  56568, 104198, 105043,  97639,   9909,    220,  49434,  18493,\n",
       "          104786]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 4. 获取候选 Token 及其分数\n",
    "# 我们可以获取 Logit 值最高的 K 个 Token\n",
    "\n",
    "k = 10 # 我们想看 Top 10 的候选者\n",
    "\n",
    "# (a) 获取 Top-K Logits\n",
    "top_k_logits, top_k_indices = torch.topk(next_token_logits, k)\n",
    "top_k_logits, top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0d44519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.9805e-05, 8.3852e-04, 2.6764e-05,  ..., 8.1092e-10, 8.1154e-10,\n",
       "         8.1090e-10]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# (b) (可选) 将 Logits 转换为概率\n",
    "# 使用 Softmax 函数将 Logits 转换为概率分布\n",
    "probabilities = torch.nn.functional.softmax(next_token_logits, dim=-1)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848fa833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1209, 0.1097, 0.0690, 0.0686, 0.0447, 0.0270, 0.0238, 0.0211, 0.0176,\n",
       "          0.0165]]),\n",
       " tensor([[ 35946,  56568, 104198, 105043,  97639,   9909,    220,  49434,  18493,\n",
       "          104786]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_probs, top_k_indices_probs = torch.topk(probabilities, k) # 这里的 indices 应该和上面一样\n",
    "top_k_probs, top_k_indices_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33c9c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æĪĳ',\n",
       " 'ä½ł',\n",
       " 'æĪĳæĺ¯',\n",
       " 'ä½łæĺ¯',\n",
       " 'æĪĳä»¬',\n",
       " 'ï¼Ī',\n",
       " 'Ġ',\n",
       " 'ĠæĪ',\n",
       " 'åľ¨',\n",
       " 'æĪĳåľ¨']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 5. 打印结果\n",
    "# 将 Token 索引解码回文本\n",
    "top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices[0]) # [0] 是因为 batch_size=1\n",
    "top_k_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5e41bb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['æĪĳ',\n",
       " 'ä½ł',\n",
       " 'æĪĳæĺ¯',\n",
       " 'ä½łæĺ¯',\n",
       " 'æĪĳä»¬',\n",
       " 'ï¼Ī',\n",
       " 'Ġ',\n",
       " 'ĠæĪ',\n",
       " 'åľ¨',\n",
       " 'æĪĳåľ¨']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_tokens_probs = tokenizer.convert_ids_to_tokens(top_k_indices_probs[0])\n",
    "top_k_tokens_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05a956b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入文本: '你是谁？'\n",
      "------------------------------\n",
      "模型预测的 Top-10 个下一个 Token (基于 Logits):\n",
      "  - Token: 'æĪĳ', Logit: 14.7066\n",
      "  - Token: 'ä½ł', Logit: 14.6095\n",
      "  - Token: 'æĪĳæĺ¯', Logit: 14.1463\n",
      "  - Token: 'ä½łæĺ¯', Logit: 14.1403\n",
      "  - Token: 'æĪĳä»¬', Logit: 13.7114\n",
      "  - Token: 'ï¼Ī', Logit: 13.2062\n",
      "  - Token: ' ', Logit: 13.0800\n",
      "  - Token: ' æĪ', Logit: 12.9611\n",
      "  - Token: 'åľ¨', Logit: 12.7823\n",
      "  - Token: 'æĪĳåľ¨', Logit: 12.7142\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"输入文本: '{prompt}'\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"模型预测的 Top-{k} 个下一个 Token (基于 Logits):\")\n",
    "\n",
    "for token, logit in zip(top_k_tokens, top_k_logits[0]):\n",
    "    # 使用 .item() 从 Tensor 中提取 Python 数字\n",
    "    print(f\"  - Token: '{token.replace('Ġ', ' ')}', Logit: {logit.item():.4f}\") # 'Ġ' 通常代表空格\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "def1160d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "模型预测的 Top-10 个下一个 Token (基于概率):\n",
      "  - Token: 'æĪĳ' -> 解码: '我', 概率: 12.0899%\n",
      "  - Token: 'ä½ł' -> 解码: '你', 概率: 10.9720%\n",
      "  - Token: 'æĪĳæĺ¯' -> 解码: '我是', 概率: 6.9037%\n",
      "  - Token: 'ä½łæĺ¯' -> 解码: '你是', 概率: 6.8625%\n",
      "  - Token: 'æĪĳä»¬' -> 解码: '我们', 概率: 4.4693%\n",
      "  - Token: 'ï¼Ī' -> 解码: '（', 概率: 2.6967%\n",
      "  - Token: 'Ġ' -> 解码: ' ', 概率: 2.3769%\n",
      "  - Token: 'ĠæĪ' -> 解码: ' �', 概率: 2.1105%\n",
      "  - Token: 'åľ¨' -> 解码: '在', 概率: 1.7649%\n",
      "  - Token: 'æĪĳåľ¨' -> 解码: '我在', 概率: 1.6487%\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 30)\n",
    "print(f\"模型预测的 Top-{k} 个下一个 Token (基于概率):\")\n",
    "\n",
    "for token, prob in zip(top_k_tokens_probs, top_k_probs[0]):\n",
    "    # 解码单个token为可读文本\n",
    "    decoded_token = tokenizer.decode([tokenizer.convert_tokens_to_ids(token)])\n",
    "    print(f\"  - Token: '{token}' -> 解码: '{decoded_token}', 概率: {prob.item():.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7797893a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "如果选择 Logit 最高的 Token，下一个输入将是: '你是谁？我'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. (可选) 演示如何选择一个 Token 并继续生成\n",
    "chosen_token_id = top_k_indices[0][0].unsqueeze(0).unsqueeze(0) # 选择 Logit 最高的那个\n",
    "new_input_ids = torch.cat([input_ids, chosen_token_id], dim=-1)\n",
    "new_prompt = tokenizer.decode(new_input_ids[0])\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"如果选择 Logit 最高的 Token，下一个输入将是: '{new_prompt}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821527a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
