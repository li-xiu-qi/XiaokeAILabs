{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aafe3b01",
   "metadata": {},
   "source": [
    "# N-gram 模型详解与示例\n",
    "\n",
    "## 什么是 N-gram？\n",
    "\n",
    "N-gram 是一种在文本分析中广泛使用的基本概念。简单来说，一个 N-gram 是文本中连续的 N 个项（items）的序列。这些“项”可以是字符、单词、音节或其他语言单位，具体取决于应用场景。\n",
    "\n",
    "- **N** 代表序列中项的数量。\n",
    "  - 当 N=1 时，称为 **unigram** (或 1-gram)。\n",
    "  - 当 N=2 时，称为 **bigram** (或 2-gram)。\n",
    "  - 当 N=3 时，称为 **trigram** (或 3-gram)。\n",
    "  - 以此类推。\n",
    "\n",
    "N-gram 模型在自然语言处理 (NLP) 中有多种应用，例如：\n",
    "- **语言建模**：预测序列中的下一个词或字符。\n",
    "- **文本分类**：将文档归类到预定义的类别。\n",
    "- **机器翻译**：在不同语言之间转换文本。\n",
    "- **拼写纠错**：识别和修正拼写错误。\n",
    "- **信息检索**：查找与查询相关的文档。\n",
    "- **语音识别**：将口语转换为文本。\n",
    "\n",
    "在本 Notebook 中，我们将重点演示基于**单词**的 N-grams，并展示如何生成它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982ca7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram 生成函数已定义。\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def generate_ngrams(text: str, n: int, word_based: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    从文本生成 n-grams。\n",
    "\n",
    "    参数:\n",
    "    text (str): 输入文本。\n",
    "    n (int): n-gram 中项的数量 (例如，1 表示 unigram, 2 表示 bigram)。\n",
    "    word_based (bool): True 表示基于单词的 n-gram, False 表示基于字符的 n-gram。\n",
    "\n",
    "    返回:\n",
    "    list: 生成的 n-grams 列表。每个 n-gram 是一个元组（对于基于单词的）或字符串（对于基于字符的）。\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    if word_based:\n",
    "        # 简单分词：按空格分割，并移除标点符号（可选，这里简化处理）\n",
    "        # 更复杂的场景可能需要专门的分词库 (如 NLTK, SpaCy, Jieba for Chinese)\n",
    "        # 为了演示，我们先转换为小写，然后用正则表达式匹配单词\n",
    "        processed_text = text.lower()\n",
    "        # 移除非字母数字字符，但保留空格用于分词\n",
    "        processed_text = re.sub(r'[^\\w\\s]', '', processed_text) \n",
    "        tokens = processed_text.split()\n",
    "        \n",
    "        if len(tokens) < n:\n",
    "            return [] # 如果词数少于n，则无法生成n-gram\n",
    "            \n",
    "        ngrams = []\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            ngrams.append(tuple(tokens[i:i+n]))\n",
    "        return ngrams\n",
    "    else: # Character-based n-grams\n",
    "        # 对于字符 n-gram，通常不需要复杂的预处理，但可以移除多余空格\n",
    "        processed_text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        if len(processed_text) < n:\n",
    "            return []\n",
    "        \n",
    "        ngrams = []\n",
    "        for i in range(len(processed_text) - n + 1):\n",
    "            ngrams.append(processed_text[i:i+n])\n",
    "        return ngrams\n",
    "\n",
    "print(\"N-gram 生成函数已定义。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9e002f",
   "metadata": {},
   "source": [
    "## 1. Unigrams (1-grams)\n",
    "\n",
    "Unigram 是文本中单个的项。如果基于单词，它就是文本中的每个单词。如果基于字符，它就是文本中的每个字符。\n",
    "\n",
    "Unigram 模型是最简单的语言模型，它假设每个单词（或字符）的出现是独立的，不依赖于前面的单词（或字符）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1399a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 英文 Unigrams (基于单词) ---\n",
      "文本: 'The quick brown fox jumps over the lazy dog.'\n",
      "Unigrams: [('the',), ('quick',), ('brown',), ('fox',), ('jumps',), ('over',), ('the',), ('lazy',), ('dog',)]\n",
      "数量: 9\n",
      "\n",
      "--- 中文 Unigrams (基于单词，使用空格分词) ---\n",
      "分词后文本: '我 爱 自然语言 处理 技术'\n",
      "Unigrams: [('我',), ('爱',), ('自然语言',), ('处理',), ('技术',)]\n",
      "数量: 5\n",
      "\n",
      "--- 英文 Unigrams (基于字符) ---\n",
      "文本: 'The quick brown fox jumps over the lazy dog.'\n",
      "Unigrams: ['T', 'h', 'e', ' ', 'q', 'u', 'i', 'c', 'k', ' ', 'b', 'r', 'o', 'w', 'n', ' ', 'f', 'o', 'x', ' ', 'j', 'u', 'm', 'p', 's', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'l', 'a', 'z', 'y', ' ', 'd', 'o', 'g', '.']\n",
      "数量: 44\n",
      "\n",
      "--- 中文 Unigrams (基于字符) ---\n",
      "文本: '我爱自然语言处理技术'\n",
      "Unigrams: ['我', '爱', '自', '然', '语', '言', '处', '理', '技', '术']\n",
      "数量: 10\n"
     ]
    }
   ],
   "source": [
    "# 英文示例文本\n",
    "english_text_example = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# 中文示例文本\n",
    "chinese_text_example = \"我爱自然语言处理技术\"\n",
    "\n",
    "print(\"--- 英文 Unigrams (基于单词) ---\")\n",
    "unigrams_en_word = generate_ngrams(english_text_example, 1, word_based=True)\n",
    "print(f\"文本: '{english_text_example}'\")\n",
    "print(f\"Unigrams: {unigrams_en_word}\")\n",
    "print(f\"数量: {len(unigrams_en_word)}\\n\")\n",
    "\n",
    "print(\"--- 中文 Unigrams (基于单词，使用空格分词) ---\")\n",
    "# 注意：对于中文，简单的空格分词通常不适用。实际应用中需要使用中文分词工具。\n",
    "# 这里为了演示，我们手动在词之间加入空格。\n",
    "chinese_text_segmented = \"我 爱 自然语言 处理 技术\"\n",
    "unigrams_zh_word = generate_ngrams(chinese_text_segmented, 1, word_based=True)\n",
    "print(f\"分词后文本: '{chinese_text_segmented}'\")\n",
    "print(f\"Unigrams: {unigrams_zh_word}\")\n",
    "print(f\"数量: {len(unigrams_zh_word)}\\n\")\n",
    "\n",
    "print(\"--- 英文 Unigrams (基于字符) ---\")\n",
    "unigrams_en_char = generate_ngrams(english_text_example, 1, word_based=False)\n",
    "print(f\"文本: '{english_text_example}'\")\n",
    "print(f\"Unigrams: {unigrams_en_char}\") # 输出会很长，可以考虑只打印前几个\n",
    "print(f\"数量: {len(unigrams_en_char)}\\n\")\n",
    "\n",
    "print(\"--- 中文 Unigrams (基于字符) ---\")\n",
    "unigrams_zh_char = generate_ngrams(chinese_text_example, 1, word_based=False)\n",
    "print(f\"文本: '{chinese_text_example}'\")\n",
    "print(f\"Unigrams: {unigrams_zh_char}\")\n",
    "print(f\"数量: {len(unigrams_zh_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c6e20",
   "metadata": {},
   "source": [
    "## 2. Bigrams (2-grams)\n",
    "\n",
    "Bigram 是文本中连续的两个项的序列。如果基于单词，它就是文本中连续的两个单词。\n",
    "\n",
    "Bigram 模型考虑了前一个单词对当前单词出现概率的影响，比 Unigram 模型能捕捉到更多的上下文信息。例如，“New York” 是一个常见的 Bigram，单独的 “New” 和 “York” 可能意义不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "226449ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 英文 Bigrams (基于单词) ---\n",
      "文本: 'The quick brown fox jumps over the lazy dog.'\n",
      "Bigrams: [('the', 'quick'), ('quick', 'brown'), ('brown', 'fox'), ('fox', 'jumps'), ('jumps', 'over'), ('over', 'the'), ('the', 'lazy'), ('lazy', 'dog')]\n",
      "数量: 8\n",
      "\n",
      "--- 中文 Bigrams (基于单词，使用空格分词) ---\n",
      "分词后文本: '我 爱 自然语言 处理 技术'\n",
      "Bigrams: [('我', '爱'), ('爱', '自然语言'), ('自然语言', '处理'), ('处理', '技术')]\n",
      "数量: 4\n",
      "\n",
      "--- 英文 Bigrams (基于字符) ---\n",
      "文本: 'The quick brown fox jumps over the lazy dog.'\n",
      "Bigrams (前20个): ['Th', 'he', 'e ', ' q', 'qu', 'ui', 'ic', 'ck', 'k ', ' b', 'br', 'ro', 'ow', 'wn', 'n ', ' f', 'fo', 'ox', 'x ', ' j']\n",
      "数量: 43\n",
      "\n",
      "--- 中文 Bigrams (基于字符) ---\n",
      "文本: '我爱自然语言处理技术'\n",
      "Bigrams: ['我爱', '爱自', '自然', '然语', '语言', '言处', '处理', '理技', '技术']\n",
      "数量: 9\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 英文 Bigrams (基于单词) ---\")\n",
    "bigrams_en_word = generate_ngrams(english_text_example, 2, word_based=True)\n",
    "print(f\"文本: '{english_text_example}'\")\n",
    "print(f\"Bigrams: {bigrams_en_word}\")\n",
    "print(f\"数量: {len(bigrams_en_word)}\\n\")\n",
    "\n",
    "print(\"--- 中文 Bigrams (基于单词，使用空格分词) ---\")\n",
    "bigrams_zh_word = generate_ngrams(chinese_text_segmented, 2, word_based=True)\n",
    "print(f\"分词后文本: '{chinese_text_segmented}'\")\n",
    "print(f\"Bigrams: {bigrams_zh_word}\")\n",
    "print(f\"数量: {len(bigrams_zh_word)}\\n\")\n",
    "\n",
    "print(\"--- 英文 Bigrams (基于字符) ---\")\n",
    "bigrams_en_char = generate_ngrams(english_text_example, 2, word_based=False)\n",
    "print(f\"文本: '{english_text_example}'\")\n",
    "print(f\"Bigrams (前20个): {bigrams_en_char[:20]}\") # 截断输出\n",
    "print(f\"数量: {len(bigrams_en_char)}\\n\")\n",
    "\n",
    "print(\"--- 中文 Bigrams (基于字符) ---\")\n",
    "bigrams_zh_char = generate_ngrams(chinese_text_example, 2, word_based=False)\n",
    "print(f\"文本: '{chinese_text_example}'\")\n",
    "print(f\"Bigrams: {bigrams_zh_char}\")\n",
    "print(f\"数量: {len(bigrams_zh_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e92760",
   "metadata": {},
   "source": [
    "## 3. Trigrams (3-grams)\n",
    "\n",
    "Trigram 是文本中连续的三个项的序列。如果基于单词，它就是文本中连续的三个单词。\n",
    "\n",
    "Trigram 模型考虑了前面两个单词对当前单词出现概率的影响，能捕捉到比 Bigram 模型更长的上下文依赖关系。例如，“natural language processing” 是一个常见的 Trigram。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d1c5c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 英文 Trigrams (基于单词) ---\n",
      "文本: 'The quick brown fox jumps over the lazy dog.'\n",
      "Trigrams: [('the', 'quick', 'brown'), ('quick', 'brown', 'fox'), ('brown', 'fox', 'jumps'), ('fox', 'jumps', 'over'), ('jumps', 'over', 'the'), ('over', 'the', 'lazy'), ('the', 'lazy', 'dog')]\n",
      "数量: 7\n",
      "\n",
      "--- 中文 Trigrams (基于单词，使用空格分词) ---\n",
      "分词后文本: '我 爱 自然语言 处理 技术'\n",
      "Trigrams: [('我', '爱', '自然语言'), ('爱', '自然语言', '处理'), ('自然语言', '处理', '技术')]\n",
      "数量: 3\n",
      "\n",
      "--- 英文 Trigrams (基于字符) ---\n",
      "文本: 'The quick brown fox jumps over the lazy dog.'\n",
      "Trigrams (前20个): ['The', 'he ', 'e q', ' qu', 'qui', 'uic', 'ick', 'ck ', 'k b', ' br', 'bro', 'row', 'own', 'wn ', 'n f', ' fo', 'fox', 'ox ', 'x j', ' ju']\n",
      "数量: 42\n",
      "\n",
      "--- 中文 Trigrams (基于字符) ---\n",
      "文本: '我爱自然语言处理技术'\n",
      "Trigrams: ['我爱自', '爱自然', '自然语', '然语言', '语言处', '言处理', '处理技', '理技术']\n",
      "数量: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 英文 Trigrams (基于单词) ---\")\n",
    "trigrams_en_word = generate_ngrams(english_text_example, 3, word_based=True)\n",
    "print(f\"文本: '{english_text_example}'\")\n",
    "print(f\"Trigrams: {trigrams_en_word}\")\n",
    "print(f\"数量: {len(trigrams_en_word)}\\n\")\n",
    "\n",
    "print(\"--- 中文 Trigrams (基于单词，使用空格分词) ---\")\n",
    "trigrams_zh_word = generate_ngrams(chinese_text_segmented, 3, word_based=True)\n",
    "print(f\"分词后文本: '{chinese_text_segmented}'\")\n",
    "print(f\"Trigrams: {trigrams_zh_word}\")\n",
    "print(f\"数量: {len(trigrams_zh_word)}\\n\")\n",
    "\n",
    "print(\"--- 英文 Trigrams (基于字符) ---\")\n",
    "trigrams_en_char = generate_ngrams(english_text_example, 3, word_based=False)\n",
    "print(f\"文本: '{english_text_example}'\")\n",
    "print(f\"Trigrams (前20个): {trigrams_en_char[:20]}\") # 截断输出\n",
    "print(f\"数量: {len(trigrams_en_char)}\\n\")\n",
    "\n",
    "print(\"--- 中文 Trigrams (基于字符) ---\")\n",
    "trigrams_zh_char = generate_ngrams(chinese_text_example, 3, word_based=False)\n",
    "print(f\"文本: '{chinese_text_example}'\")\n",
    "print(f\"Trigrams: {trigrams_zh_char}\")\n",
    "print(f\"数量: {len(trigrams_zh_char)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed0583",
   "metadata": {},
   "source": [
    "## 总结与讨论\n",
    "\n",
    "- **N 的选择**：N 的值越大，模型能捕捉到的上下文信息越长，但也更容易遇到数据稀疏问题（即很多 N-gram 在训练数据中从未出现过）。实际应用中，N 通常取 1 到 5 之间。\n",
    "- **基于单词 vs. 基于字符**：\n",
    "  - **基于单词**的 N-gram 更符合人类对语言的理解，但对于词汇量大的语言（如中文、日文、韩文，它们没有天然的空格分隔符，需要分词）和形态丰富的语言（如德语、俄语）处理起来更复杂，且更容易遇到未登录词 (Out-Of-Vocabulary, OOV) 问题。\n",
    "  - **基于字符**的 N-gram 词汇表大小有限（例如，英文字母加数字标点），不易遇到 OOV 问题，对于形态丰富的语言和某些亚洲语言有优势。但单个字符通常不携带太多语义信息，需要更长的 N 才能捕捉上下文。\n",
    "- **预处理**：文本预处理（如小写转换、标点移除、词干提取、停用词移除）对 N-gram 的生成和后续应用有重要影响。\n",
    "- **中文处理**：对于中文等不使用空格分词的语言，生成基于单词的 N-gram 前必须先进行中文分词。常用的中文分词工具有 Jieba, THULAC, HanLP, spaCy 等。\n",
    "\n",
    "这个 Notebook 提供了一个 N-gram 的基本介绍和生成方法。在实际的 NLP 项目中，N-gram 通常作为特征工程的一部分，或者作为更复杂模型（如神经网络语言模型）的基础。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartImageFinder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
