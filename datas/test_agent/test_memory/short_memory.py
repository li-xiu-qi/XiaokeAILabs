import time
import os
from openai import OpenAI



class OpenAICompatibleLLM:
    """
    ä¸€ä¸ªä½¿ç”¨OpenAIå…¼å®¹APIçš„å¤§è¯­è¨€æ¨¡å‹ç±»ã€‚
    å®ƒä¼šä»ç¯å¢ƒå˜é‡ä¸­è¯»å–é…ç½®ã€‚
    """
    def __init__(self, model="gpt-3.5-turbo"):
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_API_BASE")

        if not api_key or not base_url:
            raise ValueError("é”™è¯¯ï¼šè¯·è®¾ç½® OPENAI_API_KEY å’Œ OPENAI_API_BASE ç¯å¢ƒå˜é‡ã€‚")

        self.client = OpenAI(api_key=api_key, base_url=base_url)
        self.model = model
        print(f"ğŸ¤– LLMå®¢æˆ·ç«¯å·²åˆå§‹åŒ–ï¼Œå°†ä½¿ç”¨æ¨¡å‹: {self.model}ï¼Œæ¥å…¥ç‚¹: {base_url}")

    def generate(self, context):
        """ä½¿ç”¨APIæ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”ã€‚"""
        print(f"ğŸ¤– [LLM API] æ­£åœ¨æ ¹æ® {len(context)} æ¡æ¶ˆæ¯è°ƒç”¨APIç”Ÿæˆå›ç­”...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=context,
                temperature=0.7,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"è°ƒç”¨APIæ—¶å‡ºé”™: {e}")
            return "æŠ±æ­‰ï¼Œæˆ‘åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†ä¸€ä¸ªé—®é¢˜ã€‚"

    def summarize(self, messages_to_summarize):
        """ä½¿ç”¨APIå¯¹ä¸€æ®µå¯¹è¯å†å²è¿›è¡Œæ‘˜è¦ã€‚"""
        num_messages = len(messages_to_summarize)
        print(f"ğŸ¤– [LLM API] æ­£åœ¨å°† {num_messages} æ¡æ—§æ¶ˆæ¯å‘é€åˆ°APIè¿›è¡Œæ‘˜è¦...")
        
        # åˆ›å»ºä¸€ä¸ªä¸“é—¨ç”¨äºæ‘˜è¦çš„ä¸Šä¸‹æ–‡
        summary_prompt = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å¯¹è¯æ‘˜è¦åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä¸‹é¢çš„å¯¹è¯å†…å®¹ï¼Œç”Ÿæˆä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯ã€‚"},
        ] + messages_to_summarize

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=summary_prompt,
                temperature=0.3,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"è°ƒç”¨APIè¿›è¡Œæ‘˜è¦æ—¶å‡ºé”™: {e}")
            return f"å¯¹å‰{num_messages}æ¡æ¶ˆæ¯çš„æ‘˜è¦ç”Ÿæˆå¤±è´¥ã€‚"

def count_tokens(text):
    """ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œé€šè¿‡è®¡ç®—è¯æ•°æ¥æ¨¡æ‹Ÿè®¡ç®—tokenæ•°é‡ã€‚
    æ³¨æ„: è¿™åªæ˜¯ä¸€ä¸ªç²—ç•¥çš„æ¨¡æ‹Ÿï¼Œå®é™…tokenæ•°åº”ä½¿ç”¨tiktokenç­‰åº“è®¡ç®—ã€‚"""
    return len(text.split())

# --- åŸºç¡€è®°å¿†ç±» (æ— éœ€ä¿®æ”¹) ---

class BaseMemory:
    """æ‰€æœ‰è®°å¿†ç®¡ç†ç­–ç•¥çš„åŸºç±»ã€‚"""
    def __init__(self, llm, max_tokens=100):
        self.llm = llm
        self.history = []
        self.max_tokens = max_tokens

    def add_message(self, role, content):
        """æ·»åŠ ä¸€æ¡æ¶ˆæ¯åˆ°å†å²è®°å½•ä¸­ã€‚"""
        message = {"role": role, "content": content}
        self.history.append(message)
        # æ¨¡æ‹ŸLLMç”Ÿæˆå›åº”
        if role == 'user':
            assistant_response = self.llm.generate(self.get_context())
            self.history.append({"role": "assistant", "content": assistant_response})
        self.enforce_policy() # åº”ç”¨è®°å¿†ç­–ç•¥

    def get_context(self):
        """è·å–å½“å‰å¯ç”¨çš„ä¸Šä¸‹æ–‡ã€‚"""
        return self.history

    def get_total_tokens(self):
        """è®¡ç®—å½“å‰å†å²è®°å½•çš„æ€»tokenæ•°ã€‚"""
        return sum(count_tokens(msg['content']) for msg in self.history)

    def enforce_policy(self):
        """å­ç±»éœ€è¦å®ç°è¿™ä¸ªæ–¹æ³•æ¥å®šä¹‰è‡ªå·±çš„è®°å¿†ç®¡ç†ç­–ç•¥ã€‚"""
        raise NotImplementedError

    def print_status(self):
        """æ‰“å°å½“å‰è®°å¿†çŠ¶æ€ã€‚"""
        print(f"  [çŠ¶æ€] æ¶ˆæ¯æ•°é‡: {len(self.history)}, "
              f"Tokenæ•°é‡: {self.get_total_tokens()}/{self.max_tokens}")
        print("  " + "="*50)
        for msg in self.history:
            print(f"    {msg['role']}: {msg['content']}")
        print("  " + "="*50 + "\n")


# --- ç­–ç•¥ 1: æ»‘åŠ¨çª—å£ (Sliding Window) (æ— éœ€ä¿®æ”¹) ---

class SlidingWindowMemory(BaseMemory):
    """
    ä½¿ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥ç®¡ç†è®°å¿†ã€‚
    å½“tokenæ•°é‡è¶…è¿‡é™åˆ¶æ—¶ï¼Œä»æœ€å‰é¢ç§»é™¤æ—§çš„æ¶ˆæ¯ã€‚
    """
    def enforce_policy(self):
        print("-> åº”ç”¨æ»‘åŠ¨çª—å£ç­–ç•¥...")
        while self.get_total_tokens() > self.max_tokens and len(self.history) > 1:
            removed_message = self.history.pop(0)
            print(f"  [æ»‘åŠ¨çª—å£] Tokenè¶…é™ï¼Œç§»é™¤æœ€æ—§çš„æ¶ˆæ¯: '{removed_message['content'][:30]}...'")

# --- ç­–ç•¥ 2: æ‘˜è¦ (Summarization) (æ— éœ€ä¿®æ”¹) ---

class SummarizationMemory(BaseMemory):
    """
    ä½¿ç”¨æ‘˜è¦ç­–ç•¥ç®¡ç†è®°å¿†ã€‚
    å½“tokenæ•°é‡è¾¾åˆ°é˜ˆå€¼æ—¶ï¼Œå°†æ—§æ¶ˆæ¯è¿›è¡Œæ‘˜è¦ã€‚
    """
    def enforce_policy(self):
        print("-> åº”ç”¨æ‘˜è¦ç­–ç•¥...")
        if self.get_total_tokens() > self.max_tokens and len(self.history) > 3:
            print(f"  [æ‘˜è¦] Tokenè¶…é™ï¼Œå‡†å¤‡å¯¹æ—§æ¶ˆæ¯è¿›è¡Œæ‘˜è¦ã€‚")
            num_to_summarize = len(self.history) // 2
            messages_to_summarize = self.history[:num_to_summarize]
            summary = self.llm.summarize(messages_to_summarize)
            summary_message = {"role": "system", "content": f"å¯¹è¯æ—©æœŸå†…å®¹çš„æ‘˜è¦: {summary}"}
            self.history = [summary_message] + self.history[num_to_summarize:]
            print(f"  [æ‘˜è¦] åˆ›å»ºäº†æ–°çš„æ‘˜è¦å¹¶æ›¿æ¢äº† {num_to_summarize} æ¡æ—§æ¶ˆæ¯ã€‚")

# --- ç­–ç•¥ 3: æ··åˆç­–ç•¥ (Hybrid) (æ— éœ€ä¿®æ”¹) ---

class HybridMemory(BaseMemory):
    """
    æ··åˆç­–ç•¥ï¼šä¿ç•™æœ€è¿‘çš„å‡ æ¡æ¶ˆæ¯ï¼Œå¹¶å¯¹æ›´æ—©çš„æ¶ˆæ¯è¿›è¡Œæ‘˜è¦ã€‚
    """
    def __init__(self, llm, max_tokens=100, recent_message_limit=4):
        super().__init__(llm, max_tokens)
        self.recent_message_limit = recent_message_limit

    def enforce_policy(self):
        print("-> åº”ç”¨æ··åˆç­–ç•¥...")
        while self.get_total_tokens() > self.max_tokens and len(self.history) > self.recent_message_limit:
            num_to_process = len(self.history) - self.recent_message_limit
            messages_to_process = self.history[:num_to_process]
            if self.history[0]['role'] == 'system' and 'æ‘˜è¦' in self.history[0]['content']:
                print(f"  [æ··åˆç­–ç•¥] Tokenè¶…é™ï¼Œå°† {len(messages_to_process)-1} æ¡æ›´æ—©çš„æ¶ˆæ¯åˆå¹¶åˆ°ç°æœ‰æ‘˜è¦ä¸­ã€‚")
                summary_message = self.history.pop(0)
                messages_to_summarize = messages_to_process + [summary_message]
            else:
                messages_to_summarize = messages_to_process
                print(f"  [æ··åˆç­–ç•¥] Tokenè¶…é™ï¼Œä¸º {len(messages_to_summarize)} æ¡æœ€æ—©çš„æ¶ˆæ¯åˆ›å»ºæ‘˜è¦ã€‚")
            summary = self.llm.summarize(messages_to_summarize)
            summary_message = {"role": "system", "content": f"å¯¹è¯æ—©æœŸå†…å®¹çš„æ‘˜è¦: {summary}"}
            self.history = [summary_message] + self.history[num_to_process:]

# --- æ¼”ç¤º ---

if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–çœŸå®çš„LLMå®¢æˆ·ç«¯
        llm = OpenAICompatibleLLM()

        conversation = [
            "ä½ å¥½ï¼æˆ‘ä»¬æ¥èŠèŠæ°´æœå§ã€‚æˆ‘æœ€å–œæ¬¢è‹¹æœã€‚",
            "è‹¹æœç¡®å®ä¸é”™ï¼Œå°¤å…¶æ˜¯å¯Œå£«è‹¹æœã€‚ä½ å–œæ¬¢é¦™è•‰å—ï¼Ÿ",
            "é¦™è•‰ä¹Ÿå¾ˆå¥½ï¼Œå¯Œå«é’¾å…ƒç´ ã€‚æˆ‘ä»¬æ¥ä¸‹æ¥èŠèŠæ—…è¡Œè®¡åˆ’æ€ä¹ˆæ ·ï¼Ÿ",
            "å¥½ä¸»æ„ï¼æˆ‘æƒ³å»äº‘å—æ—…æ¸¸ï¼Œå¬è¯´é‚£é‡Œé£æ™¯å¾ˆç¾ã€‚",
            "äº‘å—çš„å¤§ç†å’Œä¸½æ±Ÿéƒ½éå¸¸æ£’ã€‚ä½ æ‰“ç®—ä»€ä¹ˆæ—¶å€™å»ï¼Ÿ",
            "ä¹Ÿè®¸æ˜¯ç§‹å¤©å§ï¼Œå¤©æ°”æ¯”è¾ƒå‡‰çˆ½ã€‚ä¸è¿‡é¢„ç®—æ˜¯ä¸ªé—®é¢˜ï¼Œéœ€è¦å¥½å¥½è§„åˆ’ä¸€ä¸‹ã€‚",
            "æ˜¯çš„ï¼Œè§„åˆ’å¾ˆé‡è¦ã€‚æˆ‘ä»¬å¯ä»¥å…ˆä»æœºç¥¨å’Œä½å®¿å¼€å§‹æŸ¥èµ·ã€‚"
        ]

        print("\n" + "#"*20 + " 1. æ»‘åŠ¨çª—å£ç­–ç•¥æ¼”ç¤º " + "#"*20)
        sliding_memory = SlidingWindowMemory(llm, max_tokens=150) # å¢åŠ äº†tokené™åˆ¶ä»¥é€‚åº”æ›´é•¿çš„çœŸå®å›å¤
        for turn in conversation[:3]: # ä»…æ¼”ç¤ºå‰å‡ è½®ï¼Œé¿å…è¿‡å¤šAPIè°ƒç”¨
            sliding_memory.add_message('user', turn)
            sliding_memory.print_status()

        print("\n" + "#"*20 + " 2. æ‘˜è¦ç­–ç•¥æ¼”ç¤º " + "#"*20)
        summarization_memory = SummarizationMemory(llm, max_tokens=150)
        for turn in conversation[:4]: # ä»…æ¼”ç¤ºå‰å‡ è½®
            summarization_memory.add_message('user', turn)
            summarization_memory.print_status()
            
        print("\n" + "#"*20 + " 3. æ··åˆç­–ç•¥æ¼”ç¤º " + "#"*20)
        hybrid_memory = HybridMemory(llm, max_tokens=150, recent_message_limit=4)
        for turn in conversation[:5]: # ä»…æ¼”ç¤ºå‰å‡ è½®
            hybrid_memory.add_message('user', turn)
            hybrid_memory.print_status()

    except ValueError as e:
        print(e)
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
