# -*- coding: utf-8 -*-
"""
çº¢æ¥¼æ¢¦çŸ¥è¯†å›¾è°±æ„å»ºå™¨
åŸºäºNLPæŠ€æœ¯æå–çº¢æ¥¼æ¢¦ä¸­çš„äººç‰©ã€åœ°ç‚¹ã€å…³ç³»ç­‰ä¿¡æ¯ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±
"""

import os
import re
import json
import networkx as nx
from typing import List, Dict, Set, Tuple, Any
from collections import defaultdict
import matplotlib.pyplot as plt
import matplotlib
from dataclasses import dataclass
import asyncio
from dotenv import load_dotenv
import yaml  # æ–°å¢ï¼šç”¨äºè§£æYAML


from semantic_splitter import SemanticSplitter
from fallback_openai_client import AsyncFallbackOpenAIClient
from prompts import HONGLOUMENG_KG_EXTRACTION_YAML_PROMPT  # å¯¼å…¥æç¤ºè¯


# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
matplotlib.rcParams['axes.unicode_minus'] = False

@dataclass
class Entity:
    """å®ä½“ç±»"""
    name: str
    type: str  # person, place, object, concept
    aliases: Set[str]
    attributes: Dict[str, Any]
    mentions: List[str]  # å‡ºç°çš„ä¸Šä¸‹æ–‡

@dataclass
class Relation:
    """å…³ç³»ç±»"""
    source: str
    target: str
    relation_type: str
    confidence: float
    context: str

class HongLouMengKGBuilder:
    """çº¢æ¥¼æ¢¦çŸ¥è¯†å›¾è°±æ„å»ºå™¨ï¼ˆå…¨æµç¨‹å¤§æ¨¡å‹è‡ªåŠ¨æŠ½å–ï¼Œæ— é¢„è®¾ï¼Œæ— åˆ†è¯ï¼‰"""
    
    def __init__(self, text_file: str = "çº¢æ¥¼æ¢¦.txt"):
        """
        åˆå§‹åŒ–çŸ¥è¯†å›¾è°±æ„å»ºå™¨
        
        Args:
            text_file: çº¢æ¥¼æ¢¦æ–‡æœ¬æ–‡ä»¶è·¯å¾„
        """
        self.text_file = text_file
        self.text = ""
        self.entities = {}  # name -> Entity
        self.relations = []  # List[Relation]
        self.graph = nx.DiGraph()
        self.llm_client = None
        self._init_llm_client()
        self._load_text()

    def _init_llm_client(self):
        """åˆå§‹åŒ–LLMå®¢æˆ·ç«¯"""
        try:
            primary_api_key = os.getenv("ZHIPU_API_KEY")
            primary_base_url = os.getenv("ZHIPU_BASE_URL")
            primary_model = os.getenv("ZHIPU_MODEL", "glm-4-flash")
            
            fallback_api_key = os.getenv("GUIJI_API_KEY")
            fallback_base_url = os.getenv("GUIJI_BASE_URL")
            fallback_model = os.getenv("GUIJI_MODEL", "THUDM/GLM-4-9B-0414")
            
            if primary_api_key and primary_base_url:
                self.llm_client = AsyncFallbackOpenAIClient(
                    primary_api_key=primary_api_key,
                    primary_base_url=primary_base_url,
                    primary_model_name=primary_model,
                    fallback_api_key=fallback_api_key,
                    fallback_base_url=fallback_base_url,
                    fallback_model_name=fallback_model
                )
                print("âœ… LLMå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°LLMé…ç½®ï¼Œå°†ä½¿ç”¨åŸºç¡€NLPæ–¹æ³•")
        except Exception as e:
            print(f"âš ï¸ LLMå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
            
    def _load_text(self):
        """åŠ è½½çº¢æ¥¼æ¢¦æ–‡æœ¬"""
        if os.path.exists(self.text_file):
            with open(self.text_file, 'r', encoding='utf-8') as f:
                self.text = f.read()
            print(f"âœ… æˆåŠŸåŠ è½½æ–‡æœ¬ï¼Œæ€»é•¿åº¦: {len(self.text)} å­—ç¬¦")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æ–‡ä»¶: {self.text_file}")
    
    async def extract_entities_and_relations_with_llm(self, chunk_size: int = 1000, max_concurrency: int = 200) -> Tuple[Dict[str, Entity], List[Relation]]:
        """
        ä½¿ç”¨LLMè‡ªåŠ¨æŠ½å–å®ä½“å’Œå…³ç³»ï¼ˆå…¨æµç¨‹è‡ªåŠ¨ï¼Œæ— ä»»ä½•é¢„è®¾/åˆ†è¯/è§„åˆ™/è¯æ€§æ ‡æ³¨ï¼‰ï¼Œæ”¯æŒå¹¶å‘å¤„ç†ã€‚
        """
        if not self.llm_client:
            print("âš ï¸ LLMå®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè·³è¿‡LLMå®ä½“ä¸å…³ç³»æŠ½å–")
            return self.entities, self.relations
        print("\nğŸ¤– å¼€å§‹ç”¨LLMè‡ªåŠ¨æŠ½å–å®ä½“å’Œå…³ç³»...")
        splitter = SemanticSplitter(target_chunk_size=chunk_size)
        chunks = splitter.split_text(self.text)
        print(f"æ–‡æœ¬åˆ†ä¸º {len(chunks)} ä¸ªå—ï¼Œå¼€å§‹LLMå¹¶å‘å¤„ç†...")

        import asyncio
        semaphore = asyncio.Semaphore(max_concurrency)
        results = [None] * len(chunks)

        async def process_chunk(i, chunk):
            async with semaphore:
                try:
                    print(f"å¤„ç†ç¬¬ {i+1}/{len(chunks)} ä¸ªæ–‡æœ¬å—...")
                    entities, relations = await self._extract_entities_and_relations_from_chunk(chunk)
                    print(f"å— {i+1} æŠ½å–åˆ°å®ä½“: {len(entities)}ï¼Œå…³ç³»: {len(relations)}")
                    print(f"ç¤ºä¾‹å®ä½“: {entities[:1] if entities else 'æ— '}")
                    print(f"ç¤ºä¾‹å…³ç³»: {relations[:1] if relations else 'æ— '}")
                    results[i] = (entities, relations)
                except Exception as e:
                    print(f"å¤„ç†å— {i+1} æ—¶å‡ºé”™: {e}")
                    results[i] = ([], [])

        await asyncio.gather(*(process_chunk(i, chunk) for i, chunk in enumerate(chunks)))        # åˆå¹¶æ‰€æœ‰ç»“æœ
        for entities, relations in results:
            # åˆå¹¶å®ä½“
            if entities is None:
                entities = []
            if relations is None:
                relations = []
                
            for entity_data in entities:
                if not isinstance(entity_data, dict):
                    continue
                name = entity_data.get("name", "").strip()
                if name and len(name) >= 2:
                    # ç¡®ä¿ attributes æ˜¯å­—å…¸ç±»å‹
                    entity_attributes = entity_data.get("attributes", {})
                    if not isinstance(entity_attributes, dict):
                        entity_attributes = {}
                    
                    if name in self.entities:
                        self.entities[name].attributes.update(entity_attributes)
                    else:
                        # ç¡®ä¿ aliases å’Œ mentions ä¹Ÿæ˜¯æ­£ç¡®ç±»å‹
                        aliases = entity_data.get("aliases", [])
                        if not isinstance(aliases, list):
                            aliases = []
                        
                        mentions = entity_data.get("mentions", [])
                        if not isinstance(mentions, list):
                            mentions = []
                            
                        self.entities[name] = Entity(
                            name=name,
                            type=entity_data.get("type", "unknown"),
                            aliases=set(aliases),
                            attributes=entity_attributes,
                            mentions=mentions
                        )
            # åˆå¹¶å…³ç³»
            for rel in relations:
                if not isinstance(rel, dict):
                    continue
                if rel.get("source") and rel.get("target"):
                    self.relations.append(
                        Relation(
                            source=rel["source"],
                            target=rel["target"],
                            relation_type=rel.get("relation_type", "æœªçŸ¥"),
                            confidence=rel.get("confidence", 0.7),
                            context=rel.get("context", "")
                        )
                    )
        print(f"âœ… LLMå®ä½“ä¸å…³ç³»æŠ½å–å®Œæˆï¼Œå½“å‰å®ä½“æ•°: {len(self.entities)}ï¼Œå…³ç³»æ•°: {len(self.relations)}")
        return self.entities, self.relations
    
    async def _extract_entities_and_relations_from_chunk(self, chunk: str) -> Tuple[list, list]:
        """
        ç”¨LLMä»æ–‡æœ¬å—ä¸­æŠ½å–å®ä½“å’Œå…³ç³»ï¼Œè¦æ±‚è¾“å‡ºYAMLæ ¼å¼
        """
        prompt = HONGLOUMENG_KG_EXTRACTION_YAML_PROMPT.format(text=chunk[:800])
        try:
            response = await self.llm_client.chat_completions_create(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=4096
            )
            content = response.choices[0].message.content.strip()

            # ä¼˜å…ˆç”¨splitæå–markdownä»£ç å—ä¸­çš„yamlå†…å®¹
            if '```yaml' in content:
                parts = content.split('```yaml')
                if len(parts) > 1:
                    yaml_part = parts[1].split('```')[0]
                    cleaned_content = yaml_part.strip()
                else:
                    cleaned_content = content
            else:
                cleaned_content = self._clean_yaml_content(content)

            # å°è¯•è§£æYAML
            yaml_match = re.search(r'(entities:|relations:)[\s\S]*', cleaned_content, re.IGNORECASE)
            if yaml_match:
                yaml_text = yaml_match.group()
                try:
                    data = yaml.safe_load(yaml_text)
                    if isinstance(data, dict):
                        return data.get("entities", []), data.get("relations", [])
                except yaml.YAMLError as yaml_error:
                    print(f"YAMLè§£æé”™è¯¯: {yaml_error}")
                    # ç›´æ¥fallbackåˆ°æ­£åˆ™è§£æï¼Œå¹¶ä¼ é€’æŠ¥é”™ä¿¡æ¯
                    return await self._fallback_parse(cleaned_content, str(yaml_error))

            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°YAMLæ ¼å¼ï¼Œå°è¯•å¤‡ç”¨è§£æ
            return await self._fallback_parse(cleaned_content)

        except Exception as e:
            print(f"LLMæŠ½å–å®ä½“å’Œå…³ç³»æ—¶å‡ºé”™: {e}")
            return [], []
    
    async def _fallback_parse(self, content: str, error_msg: str = None) -> Tuple[list, list]:
        """
        ä½¿ç”¨LLMä¿®æ­£æ ¼å¼é”™è¯¯çš„å†…å®¹ä¸ºæ ‡å‡†YAMLå¹¶è§£æã€‚
        å¯é€‰åœ°å°†æŠ¥é”™ä¿¡æ¯ä¸€å¹¶è¾“å…¥ï¼Œä¾¿äºå¤§æ¨¡å‹ä¿®å¤ã€‚
        """        # æ„é€ ä¿®æ­£æç¤ºï¼ŒåŒ…å«æŠ¥é”™ä¿¡æ¯
        fix_prompt = (
            "ä½ åˆšæ‰è¾“å‡ºçš„å†…å®¹æ ¼å¼æœ‰è¯¯ï¼Œè¯·å°†ä¸‹é¢å†…å®¹ä¿®æ­£ä¸ºä¸¥æ ¼çš„ YAML æ ¼å¼ï¼Œå¹¶ç”¨ markdown ä»£ç å—åŒ…è£¹ï¼Œåªä¿ç•™ entities å’Œ relations å­—æ®µï¼Œ"
            "æ‰€æœ‰å­—ç¬¦ä¸²å¿…é¡»ç”¨è‹±æ–‡åŒå¼•å·åŒ…è£¹ã€‚ä¸è¦è¾“å‡ºä»»ä½•è§£é‡Šã€‚\n"
        )
        if error_msg:
            fix_prompt += f"\nYAMLè§£ææŠ¥é”™ä¿¡æ¯å¦‚ä¸‹ï¼š\n{error_msg}\n"
        fix_prompt += f"\nåŸå§‹å†…å®¹ï¼š\n{content[:2000]}"
        try:
            response = await self.llm_client.chat_completions_create(
                messages=[{"role": "user", "content": fix_prompt}],
                temperature=0.1,
                max_tokens=4096
            )
            fixed_content = response.choices[0].message.content.strip()
            # æå–yamlä»£ç å—
            if '```yaml' in fixed_content:
                parts = fixed_content.split('```yaml')
                if len(parts) > 1:
                    yaml_part = parts[1].split('```')[0]
                    cleaned_content = yaml_part.strip()
                else:
                    cleaned_content = fixed_content
            else:
                cleaned_content = fixed_content
            # è§£æ
            yaml_match = re.search(r'(entities:|relations:)[\s\S]*', cleaned_content, re.IGNORECASE)
            if yaml_match:
                yaml_text = yaml_match.group()
                try:
                    data = yaml.safe_load(yaml_text)
                    if isinstance(data, dict):
                        return data.get("entities", []), data.get("relations", [])
                except Exception as e:
                    print(f"äºŒæ¬¡LLMä¿®æ­£åYAMLè§£æä»å¤±è´¥: {e}")
            print("äºŒæ¬¡LLMä¿®æ­£åä»æ— æ³•è§£æï¼Œè¿”å›ç©º")
            return [], []
        except Exception as e:
            print(f"LLMäºŒæ¬¡ä¿®æ­£å†…å®¹æ—¶å‡ºé”™: {e}")
            return [], []
    def build_graph(self):
        """æ„å»ºçŸ¥è¯†å›¾è°±"""
        print("\nğŸ“Š å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
        
        # æ·»åŠ èŠ‚ç‚¹
        for name, entity in self.entities.items():
            # è¿‡æ»¤æ‰Noneå€¼çš„å±æ€§
            clean_attributes = {k: v for k, v in entity.attributes.items() if v is not None}
            
            node_attrs = {
                "type": entity.type or "unknown",
                "frequency": clean_attributes.get("frequency", 0),
                **clean_attributes
            }
            
            self.graph.add_node(name, **node_attrs)
        
        # æ·»åŠ è¾¹
        for relation in self.relations:
            if relation.source in self.graph and relation.target in self.graph:
                # ç¡®ä¿æ‰€æœ‰è¾¹å±æ€§éƒ½ä¸æ˜¯None
                edge_attrs = {
                    "relation_type": relation.relation_type or "unknown",
                    "confidence": relation.confidence if relation.confidence is not None else 0.5,
                    "context": (relation.context[:100] + "..." if len(relation.context) > 100 else relation.context) if relation.context else ""
                }
                
                self.graph.add_edge(
                    relation.source,
                    relation.target,
                    **edge_attrs
                )
        
        print(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆ")
        print(f"   èŠ‚ç‚¹æ•°: {self.graph.number_of_nodes()}")
        print(f"   è¾¹æ•°: {self.graph.number_of_edges()}")
    
    def analyze_graph(self) -> Dict[str, Any]:
        """åˆ†æçŸ¥è¯†å›¾è°±"""
        print("\nğŸ“ˆ å¼€å§‹å›¾è°±åˆ†æ...")
        
        analysis = {}
        
        # åŸºæœ¬ç»Ÿè®¡
        analysis["basic_stats"] = {
            "nodes": self.graph.number_of_nodes(),
            "edges": self.graph.number_of_edges(),
            "density": nx.density(self.graph)
        }
        
        # ä¸­å¿ƒæ€§åˆ†æ
        if self.graph.number_of_nodes() > 0:
            degree_centrality = nx.degree_centrality(self.graph)
            betweenness_centrality = nx.betweenness_centrality(self.graph)
            
            analysis["top_degree_centrality"] = sorted(
                degree_centrality.items(), key=lambda x: x[1], reverse=True
            )[:10]
            
            analysis["top_betweenness_centrality"] = sorted(
                betweenness_centrality.items(), key=lambda x: x[1], reverse=True
            )[:10]
        
        # å®ä½“ç±»å‹åˆ†å¸ƒ
        entity_types = defaultdict(int)
        for node in self.graph.nodes():
            node_type = self.graph.nodes[node].get("type", "unknown")
            entity_types[node_type] += 1
        
        analysis["entity_type_distribution"] = dict(entity_types)
        
        # å…³ç³»ç±»å‹åˆ†å¸ƒ
        relation_types = defaultdict(int)
        for edge in self.graph.edges():
            relation_type = self.graph.edges[edge].get("relation_type", "unknown")
            relation_types[relation_type] += 1
        
        analysis["relation_type_distribution"] = dict(relation_types)
        
        print("âœ… å›¾è°±åˆ†æå®Œæˆ")
        return analysis
    
    def visualize_graph(self, output_file: str = "hongloumeng_kg.png", 
                       max_nodes: int = 50, min_frequency: int = 3):
        """å¯è§†åŒ–çŸ¥è¯†å›¾è°±"""
        print(f"\nğŸ¨ å¼€å§‹å¯è§†åŒ–çŸ¥è¯†å›¾è°±ï¼Œä¿å­˜åˆ° {output_file}")
        
        # åˆ›å»ºå­å›¾ï¼ˆåªæ˜¾ç¤ºé«˜é¢‘å®ä½“ï¼‰
        important_nodes = [
            node for node in self.graph.nodes()
            if self.graph.nodes[node].get("frequency", 0) >= min_frequency
        ][:max_nodes]
        
        subgraph = self.graph.subgraph(important_nodes)
        
        if subgraph.number_of_nodes() == 0:
            print("âš ï¸ æ²¡æœ‰ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹ç”¨äºå¯è§†åŒ–")
            return
        
        plt.figure(figsize=(20, 16))
        
        # è®¡ç®—å¸ƒå±€
        try:
            pos = nx.spring_layout(subgraph, k=2, iterations=50)
        except:
            pos = nx.random_layout(subgraph)
        
        # æ ¹æ®å®ä½“ç±»å‹è®¾ç½®é¢œè‰²
        color_map = {
            "person": "#FF6B6B",
            "place": "#4ECDC4", 
            "object": "#45B7D1",
            "concept": "#96CEB4",
            "unknown": "#FECA57"
        }
        
        node_colors = [
            color_map.get(subgraph.nodes[node].get("type", "unknown"), "#FECA57")
            for node in subgraph.nodes()
        ]
        
        # æ ¹æ®é¢‘ç‡è®¾ç½®èŠ‚ç‚¹å¤§å°
        node_sizes = [
            max(100, min(2000, subgraph.nodes[node].get("frequency", 1) * 50))
            for node in subgraph.nodes()
        ]
        
        # ç»˜åˆ¶å›¾
        nx.draw(
            subgraph,
            pos,
            with_labels=True,
            node_color=node_colors,
            node_size=node_sizes,
            font_size=8,
            font_weight='bold',
            edge_color='gray',
            alpha=0.7,
            arrowsize=20
        )
        
        plt.title("çº¢æ¥¼æ¢¦çŸ¥è¯†å›¾è°±", fontsize=16, fontweight='bold')
        
        # æ·»åŠ å›¾ä¾‹
        legend_elements = [
            plt.scatter([], [], c=color, s=100, label=type_name)
            for type_name, color in color_map.items()
            if any(subgraph.nodes[node].get("type") == type_name for node in subgraph.nodes())
        ]
        plt.legend(handles=legend_elements, loc='upper right')
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… çŸ¥è¯†å›¾è°±å¯è§†åŒ–å®Œæˆï¼Œä¿å­˜åˆ° {output_file}")
    
    def save_results(self, output_dir: str = "kg_output"):
        """ä¿å­˜ç»“æœ"""
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nğŸ’¾ å¼€å§‹ä¿å­˜ç»“æœåˆ° {output_dir}")
        
        # ä¿å­˜å®ä½“
        entities_data = {
            name: {
                "type": entity.type,
                "aliases": list(entity.aliases),
                "attributes": entity.attributes,
                "mentions": entity.mentions[:5]  # é™åˆ¶æ•°é‡
            }
            for name, entity in self.entities.items()
        }
        
        with open(os.path.join(output_dir, "entities.json"), 'w', encoding='utf-8') as f:
            json.dump(entities_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å…³ç³»
        relations_data = [
            {
                "source": relation.source,
                "target": relation.target,
                "relation_type": relation.relation_type,
                "confidence": relation.confidence,
                "context": relation.context[:200] + "..." if len(relation.context) > 200 else relation.context
            }
            for relation in self.relations
        ]
        
        with open(os.path.join(output_dir, "relations.json"), 'w', encoding='utf-8') as f:
            json.dump(relations_data, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜å›¾è°±
        nx.write_gexf(self.graph, os.path.join(output_dir, "knowledge_graph.gexf"))
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis = self.analyze_graph()
        with open(os.path.join(output_dir, "analysis.json"), 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç»“æœä¿å­˜å®Œæˆ")
        print(f"   å®ä½“æ–‡ä»¶: {os.path.join(output_dir, 'entities.json')}")
        print(f"   å…³ç³»æ–‡ä»¶: {os.path.join(output_dir, 'relations.json')}")
        print(f"   å›¾è°±æ–‡ä»¶: {os.path.join(output_dir, 'knowledge_graph.gexf')}")
        print(f"   åˆ†ææ–‡ä»¶: {os.path.join(output_dir, 'analysis.json')}")
    
    async def build_complete_kg(self, output_dir: str = "kg_output"):
        """å®Œæ•´çš„çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹ï¼ˆå…¨è‡ªåŠ¨ï¼Œæ— é¢„è®¾ï¼Œå®ä½“å’Œå…³ç³»å‡ç”¨LLMï¼‰"""
        print("ğŸš€ å¼€å§‹æ„å»ºçº¢æ¥¼æ¢¦çŸ¥è¯†å›¾è°±ï¼ˆå…¨æµç¨‹å¤§æ¨¡å‹è‡ªåŠ¨æŠ½å–ï¼‰")
        print("=" * 50)
        
        try:
            await self.extract_entities_and_relations_with_llm()
            self.build_graph()
            analysis = self.analyze_graph()
            self.visualize_graph()
            self.save_results(output_dir)
            self._print_summary(analysis)
            
            print("\nğŸ‰ çº¢æ¥¼æ¢¦çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼")
            
        except Exception as e:
            print(f"âŒ æ„å»ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
        finally:
            if self.llm_client:
                await self.llm_client.close()
    
    def _print_summary(self, analysis: Dict[str, Any]):
        """æ‰“å°æ€»ç»“ä¿¡æ¯"""
        print("\nğŸ“Š çŸ¥è¯†å›¾è°±æ„å»ºæ€»ç»“")
        print("=" * 30)
        
        print(f"ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡:")
        basic_stats = analysis.get("basic_stats", {})
        print(f"   å®ä½“æ•°é‡: {basic_stats.get('nodes', 0)}")
        print(f"   å…³ç³»æ•°é‡: {basic_stats.get('edges', 0)}")
        print(f"   å›¾å¯†åº¦: {basic_stats.get('density', 0):.4f}")
        
        print(f"\nğŸ† æ ¸å¿ƒäººç‰©ï¼ˆæŒ‰åº¦ä¸­å¿ƒæ€§ï¼‰:")
        top_degree = analysis.get("top_degree_centrality", [])
        for i, (name, centrality) in enumerate(top_degree[:5], 1):
            print(f"   {i}. {name}: {centrality:.4f}")
        
        print(f"\nğŸŒ‰ å…³é”®ä¸­ä»‹ï¼ˆæŒ‰ä»‹æ•°ä¸­å¿ƒæ€§ï¼‰:")
        top_betweenness = analysis.get("top_betweenness_centrality", [])
        for i, (name, centrality) in enumerate(top_betweenness[:5], 1):
            print(f"   {i}. {name}: {centrality:.4f}")
        
        print(f"\nğŸ“Š å®ä½“ç±»å‹åˆ†å¸ƒ:")
        entity_dist = analysis.get("entity_type_distribution", {})
        for entity_type, count in entity_dist.items():
            print(f"   {entity_type}: {count}")
        
        print(f"\nğŸ”— å…³ç³»ç±»å‹åˆ†å¸ƒ:")
        relation_dist = analysis.get("relation_type_distribution", {})
        for relation_type, count in sorted(relation_dist.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"   {relation_type}: {count}")


async def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    text_file = "çº¢æ¥¼æ¢¦.txt"
    if not os.path.exists(text_file):
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {text_file}")
        return
    
    # åˆ›å»ºçŸ¥è¯†å›¾è°±æ„å»ºå™¨
    kg_builder = HongLouMengKGBuilder(text_file)
    
    # æ„å»ºçŸ¥è¯†å›¾è°±
    await kg_builder.build_complete_kg(
        output_dir="hongloumeng_kg_output"
    )


if __name__ == "__main__":
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())
