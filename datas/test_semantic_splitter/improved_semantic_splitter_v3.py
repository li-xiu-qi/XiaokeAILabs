"""
æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²å™¨ V3
ä½¿ç”¨spacyè¿›è¡Œå¥å­åˆ†å‰²ï¼Œè§£å†³åˆ†å—å¤§å°ä¸å‡åŒ€çš„é—®é¢˜ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
"""

import os
import re
from typing import List, Dict, Any, Optional, Tuple
import numpy as np
import spacy
from dotenv import load_dotenv
from sentence_transformers import util
from embedding_models import get_default_embedding_model

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def get_spacy_model():
    """è·å–spaCyæ¨¡å‹ï¼Œå¦‚æœå°šæœªåŠ è½½åˆ™åŠ è½½æ¨¡å‹"""
    if not hasattr(get_spacy_model, "_SPACY_MODEL"):
        try:
            get_spacy_model._SPACY_MODEL = spacy.load("xx_sent_ud_sm")
            print("âœ… spacyæ¨¡å‹ 'xx_sent_ud_sm' åŠ è½½æˆåŠŸ")
            print(f"ğŸ·ï¸ æ¨¡å‹è¯­è¨€: {get_spacy_model._SPACY_MODEL.lang}")
            print(f"ğŸ“ æ¨¡å‹ç»„ä»¶: {list(get_spacy_model._SPACY_MODEL.pipe_names)}")
        except Exception as e:
            print(f"âŒ spacyæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£… 'xx_sent_ud_sm' æ¨¡å‹")
            print("   å¯é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…: python -m spacy download xx_sent_ud_sm")
            raise RuntimeError("è¯·ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼Œå¹¶ä¸”åŒ…å«æ‰€æœ‰å¿…è¦çš„æ¨¡å‹æ–‡ä»¶ã€‚")
    return get_spacy_model._SPACY_MODEL

class ImprovedSemanticSplitterV2:
    """
    æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²å™¨ V3
    ç‰¹ç‚¹ï¼š
    1. ä½¿ç”¨spacyè¿›è¡Œæ™ºèƒ½å¥å­åˆ†å‰²
    2. ä¸¥æ ¼æ§åˆ¶å—å¤§å°
    3. ä¿æŒå¥å­å®Œæ•´æ€§
    4. æ”¯æŒä¸­è‹±æ–‡æ··åˆæ–‡æœ¬
    5. é¿å…è¿‡åº¦åˆ†å‰²
    6. å¤„ç†ä¸­æ–‡æ–‡æœ¬ç‰¹å¾
    """
    
    def __init__(
        self,
        embed_model=None,
        target_chunk_size=850,      # ç›®æ ‡å—å¤§å°
        min_chunk_size=400,         # æœ€å°å—å¤§å°
        max_chunk_size=1200,        # æœ€å¤§å—å¤§å°
        similarity_threshold=0.35,  # ç›¸ä¼¼åº¦é˜ˆå€¼
        overlap_size=50,            # é‡å å¤§å°
    ):
        """
        åˆå§‹åŒ–æ”¹è¿›çš„è¯­ä¹‰åˆ†å‰²å™¨
        """
        if embed_model is None:
            self.embed_model = get_default_embedding_model()
        else:
            self.embed_model = embed_model
            
        self.target_chunk_size = target_chunk_size
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size
        self.similarity_threshold = similarity_threshold
        self.overlap_size = overlap_size # æœ¬æ¬¡æ”¹è¿›ä¸­æœªä½¿ç”¨é‡å å¤§å°ï¼Œä½†ä¿ç•™ä»¥å¤‡å°†æ¥ä½¿ç”¨
        
        # åˆå§‹åŒ–spacyæ¨¡å‹ç”¨äºå¥å­åˆ†å‰²
        print("\nğŸ”¤ æ­£åœ¨åˆå§‹åŒ–spacyæ¨¡å‹ç”¨äºå¥å­åˆ†å‰²...")
        try:
            self.nlp_model = get_spacy_model()
            # è®¾ç½®æœ€å¤§å¤„ç†é•¿åº¦ï¼Œå¤„ç†é•¿æ–‡æœ¬
            self.nlp_model.max_length = 3000000
        except Exception as e:
            print(f"âŒ spacyæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            raise

    def split_text(self, text: str) -> List[str]:
        """
        ä¸»è¦åˆ†å‰²æ–¹æ³•
        """
        print(f"å¼€å§‹åˆ†å‰²æ–‡æœ¬ï¼Œæ€»é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # 1. é¢„å¤„ç†ï¼šæŒ‰å¥å­åˆ†å‰²
        sentences = self._split_into_sentences(text)
        print(f"é¢„å¤„ç†å®Œæˆï¼Œå¾—åˆ° {len(sentences)} ä¸ªå¥å­")
        
        # 2. æŒ‰ç›®æ ‡å¤§å°è¿›è¡Œåˆå§‹åˆ†ç»„
        initial_chunks = self._create_initial_chunks(sentences)
        print(f"åˆå§‹åˆ†ç»„å®Œæˆï¼Œå¾—åˆ° {len(initial_chunks)} ä¸ªåˆå§‹å—")
        
        # 3. è¯­ä¹‰ä¼˜åŒ–ï¼šè°ƒæ•´åˆ†å—è¾¹ç•Œ
        optimized_chunks = self._semantic_optimize_chunks(initial_chunks)
        print(f"è¯­ä¹‰ä¼˜åŒ–å®Œæˆï¼Œæœ€ç»ˆå¾—åˆ° {len(optimized_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # 4. éªŒè¯å’Œåå¤„ç†
        final_chunks = self._post_process_chunks(optimized_chunks)
        return final_chunks
    
    def _split_into_sentences(self, text: str) -> List[str]:
        """
        ä½¿ç”¨spacyå°†æ–‡æœ¬åˆ†å‰²æˆå¥å­ï¼Œä¿æŒå¥å­å®Œæ•´æ€§
        """
        # æ¸…ç†æ–‡æœ¬ï¼šç»Ÿä¸€ç©ºç™½å­—ç¬¦ï¼Œä¿ç•™æ®µè½ç»“æ„
        text = re.sub(r'[ \t]+', ' ', text)  # ç»Ÿä¸€ç©ºæ ¼å’Œåˆ¶è¡¨ç¬¦ä¸ºå•ä¸ªç©ºæ ¼
        text = re.sub(r'\n{3,}', '\n\n', text)  # å¤šä¸ªæ¢è¡Œç¬¦å‹ç¼©ä¸ºä¸¤ä¸ª
        text = text.strip()
        
        print(f"ä½¿ç”¨spacyè¿›è¡Œå¥å­åˆ†å‰²ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # ä½¿ç”¨spacyè¿›è¡Œå¥å­åˆ†å‰²
        doc = self.nlp_model(text)
        spacy_sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]
        
        print(f"spacyåˆå§‹åˆ†å‰²å¾—åˆ° {len(spacy_sentences)} ä¸ªå¥å­")
        
        # è¿›ä¸€æ­¥å¤„ç†ï¼Œå¤„ç†æ®µè½åˆ†éš”å’Œæ¢è¡Œ
        final_sentences = []
        for sent in spacy_sentences:
            # æŒ‰æ®µè½åˆ†éš”ç¬¦ï¼ˆåŒæ¢è¡Œï¼‰åˆ†å‰²
            paragraph_splits = sent.split('\n\n')
            for paragraph in paragraph_splits:
                paragraph = paragraph.strip()
                if not paragraph:
                    continue
                    
                # æŒ‰å•æ¢è¡Œåˆ†å‰²ï¼ˆä¿æŒæ®µè½å†…çš„è¡Œåˆ†éš”ï¼‰
                line_splits = paragraph.split('\n')
                for line in line_splits:
                    line = line.strip()
                    if line and len(line) > 2:  # è¿‡æ»¤æ‰è¿‡çŸ­çš„ç‰‡æ®µ
                        final_sentences.append(line)
        
        # åå¤„ç†ï¼šåˆå¹¶è¿‡çŸ­çš„å¥å­
        filtered_sentences = []
        for sentence in final_sentences:
            if len(sentence) < 15 and filtered_sentences:  # å¤ªçŸ­çš„å¥å­åˆå¹¶åˆ°å‰é¢
                filtered_sentences[-1] += sentence
            else:
                filtered_sentences.append(sentence)
        
        print(f"æœ€ç»ˆå¤„ç†åå¾—åˆ° {len(filtered_sentences)} ä¸ªå¥å­")
        return filtered_sentences
    
    def _create_initial_chunks(self, sentences: List[str]) -> List[str]:
        """
        æ ¹æ®ç›®æ ‡å¤§å°åˆ›å»ºåˆå§‹å—ï¼Œä¸¥æ ¼ä¿æŒå¥å­å®Œæ•´æ€§
        """
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            # å¦‚æœå•ä¸ªå¥å­å°±è¶…è¿‡æœ€å¤§å¤§å°ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ†å‰²
            if len(sentence) > self.max_chunk_size:
                # å…ˆä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # åˆ†å‰²é•¿å¥å­
                sub_chunks = self._split_long_sentence(sentence)
                chunks.extend(sub_chunks)
                continue
                
            # è®¡ç®—åŠ ä¸Šæ–°å¥å­åçš„æ€»é•¿åº¦
            test_chunk = current_chunk + sentence if current_chunk else sentence
            
            # å¦‚æœè¶…è¿‡ç›®æ ‡å¤§å°ï¼Œæ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ†å‰²
            if len(test_chunk) > self.target_chunk_size:
                # å¦‚æœå½“å‰å—å¤ªå°ï¼Œæˆ–è€…åŠ ä¸Šæ–°å¥å­ä¸ä¼šè¶…è¿‡æœ€å¤§é™åˆ¶ï¼Œå°±ç»§ç»­æ·»åŠ 
                if len(current_chunk) < self.min_chunk_size or len(test_chunk) <= self.max_chunk_size:
                    current_chunk = test_chunk
                else:
                    # ä¿å­˜å½“å‰å—ï¼Œå¼€å§‹æ–°å—
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence
            else:
                # æ­£å¸¸æ·»åŠ å¥å­
                current_chunk = test_chunk
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        return chunks
    
    def _split_long_sentence(self, sentence: str) -> List[str]:
        """
        åˆ†å‰²è¿‡é•¿çš„å¥å­ï¼Œåœ¨åˆé€‚çš„æ ‡ç‚¹ç¬¦å·å¤„åˆ‡åˆ†
        """
        if len(sentence) <= self.max_chunk_size:
            return [sentence]
            
        chunks = []
        
        # å®šä¹‰å¯ä»¥ä½œä¸ºåˆ†å‰²ç‚¹çš„æ ‡ç‚¹ç¬¦å·ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
        split_patterns = [
            (r'[ï¼Œã€]', 'ï¼Œ'),       # é€—å·ã€é¡¿å·
            (r'[ï¼šï¼›]', 'ï¼š'),       # å†’å·ã€åˆ†å·  
            (r'[\s]+', ' '),         # ç©ºç™½å­—ç¬¦
        ]
        
        # å°è¯•æŒ‰ä¸åŒçš„æ ‡ç‚¹ç¬¦å·åˆ†å‰²
        for pattern, joiner in split_patterns:
            if len(sentence) <= self.max_chunk_size:
                break
                
            # æŒ‰æ ‡ç‚¹ç¬¦å·åˆ†å‰²
            parts = re.split(f'({pattern})', sentence)
            if len(parts) > 1:
                current_chunk = ""
                
                i = 0
                while i < len(parts):
                    part = parts[i]
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯åˆ†éš”ç¬¦
                    if re.match(pattern, part):
                        current_chunk += part
                        i += 1
                        continue
                    
                    # æ£€æŸ¥åŠ ä¸Šè¿™éƒ¨åˆ†æ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
                    test_chunk = current_chunk + part
                    if len(test_chunk) > self.target_chunk_size and current_chunk:
                        # ä¿å­˜å½“å‰å—
                        chunks.append(current_chunk.strip())
                        current_chunk = part
                    else:
                        current_chunk = test_chunk
                    
                    i += 1
                
                if current_chunk:
                    chunks.append(current_chunk.strip())
                
                # å¦‚æœæˆåŠŸåˆ†å‰²äº†ï¼Œè¿”å›ç»“æœ
                valid_chunks = [chunk for chunk in chunks if chunk.strip()]
                if len(valid_chunks) > 1:
                    return valid_chunks
                else:
                    chunks = []  # é‡ç½®ï¼Œå°è¯•ä¸‹ä¸€ä¸ªåˆ†å‰²æ¨¡å¼
        
        # å¦‚æœæ— æ³•é€šè¿‡æ ‡ç‚¹ç¬¦å·åˆç†åˆ†å‰²ï¼ŒæŒ‰å­—ç¬¦æ•°å¼ºåˆ¶åˆ†å‰²
        chunks = []
        start = 0
        while start < len(sentence):
            end = min(start + self.target_chunk_size, len(sentence))
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€æ®µï¼Œå°è¯•åœ¨é™„è¿‘æ‰¾åˆ°åˆé€‚çš„åˆ†å‰²ç‚¹
            if end < len(sentence):
                # åœ¨ç›®æ ‡ä½ç½®å‰åå¯»æ‰¾åˆé€‚çš„åˆ†å‰²ç‚¹
                best_split = end
                search_range = min(100, (end - start) // 4)  # æœç´¢èŒƒå›´
                
                for offset in range(search_range):
                    # å…ˆå‘åæ‰¾
                    pos = end + offset
                    if pos < len(sentence) and sentence[pos] in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ã€ï¼š':
                        best_split = pos + 1
                        break
                    
                    # å†å‘å‰æ‰¾
                    pos = end - offset
                    if pos > start and sentence[pos] in 'ï¼Œã€‚ï¼ï¼Ÿï¼›ã€ï¼š':
                        best_split = pos + 1
                        break
                
                chunks.append(sentence[start:best_split].strip())
                start = best_split
            else:
                chunks.append(sentence[start:end].strip())
                break
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _semantic_optimize_chunks(self, chunks: List[str]) -> List[str]:
        """
        ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ä¼˜åŒ–åˆ†å—
        """
        if len(chunks) <= 1:
            return chunks
        
        print("å¼€å§‹è¯­ä¹‰ä¼˜åŒ–...")
        
        # è®¡ç®—æ¯ä¸ªå—çš„åµŒå…¥å‘é‡
        embeddings = self.embed_model.encode(chunks)
        
        # è®¡ç®—ç›¸é‚»å—ä¹‹é—´çš„ç›¸ä¼¼åº¦å¹¶ä¼˜åŒ–
        optimized_chunks = []
        i = 0
        
        while i < len(chunks):
            current_chunk = chunks[i]
            
            # å°è¯•ä¸ä¸‹ä¸€ä¸ªå—åˆå¹¶
            if i + 1 < len(chunks):
                next_chunk = chunks[i + 1]
                
                # æ£€æŸ¥åˆå¹¶åçš„å¤§å°
                merged_size = len(current_chunk) + len(next_chunk)
                
                if merged_size <= self.max_chunk_size:
                    # è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                    similarity = self._calculate_similarity(embeddings[i], embeddings[i+1])
                    
                    # å¦‚æœç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼ï¼Œè¿›è¡Œåˆå¹¶
                    if similarity > self.similarity_threshold:
                        merged_chunk = current_chunk + next_chunk
                        optimized_chunks.append(merged_chunk)
                        i += 2  # è·³è¿‡ä¸‹ä¸€ä¸ªå—
                        continue
            
            # ä¸åˆå¹¶ï¼Œä¿ç•™å½“å‰å—
            optimized_chunks.append(current_chunk)
            i += 1
        
        return optimized_chunks
    
    def _calculate_similarity(self, embedding1, embedding2):
        """è®¡ç®—ä¸¤ä¸ªåµŒå…¥å‘é‡ä¹‹é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
        sim = util.cos_sim(np.array(embedding1), np.array(embedding2))
        return float(sim.item())
    
    def _post_process_chunks(self, chunks: List[str]) -> List[str]:
        """
        åå¤„ç†ï¼šç¡®ä¿æ‰€æœ‰å—éƒ½ç¬¦åˆå¤§å°è¦æ±‚
        """
        processed_chunks = []
        
        for chunk in chunks:
            chunk = chunk.strip()
            if not chunk:
                continue
                
            # å¦‚æœå—å¤ªå°ï¼Œå°è¯•ä¸å‰ä¸€ä¸ªå—åˆå¹¶
            if len(chunk) < self.min_chunk_size and processed_chunks:
                potential_merged = processed_chunks[-1] + chunk
                if len(potential_merged) <= self.max_chunk_size:
                    processed_chunks[-1] = potential_merged
                    continue
            
            # å¦‚æœå—å¤ªå¤§ï¼Œè¿›ä¸€æ­¥åˆ†å‰²
            if len(chunk) > self.max_chunk_size:
                sub_chunks = self._split_long_sentence(chunk)
                processed_chunks.extend(sub_chunks)
            else:
                processed_chunks.append(chunk)
        
        return processed_chunks


def save_chunks_to_files(chunks: List[str], output_dir: str, prefix: str = "çº¢æ¥¼æ¢¦"):
    """
    å°†åˆ†å—ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    """
    os.makedirs(output_dir, exist_ok=True)
    
    print(f"\nå¼€å§‹ä¿å­˜ {len(chunks)} ä¸ªæ–‡æœ¬å—åˆ° {output_dir}")
    
    # ç»Ÿè®¡ä¿¡æ¯
    sizes = [len(chunk) for chunk in chunks]
    print(f"\nåˆ†å—ç»Ÿè®¡ä¿¡æ¯:")
    print(f"å¹³å‡å¤§å°: {np.mean(sizes):.1f} å­—ç¬¦")
    print(f"æœ€å°å¤§å°: {min(sizes)} å­—ç¬¦")
    print(f"æœ€å¤§å¤§å°: {max(sizes)} å­—ç¬¦")
    print(f"æ ‡å‡†å·®: {np.std(sizes):.1f}")
    
    # å¤§å°åˆ†å¸ƒç»Ÿè®¡
    size_ranges = [
        (0, 300, "è¿‡å°"),
        (300, 600, "è¾ƒå°"),
        (600, 900, "é€‚ä¸­"),
        (900, 1200, "è¾ƒå¤§"),
        (1200, float('inf'), "è¿‡å¤§")
    ]
    
    print(f"\nå¤§å°åˆ†å¸ƒ:")
    for min_size, max_size, label in size_ranges:
        count = sum(1 for size in sizes if min_size <= size < max_size)
        percentage = count / len(sizes) * 100
        print(f"{label} ({min_size}-{max_size if max_size != float('inf') else 'âˆ'}): {count} ä¸ª ({percentage:.1f}%)")
    
    # ä¿å­˜æ–‡ä»¶
    for i, chunk in enumerate(chunks, 1):
        filename = f"{prefix}_chunk_{i:03d}.txt"
        filepath = os.path.join(output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(chunk)
        
        print(f"ä¿å­˜æ–‡ä»¶: {filename} (é•¿åº¦: {len(chunk)} å­—ç¬¦)")
    
    print(f"\næ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_dir}")


if __name__ == "__main__":
    # æµ‹è¯•æ”¹è¿›çš„åˆ†å‰²å™¨
    
    # è¯»å–çº¢æ¥¼æ¢¦æ–‡æœ¬
    text_file = "çº¢æ¥¼æ¢¦.txt"
    if os.path.exists(text_file):
        with open(text_file, 'r', encoding='utf-8') as f:
            text = f.read()
        
        print(f"è¯»å–æ–‡æœ¬æ–‡ä»¶: {text_file}")
        print(f"æ–‡æœ¬æ€»é•¿åº¦: {len(text)} å­—ç¬¦")
        
        # åˆ›å»ºæ”¹è¿›çš„åˆ†å‰²å™¨
        splitter = ImprovedSemanticSplitterV2(
            target_chunk_size=850,
            min_chunk_size=400,
            max_chunk_size=1200,
            similarity_threshold=0.35
        )
        
        # æ‰§è¡Œåˆ†å‰²
        chunks = splitter.split_text(text)
        
        # ä¿å­˜ç»“æœ
        output_dir = "çº¢æ¥¼æ¢¦_æ”¹è¿›åˆ†å‰²ç»“æœ_v3"
        save_chunks_to_files(chunks, output_dir)
        
    else:
        print(f"æ–‡æœ¬æ–‡ä»¶ {text_file} ä¸å­˜åœ¨")
