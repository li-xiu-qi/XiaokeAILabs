{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ca31875",
   "metadata": {},
   "source": [
    "# ONNX教程 - 第5部分：使用ONNX Runtime进行推理\n",
    "\n",
    "在本教程中，我们将学习如何使用ONNX Runtime加载ONNX模型并进行高效的推理。ONNX Runtime是一个跨平台的高性能推理引擎，可以在多种硬件上运行ONNX模型。\n",
    "\n",
    "## 目标\n",
    "\n",
    "1. 了解ONNX Runtime的基本概念和功能\n",
    "2. 学习如何创建和配置推理会话\n",
    "3. 使用ONNX Runtime进行模型推理\n",
    "4. 评估和比较不同执行提供程序和批次大小下的性能\n",
    "5. 可视化推理结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e54c6",
   "metadata": {},
   "source": [
    "## 1. 导入必要的库和工具"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c40f40e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Runtime已成功安装，版本：1.21.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 设置中文字体以便正确显示图表\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "plt.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题\n",
    "\n",
    "# 检查ONNX Runtime是否安装\n",
    "try:\n",
    "    import onnxruntime\n",
    "    print(f\"ONNX Runtime已成功安装，版本：{onnxruntime.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"错误：未安装ONNX Runtime。请使用 'pip install onnxruntime' 安装。\")\n",
    "\n",
    "# 创建必要的目录\n",
    "os.makedirs('./models', exist_ok=True)\n",
    "os.makedirs('./results', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea887825",
   "metadata": {},
   "source": [
    "## 2. 检查可用的执行提供程序\n",
    "\n",
    "ONNX Runtime支持多种执行提供程序，如CPU、CUDA（NVIDIA GPU）、DirectML（Windows上的GPU）等。让我们首先检查可用的执行提供程序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3321fa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "可用的执行提供程序：\n",
      "  [0] AzureExecutionProvider\n",
      "  [1] CPUExecutionProvider\n",
      "\n",
      "注意：未检测到CUDA支持。模型将在CPU上运行。\n",
      "如需GPU加速，请安装支持CUDA的ONNX Runtime版本。\n"
     ]
    }
   ],
   "source": [
    "def get_available_providers():\n",
    "    \"\"\"获取ONNX Runtime可用的执行提供程序\"\"\"\n",
    "    providers = onnxruntime.get_available_providers()\n",
    "    print(\"\\n可用的执行提供程序：\")\n",
    "    for i, provider in enumerate(providers):\n",
    "        print(f\"  [{i}] {provider}\")\n",
    "    \n",
    "    # 检查CUDA是否可用\n",
    "    if 'CUDAExecutionProvider' in providers:\n",
    "        print(\"\\nCUDA可用于加速！模型将默认使用GPU执行\")\n",
    "    else:\n",
    "        print(\"\\n注意：未检测到CUDA支持。模型将在CPU上运行。\")\n",
    "        print(\"如需GPU加速，请安装支持CUDA的ONNX Runtime版本。\")\n",
    "    \n",
    "    return providers\n",
    "\n",
    "# 检查可用的执行提供程序\n",
    "providers = get_available_providers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4b839c",
   "metadata": {},
   "source": [
    "## 3. 加载和检查ONNX模型\n",
    "\n",
    "我们将加载之前部分教程中导出的MNIST模型，并创建一个ONNX Runtime推理会话。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22fc28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_session(model_path, providers=None):\n",
    "    \"\"\"创建ONNX Runtime推理会话\"\"\"\n",
    "    try:\n",
    "        # 如果未指定提供程序，使用默认设置\n",
    "        if providers is None:\n",
    "            session = onnxruntime.InferenceSession(model_path)\n",
    "        else:\n",
    "            # 创建会话选项\n",
    "            options = onnxruntime.SessionOptions()\n",
    "            # 启用优化\n",
    "            options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "            # 启用内存模式\n",
    "            options.enable_mem_pattern = True\n",
    "            # 启用并行执行\n",
    "            options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL\n",
    "            # 创建推理会话\n",
    "            session = onnxruntime.InferenceSession(\n",
    "                model_path, \n",
    "                options, \n",
    "                providers=providers\n",
    "            )\n",
    "        \n",
    "        # 打印模型输入信息\n",
    "        print(\"\\n模型输入：\")\n",
    "        for i, input_node in enumerate(session.get_inputs()):\n",
    "            print(f\"  [{i}] 名称: {input_node.name}\")\n",
    "            print(f\"      形状: {input_node.shape}\")\n",
    "            print(f\"      类型: {input_node.type}\")\n",
    "        \n",
    "        # 打印模型输出信息\n",
    "        print(\"\\n模型输出：\")\n",
    "        for i, output_node in enumerate(session.get_outputs()):\n",
    "            print(f\"  [{i}] 名称: {output_node.name}\")\n",
    "            print(f\"      形状: {output_node.shape}\")\n",
    "            print(f\"      类型: {output_node.type}\")\n",
    "        \n",
    "        return session\n",
    "    except Exception as e:\n",
    "        print(f\"创建推理会话时出错：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 设置ONNX模型路径并创建会话\n",
    "model_path = './models/mnist_cnn.onnx'\n",
    "session = create_inference_session(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979febf",
   "metadata": {},
   "source": [
    "### 3.1 如果模型不存在，检查或生成替代模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30df3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查模型是否存在，如果不存在给出提示\n",
    "if session is None and not os.path.exists(model_path):\n",
    "    print(f\"注意：模型文件 {model_path} 不存在\")\n",
    "    print(\"请先运行第3部分教程导出ONNX模型，或查看模型文件是否位于正确的路径。\")\n",
    "    print(\"如果您有其他ONNX模型文件，也可以修改上面的model_path变量。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb468f",
   "metadata": {},
   "source": [
    "## 4. 准备和加载测试数据\n",
    "\n",
    "现在我们准备一些测试数据来评估模型的性能。我们将使用MNIST测试集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af30425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist_test_data(num_samples=10, data_dir='./data'):\n",
    "    \"\"\"加载MNIST测试数据\"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST数据集的均值和标准差\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # 下载并加载MNIST测试集\n",
    "        test_dataset = datasets.MNIST(data_dir, train=False, download=True, transform=transform)\n",
    "        \n",
    "        # 选择指定数量的样本\n",
    "        data_loader = []\n",
    "        for i in range(min(num_samples, len(test_dataset))):\n",
    "            image, label = test_dataset[i]\n",
    "            data_loader.append((image.numpy(), label))\n",
    "        \n",
    "        print(f\"成功加载{len(data_loader)}个MNIST测试样本\")\n",
    "        return data_loader\n",
    "    except Exception as e:\n",
    "        print(f\"加载MNIST数据时出错：{str(e)}\")\n",
    "        return None\n",
    "\n",
    "# 加载测试数据\n",
    "test_data = load_mnist_test_data(num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af0a253",
   "metadata": {},
   "source": [
    "## 5. 运行基本推理\n",
    "\n",
    "现在我们运行单个样本的基本推理，看看模型如何工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2011e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(session, input_data, input_name):\n",
    "    \"\"\"使用ONNX Runtime运行推理\"\"\"\n",
    "    # 准备输入，确保是4维的 [batch_size, channels, height, width]\n",
    "    if input_data.ndim == 3:  # 如果输入是3维的，添加批次维度\n",
    "        input_data = np.expand_dims(input_data, axis=0)\n",
    "    \n",
    "    # 准备输入\n",
    "    inputs = {input_name: input_data}\n",
    "    \n",
    "    # 运行推理\n",
    "    start_time = time.time()\n",
    "    outputs = session.run(None, inputs)\n",
    "    inference_time = (time.time() - start_time) * 1000  # 转换为毫秒\n",
    "    \n",
    "    return outputs, inference_time\n",
    "\n",
    "# 确保会话和测试数据已成功加载\n",
    "if session is not None and test_data is not None:\n",
    "    # 获取第一个样本\n",
    "    sample_image, sample_label = test_data[0]\n",
    "    print(f\"样本实际标签: {sample_label}\")\n",
    "    \n",
    "    # 获取输入名称\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    # 运行推理\n",
    "    outputs, inference_time = run_inference(session, sample_image, input_name)\n",
    "    \n",
    "    # 打印结果\n",
    "    prediction = np.argmax(outputs[0], axis=1)[0]\n",
    "    print(f\"模型预测: {prediction}\")\n",
    "    print(f\"推理时间: {inference_time:.2f}毫秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e50cf",
   "metadata": {},
   "source": [
    "## 6. 性能评估\n",
    "\n",
    "让我们评估模型在整个测试集上的性能，包括准确率和推理速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cf0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_performance(session, test_data, input_name, num_runs=5):\n",
    "    \"\"\"评估模型性能（速度和准确性）\"\"\"\n",
    "    inference_times = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for image, label in test_data:\n",
    "        # 多次运行以获得平均推理时间\n",
    "        times = []\n",
    "        predictions = []\n",
    "        \n",
    "        for _ in range(num_runs):\n",
    "            # 运行单次推理\n",
    "            outputs, inf_time = run_inference(session, image, input_name)\n",
    "            times.append(inf_time)\n",
    "            \n",
    "            # 获取预测结果（对数概率的索引）\n",
    "            prediction = np.argmax(outputs[0], axis=1)[0]\n",
    "            predictions.append(prediction)\n",
    "        \n",
    "        # 计算平均推理时间\n",
    "        avg_time = sum(times) / len(times)\n",
    "        inference_times.append(avg_time)\n",
    "        \n",
    "        # 检查预测是否正确（使用最常见的预测结果）\n",
    "        most_common_prediction = max(set(predictions), key=predictions.count)\n",
    "        if most_common_prediction == label:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    # 计算整体性能指标\n",
    "    accuracy = correct_predictions / len(test_data) * 100\n",
    "    avg_inference_time = sum(inference_times) / len(inference_times)\n",
    "    \n",
    "    print(\"\\n性能评估结果：\")\n",
    "    print(f\"准确率: {accuracy:.2f}% ({correct_predictions}/{len(test_data)})\")\n",
    "    print(f\"平均推理时间: {avg_inference_time:.2f}毫秒\")\n",
    "    print(f\"最短推理时间: {min(inference_times):.2f}毫秒\")\n",
    "    print(f\"最长推理时间: {max(inference_times):.2f}毫秒\")\n",
    "    \n",
    "    return accuracy, avg_inference_time\n",
    "\n",
    "# 评估模型性能\n",
    "if session is not None and test_data is not None:\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    accuracy, avg_time = evaluate_performance(session, test_data, input_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e79b87",
   "metadata": {},
   "source": [
    "## 7. 批量推理性能测试\n",
    "\n",
    "在实际应用中，通常会对多个样本进行批量推理，以提高吞吐量。让我们测试不同批次大小下的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_inference(session, batch_size=16):\n",
    "    \"\"\"批量推理演示\"\"\"\n",
    "    print(f\"\\n批量推理演示 (批次大小: {batch_size}):\")\n",
    "    \n",
    "    # 准备批量输入\n",
    "    batch_input = np.random.randn(batch_size, 1, 28, 28).astype(np.float32)\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    # 批量推理\n",
    "    start_time = time.time()\n",
    "    outputs = session.run(None, {input_name: batch_input})\n",
    "    batch_time = (time.time() - start_time) * 1000  # 转换为毫秒\n",
    "    \n",
    "    print(f\"批量推理时间: {batch_time:.2f}毫秒\")\n",
    "    print(f\"每样本平均时间: {batch_time / batch_size:.2f}毫秒\")\n",
    "    \n",
    "    return batch_time\n",
    "\n",
    "# 测试不同批次大小\n",
    "if session is not None:\n",
    "    run_batch_inference(session, batch_size=1)   # 单个样本\n",
    "    run_batch_inference(session, batch_size=16)  # 小批量\n",
    "    run_batch_inference(session, batch_size=64)  # 大批量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a45ad8e",
   "metadata": {},
   "source": [
    "## 8. 比较不同执行提供程序\n",
    "\n",
    "如果有多个执行提供程序可用（例如CPU和CUDA），让我们比较它们的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6b4eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_execution_providers(model_path, input_data):\n",
    "    \"\"\"比较不同执行提供程序的性能\"\"\"\n",
    "    providers = onnxruntime.get_available_providers()\n",
    "    results = []\n",
    "    \n",
    "    if len(providers) <= 1:\n",
    "        print(\"\\n只有一个执行提供程序可用，无法进行比较。\")\n",
    "        return None\n",
    "    \n",
    "    print(\"\\n比较不同执行提供程序的性能：\")\n",
    "    \n",
    "    # 准备单个输入样本\n",
    "    sample_input = input_data[0][0]  # 第一个样本的图像\n",
    "    \n",
    "    for provider in providers:\n",
    "        try:\n",
    "            # 创建使用特定提供程序的会话\n",
    "            print(f\"\\n使用 {provider}:\")\n",
    "            session = onnxruntime.InferenceSession(\n",
    "                model_path, \n",
    "                providers=[provider]\n",
    "            )\n",
    "            \n",
    "            input_name = session.get_inputs()[0].name\n",
    "            \n",
    "            # 预热运行\n",
    "            _ = session.run(None, {input_name: sample_input})\n",
    "            \n",
    "            # 测量性能（多次运行）\n",
    "            times = []\n",
    "            for _ in range(10):\n",
    "                _, inf_time = run_inference(session, sample_input, input_name)\n",
    "                times.append(inf_time)\n",
    "            \n",
    "            avg_time = sum(times) / len(times)\n",
    "            results.append((provider, avg_time))\n",
    "            \n",
    "            print(f\"平均推理时间: {avg_time:.2f}毫秒\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{provider} 运行失败: {str(e)}\")\n",
    "    \n",
    "    # 打印性能比较结果\n",
    "    if len(results) > 1:\n",
    "        print(\"\\n执行提供程序性能比较：\")\n",
    "        # 按推理时间排序\n",
    "        results.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # 使用最快的提供程序作为基准\n",
    "        baseline_provider, baseline_time = results[0]\n",
    "        \n",
    "        for provider, avg_time in results:\n",
    "            speedup = baseline_time / avg_time if avg_time > 0 else float('inf')\n",
    "            print(f\"{provider.ljust(30)}: {avg_time:.2f}毫秒 (速度比: {speedup:.2f}x)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 比较不同执行提供程序的性能\n",
    "if session is not None and test_data is not None and len(onnxruntime.get_available_providers()) > 1:\n",
    "    provider_results = compare_execution_providers(model_path, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0ff1e",
   "metadata": {},
   "source": [
    "## 9. 可视化推理结果\n",
    "\n",
    "让我们可视化一些预测结果，看看模型表现如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de27a644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(test_data, predictions, num_samples=5):\n",
    "    \"\"\"可视化模型预测结果\"\"\"\n",
    "    # 选择要显示的样本数量\n",
    "    num_samples = min(num_samples, len(test_data))\n",
    "    \n",
    "    # 设置中文字体\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "    plt.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题\n",
    "    \n",
    "    # 创建图形\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        image, label = test_data[i]\n",
    "        # 从输出数组中提取单个预测值作为标量\n",
    "        prediction = int(np.argmax(predictions[i][0], axis=1)[0])\n",
    "        \n",
    "        # 将图像从NCHW格式转换为显示格式\n",
    "        display_image = image.reshape(28, 28)\n",
    "        \n",
    "        # 添加子图\n",
    "        plt.subplot(1, num_samples, i + 1)\n",
    "        plt.imshow(display_image, cmap='gray')\n",
    "        \n",
    "        # 设置标题（绿色表示预测正确，红色表示预测错误）\n",
    "        if prediction == label:\n",
    "            plt.title(f\"预测: {prediction}\\n实际: {label}\", color='green')\n",
    "        else:\n",
    "            plt.title(f\"预测: {prediction}\\n实际: {label}\", color='red')\n",
    "        \n",
    "        plt.axis('off')  # 隐藏坐标轴\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图像\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "    plt.savefig('./results/mnist_predictions.png')\n",
    "    print(f\"\\n预测可视化结果已保存到 ./results/mnist_predictions.png\")\n",
    "    \n",
    "    # 显示图形\n",
    "    plt.show()\n",
    "\n",
    "# 收集预测结果并可视化\n",
    "if session is not None and test_data is not None:\n",
    "    # 收集多个样本的预测结果\n",
    "    predictions = []\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    for image, _ in test_data[:5]:  # 只使用前5个样本\n",
    "        output, _ = run_inference(session, image, input_name)\n",
    "        predictions.append(output)\n",
    "    \n",
    "    # 可视化结果\n",
    "    visualize_predictions(test_data, predictions, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face50b4",
   "metadata": {},
   "source": [
    "## 10. 可视化不同批次大小的性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b2e90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_batch_performance(session):\n",
    "    \"\"\"可视化不同批次大小下的性能比较\"\"\"\n",
    "    # 设置中文字体\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 设置中文字体为黑体\n",
    "    plt.rcParams['axes.unicode_minus'] = False    # 解决负号显示问题\n",
    "    \n",
    "    # 设置要测试的批次大小\n",
    "    batch_sizes = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "    total_times = []      # 总推理时间\n",
    "    per_sample_times = [] # 每样本平均时间\n",
    "    \n",
    "    # 获取输入名称\n",
    "    input_name = session.get_inputs()[0].name\n",
    "    \n",
    "    # 测量每个批次大小的性能\n",
    "    print(\"\\n测量不同批次大小的性能...\")\n",
    "    for batch_size in batch_sizes:\n",
    "        # 准备批量输入\n",
    "        batch_input = np.random.randn(batch_size, 1, 28, 28).astype(np.float32)\n",
    "        \n",
    "        # 预热运行\n",
    "        _ = session.run(None, {input_name: batch_input})\n",
    "        \n",
    "        # 多次运行以获得平均时间\n",
    "        times = []\n",
    "        for _ in range(5):  # 运行5次取平均值\n",
    "            start_time = time.time()\n",
    "            _ = session.run(None, {input_name: batch_input})\n",
    "            inference_time = (time.time() - start_time) * 1000  # 毫秒\n",
    "            times.append(inference_time)\n",
    "        \n",
    "        # 计算平均时间\n",
    "        avg_time = sum(times) / len(times)\n",
    "        avg_per_sample = avg_time / batch_size\n",
    "        \n",
    "        total_times.append(avg_time)\n",
    "        per_sample_times.append(avg_per_sample)\n",
    "        \n",
    "        print(f\"批次大小: {batch_size}, 总时间: {avg_time:.2f}毫秒, 每样本: {avg_per_sample:.2f}毫秒\")\n",
    "    \n",
    "    # 创建图表\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # 第一个子图：总推理时间\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(batch_sizes, total_times, 'o-', color='blue', linewidth=2, markersize=8)\n",
    "    plt.title('不同批次大小的总推理时间', fontsize=14)\n",
    "    plt.xlabel('批次大小', fontsize=12)\n",
    "    plt.ylabel('推理时间 (毫秒)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(batch_sizes)\n",
    "    \n",
    "    # 第二个子图：每样本平均时间\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(batch_sizes, per_sample_times, 'o-', color='green', linewidth=2, markersize=8)\n",
    "    plt.title('不同批次大小的每样本平均推理时间', fontsize=14)\n",
    "    plt.xlabel('批次大小', fontsize=12)\n",
    "    plt.ylabel('每样本推理时间 (毫秒)', fontsize=12)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(batch_sizes)\n",
    "    \n",
    "    # 使显示更美观\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # 保存图表\n",
    "    os.makedirs('./results', exist_ok=True)\n",
    "    plt.savefig('./results/batch_performance_comparison.png')\n",
    "    print(f\"\\n批次性能对比图已保存到 ./results/batch_performance_comparison.png\")\n",
    "    \n",
    "    # 显示图表\n",
    "    plt.show()\n",
    "    \n",
    "    # 返回结果数据\n",
    "    return {\n",
    "        \"batch_sizes\": batch_sizes,\n",
    "        \"total_times\": total_times,\n",
    "        \"per_sample_times\": per_sample_times\n",
    "    }\n",
    "\n",
    "# 可视化批次性能对比\n",
    "if session is not None:\n",
    "    batch_results = visualize_batch_performance(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debdd95e",
   "metadata": {},
   "source": [
    "## 11. 会话选项说明\n",
    "\n",
    "ONNX Runtime提供了许多会话选项，可以用来优化推理性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083a98db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_session_options():\n",
    "    \"\"\"解释ONNX Runtime会话选项\"\"\"\n",
    "    print(\"\\nONNX Runtime会话选项说明：\")\n",
    "    options = [\n",
    "        (\"graph_optimization_level\", \"图优化级别，可设置为：DISABLE_ALL, ENABLE_BASIC, ENABLE_EXTENDED, ENABLE_ALL\"),\n",
    "        (\"intra_op_num_threads\", \"算子内并行的线程数，设置为处理器核心数通常效果最佳\"),\n",
    "        (\"inter_op_num_threads\", \"算子间并行的线程数\"),\n",
    "        (\"execution_mode\", \"执行模式：ORT_SEQUENTIAL（顺序）或ORT_PARALLEL（并行）\"),\n",
    "        (\"enable_profiling\", \"是否启用性能分析\"),\n",
    "        (\"enable_mem_pattern\", \"是否启用内存模式优化\"),\n",
    "        (\"enable_cpu_mem_arena\", \"是否启用CPU内存竞技场优化\"),\n",
    "        (\"session_log_severity_level\", \"日志级别：0(Verbose), 1(Info), 2(Warning), 3(Error), 4(Fatal)\")\n",
    "    ]\n",
    "    \n",
    "    for option, description in options:\n",
    "        print(f\"  - {option.ljust(25)}: {description}\")\n",
    "\n",
    "# 解释会话选项\n",
    "explain_session_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81e0390",
   "metadata": {},
   "source": [
    "## 12. 总结\n",
    "\n",
    "在本教程中，我们学习了如何：\n",
    "\n",
    "1. 使用ONNX Runtime加载和运行ONNX模型\n",
    "2. 配置推理会话和执行提供程序\n",
    "3. 评估模型在不同条件下的性能\n",
    "4. 可视化推理结果和性能对比\n",
    "\n",
    "ONNX Runtime是一个强大的跨平台推理引擎，可以在各种硬件上高效运行ONNX模型。通过合理配置和优化，我们可以获得最佳的推理性能。\n",
    "\n",
    "在下一个教程中，我们将学习如何优化ONNX模型以提高性能，包括图优化、量化等技术。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyonnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
