#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ONNXæ•™ç¨‹ - ç¬¬6éƒ¨åˆ†ï¼šONNXæ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²
è¿™ä¸ªè„šæœ¬å±•ç¤ºå¦‚ä½•å¯¹ONNXæ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä»‹ç»å„ç§éƒ¨ç½²é€‰é¡¹
"""

import os
import time
import numpy as np
import onnx
from onnx import shape_inference

# æ£€æŸ¥ä¾èµ–åº“
try:
    import onnxruntime
    print(f"ONNX Runtimeå·²æˆåŠŸå®‰è£…ï¼Œç‰ˆæœ¬ï¼š{onnxruntime.__version__}")
except ImportError:
    print("é”™è¯¯ï¼šæœªå®‰è£…ONNX Runtimeã€‚è¯·ä½¿ç”¨ 'pip install onnxruntime' å®‰è£…ã€‚")
    exit(1)

# å°è¯•å¯¼å…¥onnxruntime-toolsï¼ˆç”¨äºä¼˜åŒ–ï¼‰
try:
    from onnxruntime.tools.onnxruntime_tools import optimizer
    ort_tools_available = True
    print("ONNX Runtime Toolså·²æˆåŠŸå¯¼å…¥")
except ImportError:
    ort_tools_available = False
    print("æç¤ºï¼šæœªå®‰è£…'onnxruntime-tools'ï¼Œä¸€äº›ä¼˜åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨")

# å°è¯•å¯¼å…¥onnx-simplifier
try:
    import onnxsim
    onnxsim_available = True
    print("ONNX Simplifierå·²æˆåŠŸå¯¼å…¥")
except ImportError:
    onnxsim_available = False
    print("æç¤ºï¼šæœªå®‰è£…'onnx-simplifier'ï¼Œæ¨¡å‹ç®€åŒ–åŠŸèƒ½å°†ä¸å¯ç”¨ã€‚ä½¿ç”¨ 'pip install onnx-simplifier' å®‰è£…")

def check_model_size(model_path):
    """æ£€æŸ¥ONNXæ¨¡å‹æ–‡ä»¶å¤§å°"""
    size_bytes = os.path.getsize(model_path)
    size_mb = size_bytes / (1024 * 1024)
    print(f"æ¨¡å‹å¤§å°: {size_mb:.2f} MB ({size_bytes:,} å­—èŠ‚)")
    return size_bytes

def count_ops_params(model_path):
    """ç»Ÿè®¡æ¨¡å‹ä¸­çš„æ“ä½œæ•°å’Œå‚æ•°æ•°é‡"""
    model = onnx.load(model_path)
    graph = model.graph
    
    # è®¡ç®—æ“ä½œæ•°
    op_count = len(graph.node)
    op_types = {}
    for node in graph.node:
        if node.op_type in op_types:
            op_types[node.op_type] += 1
        else:
            op_types[node.op_type] = 1
    
    # è®¡ç®—å‚æ•°æ•°é‡
    param_count = 0
    for initializer in graph.initializer:
        shape = initializer.dims
        count = 1
        for dim in shape:
            count *= dim
        param_count += count
    
    print(f"æ“ä½œæ•°: {op_count}")
    print(f"å‚æ•°æ•°é‡: {param_count:,}")
    print("æ“ä½œç±»å‹åˆ†å¸ƒ:")
    for op_type, count in sorted(op_types.items(), key=lambda x: x[1], reverse=True):
        print(f"  - {op_type}: {count}")
    
    return op_count, param_count, op_types

def optimize_with_onnxsim(model_path, output_path):
    """ä½¿ç”¨ONNX-Simplifierä¼˜åŒ–æ¨¡å‹"""
    if not onnxsim_available:
        print("é”™è¯¯ï¼šONNX Simplifieræœªå®‰è£…")
        return None
    
    try:
        print("ä½¿ç”¨ONNX Simplifierä¼˜åŒ–æ¨¡å‹...")
        model = onnx.load(model_path)
        model_opt, check = onnxsim.simplify(
            model,
            skip_fuse_bn=False,
            skip_reshape=False,
            skip_shape_inference=False
        )
        
        if not check:
            print("è­¦å‘Šï¼šç®€åŒ–åçš„æ¨¡å‹å¯èƒ½ä¸åŸå§‹æ¨¡å‹ä¸ç­‰æ•ˆ")
        
        # ä¿å­˜ç®€åŒ–åçš„æ¨¡å‹
        onnx.save(model_opt, output_path)
        print(f"ç®€åŒ–åçš„æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}")
        
        return output_path
    except Exception as e:
        print(f"ä½¿ç”¨ONNX Simplifierä¼˜åŒ–æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
        return None

def optimize_with_ort(model_path, output_path, optimization_level=99):
    """ä½¿ç”¨ONNX Runtimeä¼˜åŒ–æ¨¡å‹ï¼ˆä½¿ç”¨é…ç½®é€‰é¡¹è€Œéå·¥å…·åŒ…ï¼‰"""
    try:
        print("ä½¿ç”¨ONNX Runtimeä¼šè¯é€‰é¡¹ä¼˜åŒ–æ¨¡å‹...")
        
        # åˆ›å»ºä¼šè¯é€‰é¡¹
        options = onnxruntime.SessionOptions()
        # è®¾ç½®ä¼˜åŒ–çº§åˆ«
        options.graph_optimization_level = onnxruntime.GraphOptimizationLevel.ORT_ENABLE_ALL
        # å¯ç”¨å†…å­˜ä¼˜åŒ–
        options.enable_mem_pattern = True
        # å¯ç”¨CPUå†…å­˜arena
        options.enable_cpu_mem_arena = True
        # è®¾ç½®æ‰§è¡Œæ¨¡å¼ä¸ºå¹¶è¡Œ
        options.execution_mode = onnxruntime.ExecutionMode.ORT_PARALLEL
        # å¯é€‰ï¼šè®¾ç½®çº¿ç¨‹æ•°
        options.intra_op_num_threads = 4
        
        # åˆ›å»ºä¼šè¯ï¼ˆè¿™ä¼šåŠ è½½å¹¶ä¼˜åŒ–æ¨¡å‹ï¼‰
        session = onnxruntime.InferenceSession(model_path, options)
        
        # æ¨¡å‹å·²ç»è¢«åŠ è½½å¹¶ä¼˜åŒ–ï¼Œä½†ONNX Runtimeæ²¡æœ‰ç›´æ¥æä¾›å¯¼å‡ºä¼˜åŒ–åæ¨¡å‹çš„åŠŸèƒ½
        print("æ³¨æ„ï¼šONNX Runtimeå·²åº”ç”¨ä¼˜åŒ–ï¼Œä½†ä¼˜åŒ–åçš„æ¨¡å‹æ— æ³•ç›´æ¥å¯¼å‡º")
        print("      ä¼˜åŒ–å·²åº”ç”¨äºå†…å­˜ä¸­çš„æ¨¡å‹ï¼Œæœªæ¥æ¨ç†ä¼šä½¿ç”¨è¿™äº›ä¼˜åŒ–")
        
        return model_path  # è¿”å›åŸå§‹è·¯å¾„ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å¯¼å‡ºä¼˜åŒ–æ¨¡å‹
    except Exception as e:
        print(f"ä½¿ç”¨ONNX Runtimeä¼˜åŒ–æ¨¡å‹æ—¶å‡ºé”™ï¼š{str(e)}")
        return None

def convert_to_int8(model_path, output_path):
    """æ¨¡æ‹Ÿå°†æ¨¡å‹é‡åŒ–ä¸ºINT8ï¼ˆæ¼”ç¤ºç”¨é€”ï¼‰
    æ³¨æ„ï¼šçœŸæ­£çš„é‡åŒ–éœ€è¦æ ¡å‡†æ•°æ®é›†å’Œæ›´å¤æ‚çš„è¿‡ç¨‹
    """
    print("æ³¨æ„ï¼šæ­¤ç¤ºä¾‹ä»…ç”¨äºæ¼”ç¤ºç›®çš„ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œéœ€è¦ä½¿ç”¨å®Œæ•´çš„é‡åŒ–æµç¨‹å’Œæ ¡å‡†æ•°æ®é›†ã€‚")
    
    try:
        # æˆ‘ä»¬åªæ˜¯åœ¨è¿™é‡Œç®€å•åœ°æè¿°é‡åŒ–æ­¥éª¤ï¼Œè€Œä¸æ˜¯çœŸæ­£æ‰§è¡Œå®ƒ
        print("\nINT8é‡åŒ–è¿‡ç¨‹åŒ…æ‹¬ä»¥ä¸‹æ­¥éª¤ï¼š")
        print("1. æ”¶é›†æ ¡å‡†æ•°æ®é›†ï¼ˆä»£è¡¨æ€§æ ·æœ¬ï¼‰")
        print("2. è®¡ç®—æ¯ä¸ªtensorçš„åŠ¨æ€èŒƒå›´")
        print("3. å°†æµ®ç‚¹å€¼ç¼©æ”¾åˆ°INT8èŒƒå›´")
        print("4. é‡åŒ–æƒé‡å’Œæ¿€æ´»å€¼")
        
        # è¿”å›åŸå§‹æ¨¡å‹è·¯å¾„
        print("\nè¿™ä¸ªæ¼”ç¤ºä¸ä¼šçœŸæ­£æ‰§è¡Œé‡åŒ–ã€‚åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¯·ä½¿ç”¨ï¼š")
        print("- ONNX Runtimeçš„é‡åŒ–å·¥å…·")
        print("- TensorRTçš„é‡åŒ–API")
        print("- OpenVINOçš„é‡åŒ–å·¥å…·")
        
        return model_path
    except Exception as e:
        print(f"æ¼”ç¤ºé‡åŒ–æµç¨‹æ—¶å‡ºé”™ï¼š{str(e)}")
        return None

def benchmark_model(model_path, num_iterations=100):
    """å¯¹æ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•"""
    try:
        print(f"\nå¯¹æ¨¡å‹ {os.path.basename(model_path)} è¿›è¡ŒåŸºå‡†æµ‹è¯•...")
        
        # åˆ›å»ºæ¨ç†ä¼šè¯
        session = onnxruntime.InferenceSession(model_path)
        
        # è·å–è¾“å…¥åç§°å’Œå½¢çŠ¶
        input_name = session.get_inputs()[0].name
        input_shape = session.get_inputs()[0].shape
        
        # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
        # æ³¨æ„ï¼šå¯¹äºMNISTæ¨¡å‹ï¼Œè¾“å…¥å½¢çŠ¶åº”è¯¥æ˜¯[batch, 1, 28, 28]
        batch_size = 1
        if 'batch_size' in input_shape or input_shape[0] == -1:
            # å¤„ç†åŠ¨æ€æ‰¹é‡å¤§å°
            if len(input_shape) == 4:  # å‡è®¾æ˜¯NCHWæ ¼å¼
                input_data = np.random.randn(batch_size, input_shape[1], input_shape[2], input_shape[3]).astype(np.float32)
            else:
                # å›é€€åˆ°é»˜è®¤MNISTè¾“å…¥
                input_data = np.random.randn(batch_size, 1, 28, 28).astype(np.float32)
        else:
            # ä½¿ç”¨å›ºå®šå½¢çŠ¶
            input_data = np.random.randn(*input_shape).astype(np.float32)
        
        # é¢„çƒ­è¿è¡Œ
        print("é¢„çƒ­æ¨ç†...")
        for _ in range(10):
            session.run(None, {input_name: input_data})
        
        # åŸºå‡†æµ‹è¯•
        print(f"è¿è¡Œ {num_iterations} æ¬¡è¿­ä»£çš„åŸºå‡†æµ‹è¯•...")
        start_time = time.time()
        for _ in range(num_iterations):
            session.run(None, {input_name: input_data})
        end_time = time.time()
        
        # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
        total_time = end_time - start_time
        avg_time = total_time / num_iterations * 1000  # æ¯«ç§’
        fps = num_iterations / total_time
        
        print(f"æ€»æ—¶é—´: {total_time:.2f} ç§’")
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time:.2f} æ¯«ç§’/å¼ ")
        print(f"ååé‡: {fps:.2f} FPS (æ¯ç§’æ¨ç†æ¬¡æ•°)")
        
        return avg_time, fps
    except Exception as e:
        print(f"è¿è¡ŒåŸºå‡†æµ‹è¯•æ—¶å‡ºé”™ï¼š{str(e)}")
        return None, None

def explain_deployment_options():
    """è§£é‡ŠONNXæ¨¡å‹çš„ä¸åŒéƒ¨ç½²é€‰é¡¹"""
    print("\n" + "=" * 50)
    print("ONNXæ¨¡å‹éƒ¨ç½²é€‰é¡¹")
    print("=" * 50)
    
    # æœåŠ¡å™¨éƒ¨ç½²é€‰é¡¹
    print("\næœåŠ¡å™¨éƒ¨ç½²:")
    server_options = [
        ("ONNX Runtime Server", "æ€§èƒ½ä¼˜ç§€ï¼Œæ”¯æŒå¤šç§ç¡¬ä»¶åŠ é€Ÿï¼Œé€‚åˆå¤§å¤šæ•°æœåŠ¡å™¨åœºæ™¯"),
        ("TensorRT", "è‹±ä¼Ÿè¾¾GPUä¸Šæ€§èƒ½æœ€ä½³ï¼Œé€‚åˆé«˜ååé‡è¦æ±‚åœºæ™¯"),
        ("OpenVINO", "è‹±ç‰¹å°”ç¡¬ä»¶ä¸Šæ€§èƒ½æœ€ä½³ï¼ŒCPUå’ŒVPUä¼˜åŒ–"),
        ("TorchServe", "æä¾›æ¨¡å‹æœåŠ¡ï¼Œæ”¯æŒå¤šç§æ ¼å¼åŒ…æ‹¬ONNX"),
        ("Triton Inference Server", "é€‚åˆç”Ÿäº§ç¯å¢ƒçš„æ¨¡å‹æœåŠ¡å™¨ï¼Œæ”¯æŒå¤šç§æ ¼å¼")
    ]
    
    for option, desc in server_options:
        print(f"  - {option.ljust(25)}: {desc}")
    
    # è¾¹ç¼˜è®¾å¤‡éƒ¨ç½²é€‰é¡¹
    print("\nè¾¹ç¼˜è®¾å¤‡éƒ¨ç½²:")
    edge_options = [
        ("ONNX Runtime Mobile", "é€‚åˆç§»åŠ¨è®¾å¤‡ï¼Œå ç”¨ç©ºé—´å°"),
        ("TensorFlow Lite", "å¯ä»¥è½¬æ¢ONNXæ¨¡å‹ï¼Œé€‚åˆç§»åŠ¨è®¾å¤‡"),
        ("OpenVINO", "é€‚åˆIntelè¾¹ç¼˜è®¾å¤‡ï¼ˆNCS2ç­‰ï¼‰"),
        ("TensorRT", "é€‚åˆJetsonç­‰è‹±ä¼Ÿè¾¾è¾¹ç¼˜è®¾å¤‡"),
        ("CoreML", "é€‚åˆè‹¹æœè®¾å¤‡ï¼Œå¯ä»ONNXè½¬æ¢")
    ]
    
    for option, desc in edge_options:
        print(f"  - {option.ljust(25)}: {desc}")
    
    # æµè§ˆå™¨éƒ¨ç½²é€‰é¡¹
    print("\næµè§ˆå™¨éƒ¨ç½²:")
    web_options = [
        ("ONNX.js", "ç›´æ¥åœ¨æµè§ˆå™¨ä¸­è¿è¡ŒONNXæ¨¡å‹"),
        ("TensorFlow.js", "å¯ä»¥è½¬æ¢ONNXæ¨¡å‹ï¼Œåœ¨æµè§ˆå™¨ä¸­è¿è¡Œ"),
        ("WebNN", "åˆ©ç”¨Web Neural Network APIï¼Œç¡¬ä»¶åŠ é€Ÿ")
    ]
    
    for option, desc in web_options:
        print(f"  - {option.ljust(25)}: {desc}")

def explain_optimization_techniques():
    """è§£é‡ŠONNXæ¨¡å‹ä¼˜åŒ–æŠ€æœ¯"""
    print("\n" + "=" * 50)
    print("ONNXæ¨¡å‹ä¼˜åŒ–æŠ€æœ¯")
    print("=" * 50)
    
    techniques = [
        ("å›¾ä¼˜åŒ–", "åˆå¹¶æ“ä½œã€åˆ é™¤å†—ä½™èŠ‚ç‚¹ã€å¸¸é‡æŠ˜å ç­‰"),
        ("ç®—å­èåˆ", "å°†å¤šä¸ªå°ç®—å­èåˆä¸ºä¸€ä¸ªæ›´é«˜æ•ˆçš„ç®—å­"),
        ("é‡åŒ–", "å°†æµ®ç‚¹è¿ç®—é™ä¸ºä½ç²¾åº¦æ•´æ•°è¿ç®—ï¼ˆFP32â†’INT8/FP16ï¼‰"),
        ("å‰ªæ", "ç§»é™¤å¯¹è¾“å‡ºå½±å“å°çš„æƒé‡å’Œè¿æ¥"),
        ("çŸ¥è¯†è’¸é¦", "ä½¿ç”¨å¤§æ¨¡å‹è®­ç»ƒå°æ¨¡å‹"),
        ("åŠ¨æ€å½¢çŠ¶ä¼˜åŒ–", "é’ˆå¯¹ä¸åŒè¾“å…¥å¤§å°ä¼˜åŒ–æ‰§è¡Œè·¯å¾„"),
        ("å†…å­˜ä¼˜åŒ–", "å‡å°‘å³°å€¼å†…å­˜ä½¿ç”¨ï¼Œé‡ç”¨ç¼“å†²åŒº")
    ]
    
    for technique, desc in techniques:
        print(f"  - {technique.ljust(25)}: {desc}")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("ONNXæ•™ç¨‹ - ç¬¬6éƒ¨åˆ†ï¼šONNXæ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²")
    print("=" * 50)
    
    # è®¾ç½®ONNXæ¨¡å‹è·¯å¾„ï¼ˆä½¿ç”¨ä¹‹å‰å¯¼å‡ºçš„MNISTæ¨¡å‹ï¼‰
    model_path = './models/mnist_cnn.onnx'
    
    # 1. æ£€æŸ¥åŸå§‹æ¨¡å‹å¤§å°å’Œå¤æ‚åº¦
    print("\n[1] æ£€æŸ¥åŸå§‹æ¨¡å‹")
    if not os.path.exists(model_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
        print("è¯·å…ˆè¿è¡Œç¬¬3éƒ¨åˆ†æ•™ç¨‹ä»¥å¯¼å‡ºONNXæ¨¡å‹ã€‚")
        return
    
    original_size = check_model_size(model_path)
    op_count, param_count, _ = count_ops_params(model_path)
    
    # 2. ä½¿ç”¨ONNX Simplifierä¼˜åŒ–æ¨¡å‹
    print("\n[2] ä½¿ç”¨ONNX Simplifierä¼˜åŒ–æ¨¡å‹")
    simplified_model_path = './models/mnist_cnn_simplified.onnx'
    if onnxsim_available:
        simplified_path = optimize_with_onnxsim(model_path, simplified_model_path)
        if simplified_path:
            simplified_size = check_model_size(simplified_path)
            print(f"æ¨¡å‹å¤§å°å‡å°‘: {(original_size - simplified_size) / original_size * 100:.2f}%")
            count_ops_params(simplified_path)
    else:
        print("è·³è¿‡ONNX Simplifierä¼˜åŒ–ï¼Œå› ä¸ºåº“æœªå®‰è£…ã€‚")
        simplified_path = model_path
    
    # 3. ä½¿ç”¨ONNX Runtimeä¼˜åŒ–æ¨¡å‹
    print("\n[3] ä½¿ç”¨ONNX Runtimeä¼˜åŒ–æ¨¡å‹")
    ort_model_path = './models/mnist_cnn_ort_optimized.onnx'
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬å®é™…ä¸Šæ— æ³•å¯¼å‡ºä¼˜åŒ–åçš„æ¨¡å‹ï¼Œä½†ä¼šåº”ç”¨å†…å­˜ä¸­çš„ä¼˜åŒ–
    optimize_with_ort(simplified_path, ort_model_path)
    
    # 4. æ¨¡æ‹ŸINT8é‡åŒ–ï¼ˆä»…æ¼”ç¤ºï¼‰
    print("\n[4] æ¨¡æ‹ŸINT8é‡åŒ–")
    quantized_model_path = './models/mnist_cnn_int8.onnx'
    convert_to_int8(simplified_path, quantized_model_path)
    
    # 5. åŸºå‡†æµ‹è¯•æ¯”è¾ƒä¸åŒæ¨¡å‹
    print("\n[5] åŸºå‡†æµ‹è¯•æ¯”è¾ƒ")
    print("\na) åŸå§‹æ¨¡å‹æ€§èƒ½ï¼š")
    orig_time, orig_fps = benchmark_model(model_path, num_iterations=50)
    
    if onnxsim_available and simplified_path != model_path:
        print("\nb) ç®€åŒ–åæ¨¡å‹æ€§èƒ½ï¼š")
        simp_time, simp_fps = benchmark_model(simplified_path, num_iterations=50)
        
        if orig_time and simp_time:
            speedup = orig_time / simp_time
            print(f"\nç®€åŒ–åçš„æ¨¡å‹é€Ÿåº¦æå‡: {speedup:.2f}x")
    
    # 6. è§£é‡Šæ¨¡å‹ä¼˜åŒ–æŠ€æœ¯
    print("\n[6] æ¨¡å‹ä¼˜åŒ–æŠ€æœ¯")
    explain_optimization_techniques()
    
    # 7. è§£é‡Šéƒ¨ç½²é€‰é¡¹
    print("\n[7] éƒ¨ç½²é€‰é¡¹")
    explain_deployment_options()
    
    print("\nONNXæ¨¡å‹ä¼˜åŒ–ä¸éƒ¨ç½²æ•™ç¨‹å®Œæˆï¼")
    print("æ­å–œä½ å®Œæˆäº†æ•´ä¸ªONNXæ•™ç¨‹ç³»åˆ—ï¼ğŸ‘")

if __name__ == "__main__":
    main()