# -*- coding: utf-8 -*-
"""
ä½¿ç”¨ã€Šçº¢æ¥¼æ¢¦ã€‹æ–‡æœ¬æµ‹è¯•è¿Ÿåˆ†(Late Chunking)å’Œä¼ ç»Ÿåˆ†å‰²æ–¹æ³•çš„å¯¹æ¯”
"""

import os
import sys
import torch
import numpy as np
from typing import List, Tuple, Dict, Any
from transformers import AutoTokenizer, AutoModel
import torch.nn.functional as F
from dataclasses import dataclass
import time

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = r"C:\Users\k\Documents\project\programming_project\python_project\importance\XiaokeAILabs"
sys.path.append(project_root)

# å¯¼å…¥è¿Ÿåˆ†å¤„ç†å™¨
from datas.test_late_chunking.test_late_chunking import LateChunkingProcessor, ChunkInfo

def load_hongloumeng_text():
    """åŠ è½½ã€Šçº¢æ¥¼æ¢¦ã€‹æ–‡æœ¬"""
    hongloumeng_path = os.path.join(project_root, "datas", "test_late_chunking", "çº¢æ¥¼æ¢¦.txt")
    
    if not os.path.exists(hongloumeng_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°ã€Šçº¢æ¥¼æ¢¦ã€‹æ–‡ä»¶: {hongloumeng_path}")
        return None
    
    try:
        with open(hongloumeng_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"æˆåŠŸåŠ è½½ã€Šçº¢æ¥¼æ¢¦ã€‹æ–‡æœ¬ï¼Œæ€»é•¿åº¦: {len(content)} å­—ç¬¦")
        return content
    
    except Exception as e:
        print(f"è¯»å–ã€Šçº¢æ¥¼æ¢¦ã€‹æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return None

def extract_sample_text(full_text: str, start_chars: int = 1000, length: int = 8000) -> str:
    """ä»å®Œæ•´æ–‡æœ¬ä¸­æå–æ ·æœ¬ç”¨äºæµ‹è¯•ï¼Œé»˜è®¤æå–8000å­—ç¬¦"""
    if not full_text:
        return ""
    
    # ä»æŒ‡å®šä½ç½®å¼€å§‹æå–
    sample = full_text[start_chars:start_chars + length]
    
    # å°è¯•åœ¨å¥å­è¾¹ç•Œç»“æŸ
    for end_char in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
        last_sentence_end = sample.rfind(end_char)
        if last_sentence_end > length * 0.8:  # è‡³å°‘ä¿ç•™80%çš„é•¿åº¦
            sample = sample[:last_sentence_end + 1]
            break
    
    return sample.strip()

def traditional_chunking_encode(processor: LateChunkingProcessor, chunks: List[str]) -> List[np.ndarray]:
    """
    ä¼ ç»Ÿåˆ†å—æ–¹æ³•ï¼šåˆ†åˆ«ç¼–ç æ¯ä¸ªå—
    
    Args:
        processor: LateChunkingProcessorå®ä¾‹
        chunks: æ–‡æœ¬å—åˆ—è¡¨
        
    Returns:
        æ¯ä¸ªå—çš„embeddingåˆ—è¡¨
    """
    embeddings = []
    
    print(f"  æ­£åœ¨ç¼–ç  {len(chunks)} ä¸ªä¼ ç»Ÿåˆ†å—...")
    for i, chunk in enumerate(chunks):
        if i % 5 == 0:
            print(f"    è¿›åº¦: {i+1}/{len(chunks)}")
        
        # åˆ†åˆ«ç¼–ç æ¯ä¸ªå—
        inputs = processor.tokenizer(
            chunk,
            return_tensors='pt',
            truncation=True,
            max_length=512,
            padding=True
        )
        inputs = {k: v.to(processor.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = processor.model(**inputs)
            # ä½¿ç”¨[CLS]tokençš„è¡¨ç¤º
            chunk_embedding = outputs.last_hidden_state[0, 0, :].cpu().numpy()
            chunk_embedding = chunk_embedding / np.linalg.norm(chunk_embedding)
            embeddings.append(chunk_embedding)
    
    return embeddings

def create_traditional_chunks(text: str, chunk_size: int = 800) -> List[str]:
    """åˆ›å»ºä¼ ç»Ÿçš„æ–‡æœ¬åˆ†å—ï¼Œé»˜è®¤å—å¤§å°ä¸º800å­—ç¬¦"""
    # æŒ‰å¥å­åˆ†å‰²
    sentences = []
    current_sentence = ""
    
    for char in text:
        current_sentence += char
        if char in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'â€¦']:
            if current_sentence.strip():
                sentences.append(current_sentence.strip())
            current_sentence = ""
    
    # å¦‚æœæœ€åè¿˜æœ‰æœªå®Œæˆçš„å¥å­
    if current_sentence.strip():
        sentences.append(current_sentence.strip())
    
    # å°†å¥å­ç»„åˆæˆå—
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk) + len(sentence) <= chunk_size:
            current_chunk += sentence
        else:
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
    
    # æ·»åŠ æœ€åä¸€ä¸ªå—
    if current_chunk:
        chunks.append(current_chunk)
    
    return chunks

def test_search_quality(query: str, processor: LateChunkingProcessor, 
                       late_chunk_infos: List[ChunkInfo], 
                       traditional_chunks: List[str], 
                       traditional_embeddings: List[np.ndarray]) -> None:
    """æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„æœç´¢è´¨é‡"""
    
    print(f"\næŸ¥è¯¢: '{query}'")
    print("-" * 50)
    
    # ç¼–ç æŸ¥è¯¢
    query_inputs = processor.tokenizer(
        query,
        return_tensors='pt',
        truncation=True,
        max_length=512,
        padding=True
    )
    query_inputs = {k: v.to(processor.device) for k, v in query_inputs.items()}
    
    with torch.no_grad():
        query_outputs = processor.model(**query_inputs)
        query_embedding = query_outputs.last_hidden_state[0, 0, :].cpu().numpy()
        query_embedding = query_embedding / np.linalg.norm(query_embedding)
    
    # è¿Ÿåˆ†æ–¹æ³•æœç´¢
    late_results = processor.similarity_search(query, late_chunk_infos, top_k=3)
    
    print("ğŸ”¥ è¿Ÿåˆ†æ–¹æ³•ç»“æœ:")
    for i, (chunk_info, score) in enumerate(late_results):
        preview = chunk_info.text.replace('\n', ' ')[:100]
        print(f"  {i+1}. [ç›¸ä¼¼åº¦: {score:.4f}] {preview}...")
    
    # ä¼ ç»Ÿæ–¹æ³•æœç´¢
    traditional_similarities = []
    for chunk, embedding in zip(traditional_chunks, traditional_embeddings):
        similarity = np.dot(query_embedding, embedding)
        traditional_similarities.append((chunk, float(similarity)))
    
    traditional_similarities.sort(key=lambda x: x[1], reverse=True)
    
    print("\nğŸ“š ä¼ ç»Ÿåˆ†å—ç»“æœ:")
    for i, (chunk, score) in enumerate(traditional_similarities[:3]):
        preview = chunk.replace('\n', ' ')[:100]
        print(f"  {i+1}. [ç›¸ä¼¼åº¦: {score:.4f}] {preview}...")

def run_hongloumeng_test():
    """è¿è¡Œã€Šçº¢æ¥¼æ¢¦ã€‹æµ‹è¯•"""
    
    print("=" * 80)
    print("ğŸ›ï¸  ã€Šçº¢æ¥¼æ¢¦ã€‹è¿Ÿåˆ† vs ä¼ ç»Ÿåˆ†å‰²å¯¹æ¯”æµ‹è¯•")
    print("=" * 80)
    
    # åŠ è½½æ–‡æœ¬
    full_text = load_hongloumeng_text()
    if not full_text:
        return
      # æå–æµ‹è¯•æ ·æœ¬ (ä½¿ç”¨æ›´é•¿çš„æ–‡æœ¬)
    test_text = extract_sample_text(full_text, start_chars=500, length=8000)
    print(f"\nğŸ“– æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(test_text)} å­—ç¬¦")
    print(f"ğŸ“ æ–‡æœ¬é¢„è§ˆ: {test_text[:200]}...")
    
    # åˆå§‹åŒ–æ¨¡å‹
    model_path = r"C:\Users\k\Desktop\BaiduSyncdisk\baidu_sync_documents\hf_models\bge-m3"
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ {model_path}")
        return
    
    print(f"\nğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
    processor = LateChunkingProcessor(model_path, max_length=8192)
    
    # ä½¿ç”¨å•ä¸€å—å¤§å°è¿›è¡Œæµ‹è¯•
    chunk_size = 800    # ä½¿ç”¨å•ä¸€å—å¤§å°è¿›è¡Œæµ‹è¯•
    chunk_size = 800
    
    print(f"\n" + "="*60)
    print(f"ğŸ“ æµ‹è¯•å—å¤§å°: {chunk_size} tokens")
    print("="*60)
    
    # è¿Ÿåˆ†æ–¹æ³•
    print("\nâ±ï¸  è¿Ÿåˆ†æ–¹æ³•å¤„ç†ä¸­...")
    start_time = time.time()
    late_chunk_infos = processor.process_document(test_text, chunk_size=chunk_size)
    late_time = time.time() - start_time
    
    print(f"âœ… è¿Ÿåˆ†å®Œæˆ - è€—æ—¶: {late_time:.2f}ç§’, ç”Ÿæˆå—æ•°: {len(late_chunk_infos)}")
    
    # ä¼ ç»Ÿåˆ†å—æ–¹æ³•
    print("\nâ±ï¸  ä¼ ç»Ÿåˆ†å—æ–¹æ³•å¤„ç†ä¸­...")
    start_time = time.time()
    traditional_chunks = create_traditional_chunks(test_text, chunk_size=chunk_size)
    traditional_embeddings = traditional_chunking_encode(processor, traditional_chunks)
    traditional_time = time.time() - start_time
    
    print(f"âœ… ä¼ ç»Ÿåˆ†å—å®Œæˆ - è€—æ—¶: {traditional_time:.2f}ç§’, ç”Ÿæˆå—æ•°: {len(traditional_chunks)}")
    
    # æ€§èƒ½å¯¹æ¯”
    print(f"\nğŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"  â€¢ è¿Ÿåˆ†æ–¹æ³•: {late_time:.2f}ç§’")
    print(f"  â€¢ ä¼ ç»Ÿæ–¹æ³•: {traditional_time:.2f}ç§’")
    print(f"  â€¢ é€Ÿåº¦æå‡: {traditional_time/late_time:.2f}x" if late_time > 0 else "  â€¢ æ— æ³•è®¡ç®—é€Ÿåº¦æå‡")
    
    # æµ‹è¯•æœç´¢è´¨é‡
    queries = [
        "è´¾å®ç‰çš„æ€§æ ¼ç‰¹ç‚¹",
        "æ—é»›ç‰è¿›è´¾åºœ",
        "è´¾é›¨æ‘çš„ä»•é€”ç»å†",
        "è£å›½åºœçš„ç¹å",
        "ç”„å£«éšçš„æ•…äº‹"
    ]
    
    print(f"\nğŸ” æœç´¢è´¨é‡æµ‹è¯• (å—å¤§å°: {chunk_size})")
    for query in queries:
        test_search_quality(query, processor, late_chunk_infos, 
                          traditional_chunks, traditional_embeddings)
    
    print(f"\nğŸ’¾ å—ä¿¡æ¯ç»Ÿè®¡:")
    print(f"  è¿Ÿåˆ†æ–¹æ³•:")
    print(f"    - å¹³å‡å—é•¿åº¦: {np.mean([len(info.text) for info in late_chunk_infos]):.1f} å­—ç¬¦")
    print(f"    - æœ€çŸ­å—: {min([len(info.text) for info in late_chunk_infos])} å­—ç¬¦")
    print(f"    - æœ€é•¿å—: {max([len(info.text) for info in late_chunk_infos])} å­—ç¬¦")
    
    print(f"  ä¼ ç»Ÿæ–¹æ³•:")
    print(f"    - å¹³å‡å—é•¿åº¦: {np.mean([len(chunk) for chunk in traditional_chunks]):.1f} å­—ç¬¦")
    print(f"    - æœ€çŸ­å—: {min([len(chunk) for chunk in traditional_chunks])} å­—ç¬¦")
    print(f"    - æœ€é•¿å—: {max([len(chunk) for chunk in traditional_chunks])} å­—ç¬¦")

def test_different_text_samples():
    """æµ‹è¯•ä¸åŒçš„æ–‡æœ¬æ ·æœ¬"""
    
    print("\n" + "="*80)
    print("ğŸ“š ä¸åŒæ–‡æœ¬ç‰‡æ®µæµ‹è¯•")
    print("="*80)
    
    full_text = load_hongloumeng_text()
    if not full_text:
        return
    
    # æµ‹è¯•ä¸åŒç« èŠ‚çš„ç‰‡æ®µ
    sample_configs = [
        {"start": 1000, "length": 2000, "name": "ç¬¬ä¸€ç« å¼€å¤´"},
        {"start": 5000, "length": 2000, "name": "ç¬¬ä¸€ç« ä¸­æ®µ"},
        {"start": 15000, "length": 2000, "name": "ç¬¬äºŒç« å†…å®¹"},
    ]
    
    model_path = r"C:\Users\k\Desktop\BaiduSyncdisk\baidu_sync_documents\hf_models\bge-m3"
    if not os.path.exists(model_path):
        print(f"âŒ é”™è¯¯ï¼šæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ {model_path}")
        return
    
    processor = LateChunkingProcessor(model_path, max_length=4096)
    
    for config in sample_configs:
        print(f"\nğŸ“– æµ‹è¯•ç‰‡æ®µ: {config['name']}")
        print("-" * 40)
        
        sample_text = extract_sample_text(full_text, config['start'], config['length'])
        print(f"æ–‡æœ¬é•¿åº¦: {len(sample_text)} å­—ç¬¦")
          # è¿Ÿåˆ†å¤„ç†
        late_chunks = processor.process_document(sample_text, chunk_size=800)
        
        # ä¼ ç»Ÿåˆ†å—
        traditional_chunks = create_traditional_chunks(sample_text, chunk_size=800)
        
        print(f"è¿Ÿåˆ†å—æ•°: {len(late_chunks)}, ä¼ ç»Ÿå—æ•°: {len(traditional_chunks)}")
        
        # ç®€å•æœç´¢æµ‹è¯•
        query = "è´¾å®ç‰"
        if query in sample_text:
            late_results = processor.similarity_search(query, late_chunks, top_k=1)
            if late_results:
                print(f"æŸ¥è¯¢'{query}' - è¿Ÿåˆ†æœ€ä½³åŒ¹é…ç›¸ä¼¼åº¦: {late_results[0][1]:.4f}")

if __name__ == "__main__":
    try:
        # è¿è¡Œä¸»æµ‹è¯•
        run_hongloumeng_test()
        
        # è¿è¡Œä¸åŒæ ·æœ¬æµ‹è¯•
        test_different_text_samples()
        
        print("\n" + "="*80)
        print("ğŸ‰ æµ‹è¯•å®Œæˆï¼")
        print("="*80)
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
