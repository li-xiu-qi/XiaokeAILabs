{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e07f726",
   "metadata": {},
   "source": [
    "# 探索 ONNX 模型输入输出\n",
    "\n",
    "本 Notebook 用于逐步加载 ONNX 模型，并检查其输入和输出的详细信息，以便正确准备推理所需的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3bcfa1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\openvino\\lib\\site-packages\\openvino\\runtime\\__init__.py:10: DeprecationWarning: The `openvino.runtime` module is deprecated and will be removed in the 2026.0 release. Please replace `openvino.runtime` with `openvino`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import openvino.runtime as ov\n",
    "from pathlib import Path\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7033e30b",
   "metadata": {},
   "source": [
    "## 1. 配置模型路径和推理设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87e34fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX 模型文件路径: C:\\\\Users\\\\k\\\\Desktop\\\\BaiduSyncdisk\\\\baidu_sync_documents\\\\hf_models\\\\jina-clip-v2\\\\onnx\\\\model.onnx\n",
      "推理设备: CPU\n"
     ]
    }
   ],
   "source": [
    "# --- 配置 ---\n",
    "# 输入：你的 ONNX 模型文件路径\n",
    "# 你可以更改为列表中的其他 .onnx 文件，例如 model_fp16.onnx\n",
    "onnx_model_path = r\"C:\\\\Users\\\\k\\\\Desktop\\\\BaiduSyncdisk\\\\baidu_sync_documents\\\\hf_models\\\\jina-clip-v2\\\\onnx\\\\model.onnx\"\n",
    "\n",
    "# 推理设备，例如 \"CPU\", \"GPU\", \"NPU\"\n",
    "device_name = \"CPU\"\n",
    "\n",
    "model_file = Path(onnx_model_path)\n",
    "if not model_file.exists():\n",
    "    print(f\"错误：ONNX 模型文件 '{onnx_model_path}' 不存在。请检查路径。\")\n",
    "else:\n",
    "    print(f\"ONNX 模型文件路径: {onnx_model_path}\")\n",
    "    print(f\"推理设备: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e819d58c",
   "metadata": {},
   "source": [
    "## 2. 初始化 OpenVINO Core 并读取模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05b8eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可用的 OpenVINO 设备: ['CPU', 'GPU', 'NPU']\n",
      "正在从 'C:\\\\Users\\\\k\\\\Desktop\\\\BaiduSyncdisk\\\\baidu_sync_documents\\\\hf_models\\\\jina-clip-v2\\\\onnx\\\\model.onnx' 读取 ONNX 模型...\n",
      "模型读取成功。\n"
     ]
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "print(f\"可用的 OpenVINO 设备: {core.available_devices}\")\n",
    "\n",
    "model = None\n",
    "if model_file.exists():\n",
    "    try:\n",
    "        print(f\"正在从 '{onnx_model_path}' 读取 ONNX 模型...\")\n",
    "        model = core.read_model(model=str(model_file)) # Path 对象需要转换为 str\n",
    "        print(\"模型读取成功。\")\n",
    "    except Exception as e:\n",
    "        print(f\"读取模型时发生错误: {e}\")\n",
    "else:\n",
    "    print(\"模型文件未找到，跳过读取。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0054a9c0",
   "metadata": {},
   "source": [
    "## 3. 检查模型输入信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d74d258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 模型输入信息 ---\n",
      "  输入 #0:\n",
      "    名称 (any_name): input_ids\n",
      "    形状 (partial_shape): [?,?]\n",
      "    数据类型: <Type: 'int64_t'>\n",
      "    Tensor 名称 (get_tensor().name): N/A\n",
      "  输入 #1:\n",
      "    名称 (any_name): pixel_values\n",
      "    形状 (partial_shape): [?,3,512,512]\n",
      "    数据类型: <Type: 'float32'>\n",
      "    Tensor 名称 (get_tensor().name): N/A\n"
     ]
    }
   ],
   "source": [
    "if model:\n",
    "    print(\"\\n--- 模型输入信息 ---\")\n",
    "    for i, input_tensor in enumerate(model.inputs):\n",
    "        print(f\"  输入 #{i}:\")\n",
    "        print(f\"    名称 (any_name): {input_tensor.any_name}\")\n",
    "        # 打印所有可能的名称\n",
    "        # print(f\"    所有名称: {input_tensor.names}\") \n",
    "        print(f\"    形状 (partial_shape): {input_tensor.partial_shape}\")\n",
    "        print(f\"    数据类型: {input_tensor.element_type}\")\n",
    "        print(f\"    Tensor 名称 (get_tensor().name): {input_tensor.get_tensor().name if hasattr(input_tensor.get_tensor(), 'name') else 'N/A'}\")\n",
    "\n",
    "else:\n",
    "    print(\"模型未加载，无法显示输入信息。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca34564",
   "metadata": {},
   "source": [
    "## 3.1 模型输入详细解析\n",
    "\n",
    "模型具有两个输入:\n",
    "\n",
    "1. **输入 #0 (input_ids)**:\n",
    "  - 这是一个文本输入张量，用于接收文本标记ID\n",
    "  - 形状为动态的 `[?,?]`，表示批次大小和序列长度都是可变的\n",
    "  - 数据类型为64位整数 (`int64_t`)\n",
    "  - 通常用于传递文本的token ID序列\n",
    "\n",
    "2. **输入 #1 (pixel_values)**:\n",
    "  - 这是一个图像输入张量，用于接收预处理后的图像数据\n",
    "  - 形状为 `[?,3,512,512]`，表示:\n",
    "    - 批次大小是动态的 (`?`)\n",
    "    - 3个颜色通道 (RGB)\n",
    "    - 图像尺寸为512×512像素\n",
    "  - 数据类型为32位浮点数 (`float32`)\n",
    "  - 通常用于传递归一化后的图像像素值\n",
    "\n",
    "这是一个典型的多模态CLIP模型输入格式，同时处理文本和图像数据，用于计算文本和图像之间的相似度。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad7c6f6",
   "metadata": {},
   "source": [
    "## 4. 检查模型输出信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b3fa695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 模型输出信息 ---\n",
      "  输出 #0:\n",
      "    名称 (any_name): text_embeddings\n",
      "    形状 (partial_shape): [?,1024]\n",
      "    数据类型: <Type: 'float32'>\n",
      "    Tensor 名称 (get_tensor().name): N/A\n",
      "  输出 #1:\n",
      "    名称 (any_name): image_embeddings\n",
      "    形状 (partial_shape): [?,1024]\n",
      "    数据类型: <Type: 'float32'>\n",
      "    Tensor 名称 (get_tensor().name): N/A\n",
      "  输出 #2:\n",
      "    名称 (any_name): l2norm_text_embeddings\n",
      "    形状 (partial_shape): [?,1024]\n",
      "    数据类型: <Type: 'float32'>\n",
      "    Tensor 名称 (get_tensor().name): N/A\n",
      "  输出 #3:\n",
      "    名称 (any_name): l2norm_image_embeddings\n",
      "    形状 (partial_shape): [?,1024]\n",
      "    数据类型: <Type: 'float32'>\n",
      "    Tensor 名称 (get_tensor().name): N/A\n"
     ]
    }
   ],
   "source": [
    "if model:\n",
    "    print(\"\\n--- 模型输出信息 ---\")\n",
    "    for i, output_tensor in enumerate(model.outputs):\n",
    "        print(f\"  输出 #{i}:\")\n",
    "        print(f\"    名称 (any_name): {output_tensor.any_name}\")\n",
    "        # print(f\"    所有名称: {output_tensor.names}\")\n",
    "        print(f\"    形状 (partial_shape): {output_tensor.partial_shape}\")\n",
    "        print(f\"    数据类型: {output_tensor.element_type}\")\n",
    "        print(f\"    Tensor 名称 (get_tensor().name): {output_tensor.get_tensor().name if hasattr(output_tensor.get_tensor(), 'name') else 'N/A'}\")\n",
    "\n",
    "else:\n",
    "    print(\"模型未加载，无法显示输出信息。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c001cf7",
   "metadata": {},
   "source": [
    "## 4.1 模型输出详细解析\n",
    "\n",
    "模型共有四个输出：\n",
    "\n",
    "1. **text_embeddings**:\n",
    "    - 形状为 `[?,1024]`，表示批次大小为动态，每个文本的嵌入向量维度为1024\n",
    "    - 数据类型为32位浮点数（`float32`）\n",
    "    - 这是文本的原始嵌入向量表示\n",
    "\n",
    "2. **image_embeddings**:\n",
    "    - 形状为 `[?,1024]`，表示批次大小为动态，每个图像的嵌入向量维度为1024\n",
    "    - 数据类型为32位浮点数（`float32`）\n",
    "    - 这是图像的原始嵌入向量表示\n",
    "\n",
    "3. **l2norm_text_embeddings**:\n",
    "    - 形状为 `[?,1024]`，与text_embeddings相同\n",
    "    - 数据类型为32位浮点数（`float32`）\n",
    "    - 这是经过L2归一化处理后的文本嵌入向量，用于计算余弦相似度\n",
    "\n",
    "4. **l2norm_image_embeddings**:\n",
    "    - 形状为 `[?,1024]`，与image_embeddings相同\n",
    "    - 数据类型为32位浮点数（`float32`）\n",
    "    - 这是经过L2归一化处理后的图像嵌入向量，用于计算余弦相似度\n",
    "\n",
    "L2归一化后的嵌入向量可直接用于计算文本与图像之间的余弦相似度，主要用于跨模态匹配任务（如图文检索）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af27897",
   "metadata": {},
   "source": [
    "## 5. 准备输入数据 (需要根据上面的信息修改)\n",
    "\n",
    "**重要:** 下面的代码块用于准备输入数据。你需要仔细查看上面单元格打印出的“模型输入信息”，并相应地修改此部分。\n",
    "特别是要注意：\n",
    "- **输入名称**: 确保 `inputs` 字典中的键与模型期望的输入名称完全匹配。\n",
    "- **形状**: 根据 `partial_shape` 创建具有正确维度的 NumPy 数组。如果存在动态维度 (例如 `?` 或 `-1`)，你需要为其选择一个具体的值 (例如，批处理大小为1，序列长度根据你的数据决定)。\n",
    "- **数据类型**: 确保 NumPy 数组的数据类型 (`dtype`) 与模型期望的 `element_type` 一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2142a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "请根据以下模型输入信息，修改下面的虚拟数据准备代码：\n",
      "  - 输入名称: input_ids, 形状: [?,?], 数据类型: <Type: 'int64_t'>\n",
      "  - 输入名称: pixel_values, 形状: [?,3,512,512], 数据类型: <Type: 'float32'>\n",
      "为输入 'input_ids' 创建了形状为 [1, 77]，类型为 <class 'numpy.int64'> 的虚拟数据。\n",
      "为输入 'pixel_values' 创建了形状为 [1, 3, 512, 512]，类型为 <class 'numpy.float32'> 的虚拟数据。\n"
     ]
    }
   ],
   "source": [
    "inputs = {}\n",
    "if model:\n",
    "    print(\"请根据以下模型输入信息，修改下面的虚拟数据准备代码：\")\n",
    "    for input_tensor in model.inputs:\n",
    "        print(f\"  - 输入名称: {input_tensor.any_name}, 形状: {input_tensor.partial_shape}, 数据类型: {input_tensor.element_type}\")\n",
    "\n",
    "    # --- !!! 开始修改此部分 !!! ---\n",
    "    # 根据用户提供的模型输入信息准备数据\n",
    "    \n",
    "    # 输入1: input_ids\n",
    "    input_name_1 = \"input_ids\" \n",
    "    # 形状 [batch_size, sequence_length], 例如 [1, 77]\n",
    "    shape_1 = [1, 77] \n",
    "    dtype_1 = np.int64 \n",
    "    # 假设 token IDs 范围在 0 到模型词汇表大小减1 (例如 30000 for many tokenizers)\n",
    "    dummy_data_1 = np.random.randint(0, 30000, size=shape_1, dtype=dtype_1)\n",
    "    inputs[input_name_1] = dummy_data_1\n",
    "    print(f\"为输入 '{input_name_1}' 创建了形状为 {shape_1}，类型为 {dtype_1} 的虚拟数据。\")\n",
    "\n",
    "    # 输入2: pixel_values\n",
    "    input_name_2 = \"pixel_values\" \n",
    "    # 形状 [batch_size, channels, height, width], 例如 [1, 3, 512, 512]\n",
    "    shape_2 = [1, 3, 512, 512] \n",
    "    dtype_2 = np.float32 \n",
    "    # 像素值通常在 [0, 1] 或 [-1, 1] 范围内，具体取决于预处理\n",
    "    # 这里我们生成随机的 float32 数据，模拟归一化后的像素值 (例如，在0到1之间)\n",
    "    dummy_data_2 = np.random.rand(*shape_2).astype(dtype_2)\n",
    "    inputs[input_name_2] = dummy_data_2\n",
    "    print(f\"为输入 '{input_name_2}' 创建了形状为 {shape_2}，类型为 {dtype_2} 的虚拟数据。\")\n",
    "    # --- !!! 结束修改此部分 !!! ---\n",
    "\n",
    "    if not inputs:\n",
    "        print(\"\\\\n错误：未能准备任何输入数据。请检查上面的模型输入信息并修改此单元格中的代码。\")\n",
    "else:\n",
    "    print(\"模型未加载，无法准备输入数据。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa43df",
   "metadata": {},
   "source": [
    "## 6. 编译模型并执行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2310fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\n正在将模型编译到设备 'CPU'...\n",
      "模型编译成功。\n",
      "推理请求创建成功。\n",
      "\\n正在使用准备好的输入进行推理...\n",
      "推理完成。\n"
     ]
    }
   ],
   "source": [
    "compiled_model = None\n",
    "infer_request = None\n",
    "results = None\n",
    "\n",
    "if model and inputs:\n",
    "    try:\n",
    "        print(f\"\\\\n正在将模型编译到设备 '{device_name}'...\")\n",
    "        compiled_model = core.compile_model(model=model, device_name=device_name)\n",
    "        print(\"模型编译成功。\")\n",
    "\n",
    "        infer_request = compiled_model.create_infer_request()\n",
    "        print(\"推理请求创建成功。\")\n",
    "        \n",
    "        print(f\"\\\\n正在使用准备好的输入进行推理...\")\n",
    "        # print(\"输入数据详情:\")\n",
    "        # for name, data in inputs.items():\n",
    "        #    print(f\"  {name}: shape={data.shape}, dtype={data.dtype}\")\n",
    "\n",
    "        results = infer_request.infer(inputs=inputs)\n",
    "        print(\"推理完成。\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n编译或推理过程中发生错误: {e}\")\n",
    "        print(\"请检查：\")\n",
    "        print(\"1. 输入数据是否与模型期望的完全一致（名称、形状、数据类型）。\")\n",
    "        print(f\"2. 设备 '{device_name}' 是否可用且模型是否支持在该设备上运行。\")\n",
    "\n",
    "elif not model:\n",
    "    print(\"模型未加载，无法编译或推理。\")\n",
    "else:\n",
    "    print(\"输入数据未准备好，无法编译或推理。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5475ce7a",
   "metadata": {},
   "source": [
    "## 7. 查看推理结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    print(\"\\\\n--- 推理结果 ---\")\n",
    "    # `results` 是一个字典，键是输出张量的名称 (通常是 compiled_model.outputs[i].get_tensor().name)\n",
    "    # 值是包含数据的 NumPy 数组。\n",
    "    \n",
    "    # 打印 compiled_model 的输出张量信息，以便与 results 字典的键进行匹配\n",
    "    # print(\"\\\\nCompiled Model Output Tensor Info:\")\n",
    "    # for i, out_tensor_node in enumerate(compiled_model.outputs):\n",
    "    #     print(f\"  Output Node #{i}:\")\n",
    "    #     print(f\"    Any Name: {out_tensor_node.get_any_name()}\") # 通常是原始模型中的名称\n",
    "    #     print(f\"    Tensor Names: {out_tensor_node.get_names()}\") # 可能包含多个名称，包括内部名称\n",
    "    #     # print(f\"    Tensor Name (get_tensor().name): {out_tensor_node.get_tensor().name}\") # 这是结果字典中常用的键\n",
    "\n",
    "    print(\"\\\\nResults Dictionary:\")\n",
    "    for output_name_from_result, result_data in results.items():\n",
    "        print(f\"  输出名称 (from results key): {output_name_from_result}\")\n",
    "        print(f\"    形状: {result_data.shape}\")\n",
    "        print(f\"    数据类型: {result_data.dtype}\")\n",
    "        print(f\"    部分数据: {result_data.flatten()[:10]}...\") # 打印前10个扁平化数据\n",
    "        \n",
    "    # 你也可以尝试通过 compiled_model.outputs 来迭代并获取结果\n",
    "    # print(\"\\\\nIterating through compiled_model.outputs:\")\n",
    "    # for output_node in compiled_model.outputs:\n",
    "    #     try:\n",
    "    #         # 尝试使用 output_node 直接从 results 中获取数据\n",
    "    #         # 这依赖于 OpenVINO 如何将 output_node 映射到 results 字典的键\n",
    "    #         # 最可靠的方式通常是使用 output_node.get_tensor().name 或 output_node.get_any_name()\n",
    "    #         # 如果这些名称与 results 字典中的键不完全匹配，你可能需要更复杂的匹配逻辑\n",
    "            \n",
    "    #         # 尝试使用 any_name (通常是原始模型中的名称)\n",
    "    #         key_to_try = output_node.get_any_name()\n",
    "    #         if key_to_try in results:\n",
    "    #             result_data = results[key_to_try]\n",
    "    #             print(f\"  输出名称 (any_name): {key_to_try}\")\n",
    "    #             print(f\"    形状: {result_data.shape}\")\n",
    "    #             print(f\"    部分数据: {result_data.flatten()[:10]}...\")\n",
    "    #         else:\n",
    "    #             # 尝试使用 tensor name (通常是结果字典中的键)\n",
    "    #             # 有时 get_tensor().name 可能不存在或不唯一\n",
    "    #             found_by_tensor_name = False\n",
    "    #             if hasattr(output_node.get_tensor(), 'name'):\n",
    "    #                 tensor_name = output_node.get_tensor().name\n",
    "    #                 if tensor_name in results:\n",
    "    #                     result_data = results[tensor_name]\n",
    "    #                     print(f\"  输出名称 (tensor_name): {tensor_name}\")\n",
    "    #                     print(f\"    形状: {result_data.shape}\")\n",
    "    #                     print(f\"    部分数据: {result_data.flatten()[:10]}...\")\n",
    "    #                     found_by_tensor_name = True\n",
    "                \n",
    "    #             if not found_by_tensor_name:\n",
    "    #                 print(f\"  警告: 未能在 results 中找到输出 '{key_to_try}' 或其 tensor name。\")\n",
    "    #                 print(f\"    Available keys in results: {list(results.keys())}\")\n",
    "\n",
    "    #     except Exception as e_res:\n",
    "    #         print(f\"处理输出 {output_node.get_any_name()} 时出错: {e_res}\")\n",
    "            \n",
    "elif infer_request and not results:\n",
    "    print(\"推理已执行，但 results 为空。可能在推理步骤中发生了错误，或者模型没有输出。\")\n",
    "elif model and not inputs:\n",
    "    print(\"输入数据未准备，无法查看结果。\")\n",
    "else:\n",
    "    print(\"模型未加载或推理未执行，无法显示结果。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d03ad91",
   "metadata": {},
   "source": [
    "## 8. (可选) 清理资源\n",
    "虽然 Python 的垃圾回收通常会处理，但在某些情况下，显式删除对象可能有助于更快地释放资源，尤其是在处理大型模型或在循环中操作时。\n",
    "对于 OpenVINO 对象，通常不需要手动删除，因为它们会在超出作用域时被清理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5996469f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model\n",
    "# del compiled_model\n",
    "# del infer_request\n",
    "# print(\"资源已清理 (如果取消注释)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
