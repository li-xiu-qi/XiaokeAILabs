{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17aa8fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\.conda\\envs\\modelscope\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "库导入完成！\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的库\n",
    "import torch\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "import gc\n",
    "\n",
    "# 设置随机种子确保结果可重现\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"库导入完成！\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392cc3a4",
   "metadata": {},
   "source": [
    "## 1. 加载本地Qwen2.5模型\n",
    "\n",
    "我们将从您指定的本地路径加载Qwen2.5-0.5B-Instruct模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8c259b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载模型...\n",
      "模型加载完成！\n",
      "模型参数量: 494,032,768\n",
      "模型类型: qwen2\n",
      "模型词汇表大小: 151665\n",
      "模型设备: cpu\n"
     ]
    }
   ],
   "source": [
    "# 模型路径\n",
    "model_path = r\"C:\\Users\\k\\Desktop\\BaiduSyncdisk\\baidu_sync_documents\\hf_models\\Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "# 加载tokenizer和模型\n",
    "print(\"正在加载模型...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# 设置pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"未设置pad_token，使用eos_token作为pad_token\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"模型加载完成！\")\n",
    "print(f\"模型参数量: {model.num_parameters():,}\")\n",
    "print(f\"模型类型: {model.config.model_type}\")\n",
    "print(f\"模型词汇表大小: {len(tokenizer)}\")\n",
    "print(f\"模型设备: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a443d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试无KV Cache的推理\n",
    "test_prompt = \"人工智能的未来发展趋势是\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2986ff53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[104455,   9370, 100353, 108616,  20412]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(test_prompt, return_tensors=\"pt\").to(model.device)\n",
    "input_ids # 生成的输入ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc83db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs类型: <class 'transformers.modeling_outputs.CausalLMOutputWithPast'>\n",
      "outputs内容: CausalLMOutputWithPast(loss=None, logits=tensor([[[ 5.2070,  9.7266,  3.8828,  ..., -2.3438, -2.3438, -2.3438],\n",
      "         [ 5.8750, 13.0781,  5.7617,  ..., -2.7461, -2.7461, -2.7461],\n",
      "         [10.0234,  9.0781,  4.7422,  ..., -4.0195, -4.0195, -4.0195],\n",
      "         [ 9.7344,  8.6875,  6.6406,  ..., -3.9551, -3.9551, -3.9551],\n",
      "         [ 5.9961, 11.2500,  7.0625,  ..., -4.1055, -4.1055, -4.1055]]],\n",
      "       dtype=torch.float16), past_key_values=<transformers.cache_utils.DynamicCache object at 0x0000022A9639D360>, hidden_states=None, attentions=None)\n",
      "past_key_values类型: <class 'transformers.cache_utils.DynamicCache'>\n",
      "past_key_values内容: <transformers.cache_utils.DynamicCache object at 0x0000022A9639D360>\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    # 打印下output的内容\n",
    "    print(f\"outputs类型: {type(outputs)}\") # transformers.modeling_outputs.CausalLMOutputWithPast\n",
    "    print(f\"outputs内容: {outputs}\") # 输出整个outputs对象的内容\n",
    "    # 打印下outputs的其他属性，比如past_key_values,\n",
    "    print(f\"past_key_values类型: {type(outputs.past_key_values)}\") # transformers.modeling_outputs.CausalLMOutputWithPast\n",
    "    print(f\"past_key_values内容: {outputs.past_key_values}\") # 输出past_key_values的内容\n",
    "    print(\"---\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5cf2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出logits形状: torch.Size([1, 5, 151936])\n",
      "输出logits类型: torch.float16\n",
      "输出logits设备: cpu\n",
      "输出logits内容: tensor([[[ 5.2070,  9.7266,  3.8828,  ..., -2.3438, -2.3438, -2.3438],\n",
      "         [ 5.8750, 13.0781,  5.7617,  ..., -2.7461, -2.7461, -2.7461],\n",
      "         [10.0234,  9.0781,  4.7422,  ..., -4.0195, -4.0195, -4.0195],\n",
      "         [ 9.7344,  8.6875,  6.6406,  ..., -3.9551, -3.9551, -3.9551],\n",
      "         [ 5.9961, 11.2500,  7.0625,  ..., -4.1055, -4.1055, -4.1055]]],\n",
      "       dtype=torch.float16)\n",
      "输出logits的第一个token的logits: tensor([5.2070, 9.7266, 3.8828, 0.4221, 3.5195, 5.7695, 4.5703, 8.4062, 8.5625,\n",
      "        6.0820], dtype=torch.float16)\n",
      "输出logits的最后一个token的logits: tensor([ 5.9961, 11.2500,  7.0625,  6.9805,  3.8594,  5.1445,  8.9609, 14.9141,\n",
      "         6.1797,  7.1914], dtype=torch.float16)\n",
      "输出logits的第一个维度: 1\n",
      "输出logits的第二个维度: 5\n",
      "输出logits的第三个维度: 151936\n",
      "输出logits的最后一个token的ID: 2130\n",
      "____________________________________________________________\n",
      "下一个token的ID: tensor([2130])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    print(f\"输出logits形状: {outputs.logits.shape}\") # torch.Size([1, 5, 151936]),意思是批次大小为1，序列长度为5，词汇表大小为151936\n",
    "    print(f\"输出logits类型: {outputs.logits.dtype}\") # torch.float16\n",
    "    print(f\"输出logits设备: {outputs.logits.device}\") # cpu\n",
    "    print(f\"输出logits内容: {outputs.logits}\") # 输出logits内容，这个内容指的是每个token的logits分数\n",
    "    print(f\"输出logits的第一个token的logits: {outputs.logits[0, 0, :10]}\") # 输出第一个token的前10个logits分数, 这可以帮助我们理解模型对第一个token的预测\n",
    "    print(f\"输出logits的最后一个token的logits: {outputs.logits[0, -1, :10]}\") # 输出最后一个token的前10个logits分数, 这可以帮助我们理解模型对最后一个token的预测\n",
    "    # logits 是一个三维张量，形状为 (batch_size, sequence_length, vocab_size)，\n",
    "    # 打印第一个维度\n",
    "    print(f\"输出logits的第一个维度: {outputs.logits.shape[0]}\") # 输出logits的第一个维度，表示批次大小\n",
    "    print(f\"输出logits的第二个维度: {outputs.logits.shape[1]}\") # 输出logits的第二个维度，表示序列长度\n",
    "    print(f\"输出logits的第三个维度: {outputs.logits.shape[2]}\") # 输出logits的第三个维度，表示词汇表大小\n",
    "    # token的id在矩阵的最后一个维度\n",
    "    next_token_id = torch.argmax(outputs.logits[:, -1, :], dim=-1) # 获取下一个token的ID\n",
    "    print(\"___\" * 20)\n",
    "    print(f\"下一个token的ID: {next_token_id}\") # 输出下一个token的ID，这个ID是模型预测的下一个token\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12aae48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出logits形状: torch.Size([1, 5, 151936])\n",
      "第0个位置预测的下一个token ID: 9370\n",
      "第0个位置预测的下一个token: '的'\n",
      "第1个位置预测的下一个token ID: 2073\n",
      "第1个位置预测的下一个token: '“'\n",
      "第2个位置预测的下一个token ID: 102021\n",
      "第2个位置预测的下一个token: '是什么'\n",
      "第3个位置预测的下一个token ID: 102021\n",
      "第3个位置预测的下一个token: '是什么'\n",
      "第4个位置预测的下一个token ID: 2130\n",
      "第4个位置预测的下一个token: '____'\n"
     ]
    }
   ],
   "source": [
    "# 遍历第二个维度\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    print(f\"输出logits形状: {outputs.logits.shape}\")\n",
    "    \n",
    "    # 遍历序列中每个位置，获取预测的下一个token ID\n",
    "    for i in range(outputs.logits.shape[1]):\n",
    "        # 获取第i个位置对下一个token的预测\n",
    "        predicted_token_id = outputs.logits[0, i, :].argmax()\n",
    "        print(f\"第{i}个位置预测的下一个token ID: {predicted_token_id}\")\n",
    "        \n",
    "        # 如果想看对应的token文本\n",
    "        predicted_token_text = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"第{i}个位置预测的下一个token: '{predicted_token_text}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d6b3f",
   "metadata": {},
   "source": [
    "\n",
    "## Transformer模型的预测机制\n",
    "\n",
    "在Transformer的**因果语言建模**（Causal Language Modeling）中，模型确实会为序列中的**每个位置**都预测下一个token。\n",
    "\n",
    "### 具体来说：\n",
    "\n",
    "````python\n",
    "# 遍历第二个维度\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    print(f\"输出logits形状: {outputs.logits.shape}\")\n",
    "    \n",
    "    # 遍历序列中每个位置，获取预测的下一个token ID\n",
    "    for i in range(outputs.logits.shape[1]):\n",
    "        # 获取第i个位置对下一个token的预测\n",
    "        predicted_token_id = outputs.logits[0, i, :].argmax()\n",
    "        print(f\"第{i}个位置预测的下一个token ID: {predicted_token_id}\")\n",
    "        \n",
    "        # 解释每个位置的含义\n",
    "        if i == 0:\n",
    "            print(f\"  → 这是模型看到第1个token后，预测第2个token\")\n",
    "        elif i == outputs.logits.shape[1] - 1:\n",
    "            print(f\"  → 这是模型看到完整输入后，预测下一个新token (用于生成)\")\n",
    "        else:\n",
    "            print(f\"  → 这是模型看到前{i+1}个token后，预测第{i+2}个token\")\n",
    "        \n",
    "        # 如果想看对应的token文本\n",
    "        predicted_token_text = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"第{i}个位置预测的下一个token: '{predicted_token_text}'\")\n",
    "        print(\"-\" * 50)\n",
    "````\n",
    "\n",
    "### 为什么是这样设计的？\n",
    "\n",
    "1. **训练时的需要**：\n",
    "   - 在训练过程中，模型需要学习在看到部分序列时预测下一个token\n",
    "   - 这样可以从一个序列中获得多个训练样本\n",
    "\n",
    "2. **注意力掩码的作用**：\n",
    "   - 第i个位置只能看到前i个token（包括自己）\n",
    "   - 不能看到后面的token（因果性约束）\n",
    "\n",
    "3. **并行训练**：\n",
    "   - 可以同时计算所有位置的预测，提高训练效率\n",
    "\n",
    "### 在例子中：\n",
    "\n",
    "如果输入是\"人工智能的未来发展趋势是\"（5个token），那么：\n",
    "- 位置0：看到\"人工\" → 预测下一个token\n",
    "- 位置1：看到\"人工智能\" → 预测下一个token  \n",
    "- 位置2：看到\"人工智能的\" → 预测下一个token\n",
    "- 位置3：看到\"人工智能的未来\" → 预测下一个token\n",
    "- 位置4：看到\"人工智能的未来发展趋势是\" → 预测下一个token（这个最重要！）\n",
    "\n",
    "**在实际生成时，我们只关心最后一个位置的预测**，因为那是基于完整输入的预测结果。\n",
    "\n",
    "这就是为什么在生成代码中我们用 `outputs.logits[:, -1, :]` 来获取最后一个位置的预测！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
